[
  {
    "question": "How does the workflow use and access variables, context objects, outputs from previous jobs, and secrets?",
    "answer": "# Workflow to demonstrate variables and context objects\n\nname: Variables and Context\n\n# Controls when the action will run. Workflow runs when manually triggered using the UI or API\non:\n  workflow_dispatch:\n    # Inputs the workflow accepts.\n    inputs:\n      name:\n        # Friendly description to be shown in the UI instead of 'name'\n        description: 'Person to greet'\n        # Default value if no value is explicitly provided\n        default: 'World'\n        # Input has to be provided for the workflow to run\n        required: true\n\nenv:\n  VAR1: myworkflowvar1\n  VAR2: myworkflowvar2\n  VAR3: myworkflowvar3\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n\n  job1:\n    runs-on: ubuntu-latest \n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJSON(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      \n  #step/job output variables\n  job2:\n    runs-on: ubuntu-latest\n    \n    outputs:\n      output1: ${{ steps.step1.outputs.step1value }}\n      output2: ${{ steps.step2.outputs.step2value }}\n    \n    steps:\n      - name: Step 1\n        id: step1\n        # run: echo \"::set-output name=step1value::hello\"\n        run: echo \"step1value=hello\" >> $GITHUB_OUTPUT\n\n      - name: Step 2\n        id: step2\n        # run: echo \"::set-output name=step2value::world\"\n        run: echo \"step2value=world\" >> $GITHUB_OUTPUT\n  \n  job3:\n    runs-on: ubuntu-latest\n    needs: job2\n    steps:\n      - run: echo ${{needs.job2.outputs.output1}} ${{needs.job2.outputs.output2}}\n\n  # access/set env and secrets \n  job4:\n    runs-on: ubuntu-latest\n    env:\n      VAR2: myjobvar2\n      VAR3: myjobvar3\n      SECRET: ${{ secrets.mySecret }}\n    steps:\n\n      - run: |\n          echo $VAR1\n          echo ${{env.VAR1}}\n\n          echo \"\"\n\n          echo $VAR2\n\n          echo $VAR3\n\n          echo $SECRET\n        env: \n          VAR3: mystepvar3\n",
    "source": "nampereira/desofs-tp04",
    "path": ".github/workflows/2-context.yaml",
    "url": "https://github.com/nampereira/desofs-tp04/blob/a901b8f0160c924cc14bc6013b432e8bed448453/.github/workflows/2-context.yaml",
    "retrieved_at": "2025-08-14T12:24:45.141505Z"
  },
  {
    "question": "How does the workflow ensure cache consistency across different Dart SDK versions and packages?",
    "answer": "# Created with package:mono_repo v6.6.3\nname: Dart CI\non:\n  push:\n    branches:\n      - main\n      - master\n  pull_request:\n  schedule:\n    - cron: \"0 0 * * 0\"\ndefaults:\n  run:\n    shell: bash\nenv:\n  PUB_ENVIRONMENT: bot.github\npermissions: read-all\n\njobs:\n  job_001:\n    name: mono_repo self validate\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:stable\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: stable\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - name: mono_repo self validate\n        run: dart pub global activate mono_repo 6.6.3\n      - name: mono_repo self validate\n        run: dart pub global run mono_repo generate --validate\n  job_002:\n    name: \"smoke_test; PKGS: app, pkg/_pub_shared, pkg/api_builder, pkg/fake_gcloud, pkg/pub_package_reader, pkg/web_app, pkg/web_css; `dart format --output=none --set-exit-if-changed .`, `dart analyze --fatal-infos  .`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app-pkg/_pub_shared-pkg/api_builder-pkg/fake_gcloud-pkg/pub_package_reader-pkg/web_app-pkg/web_css;commands:format-analyze_0\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app-pkg/_pub_shared-pkg/api_builder-pkg/fake_gcloud-pkg/pub_package_reader-pkg/web_app-pkg/web_css\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - id: pkg__pub_shared_pub_get\n        name: pkg/_pub_shared; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n      - name: \"pkg/_pub_shared; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg__pub_shared_pub_get.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n      - name: \"pkg/_pub_shared; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg__pub_shared_pub_get.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n      - id: pkg_api_builder_pub_get\n        name: pkg/api_builder; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/api_builder\n      - name: \"pkg/api_builder; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_api_builder_pub_get.conclusion == 'success'\"\n        working-directory: pkg/api_builder\n      - name: \"pkg/api_builder; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_api_builder_pub_get.conclusion == 'success'\"\n        working-directory: pkg/api_builder\n      - id: pkg_fake_gcloud_pub_get\n        name: pkg/fake_gcloud; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n      - name: \"pkg/fake_gcloud; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_fake_gcloud_pub_get.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n      - name: \"pkg/fake_gcloud; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_fake_gcloud_pub_get.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n      - id: pkg_pub_package_reader_pub_get\n        name: pkg/pub_package_reader; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n      - name: \"pkg/pub_package_reader; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_pub_package_reader_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n      - name: \"pkg/pub_package_reader; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_pub_package_reader_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n      - id: pkg_web_app_pub_get\n        name: pkg/web_app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - name: \"pkg/web_app; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_web_app_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - name: \"pkg/web_app; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_web_app_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - id: pkg_web_css_pub_get\n        name: pkg/web_css; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_css\n      - name: \"pkg/web_css; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_web_css_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_css\n      - name: \"pkg/web_css; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_web_css_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_css\n  job_003:\n    name: \"smoke_test; PKG: pkg/indexed_blob; `dart format --output=none --set-exit-if-changed .`, `dart analyze --fatal-infos .`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/indexed_blob;commands:format-analyze_1\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/indexed_blob\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_indexed_blob_pub_get\n        name: pkg/indexed_blob; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n      - name: \"pkg/indexed_blob; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_indexed_blob_pub_get.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n      - name: \"pkg/indexed_blob; dart analyze --fatal-infos .\"\n        run: dart analyze --fatal-infos .\n        if: \"always() && steps.pkg_indexed_blob_pub_get.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n  job_004:\n    name: \"smoke_test; PKG: pkg/pub_integration; `dart format --output=none --set-exit-if-changed .`, `dart analyze --fatal-infos lib/`, `dart analyze --fatal-infos test/`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:format-analyze_2-analyze_3\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart analyze --fatal-infos lib/\"\n        run: dart analyze --fatal-infos lib/\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart analyze --fatal-infos test/\"\n        run: dart analyze --fatal-infos test/\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n  job_005:\n    name: \"smoke_test; PKG: pkg/pub_worker; `dart format --output=none --set-exit-if-changed .`, `dart analyze --fatal-infos --fatal-warnings bin/ lib/`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker;commands:format-analyze_4\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_worker_pub_get\n        name: pkg/pub_worker; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart analyze --fatal-infos --fatal-warnings bin/ lib/\"\n        run: dart analyze --fatal-infos --fatal-warnings bin/ lib/\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n  job_006:\n    name: \"build; PKG: pkg/web_app; `./build.sh`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_app;commands:command_1\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_web_app_pub_get\n        name: pkg/web_app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - name: pkg/web_app; ./build.sh\n        run: ./build.sh\n        if: \"always() && steps.pkg_web_app_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n  job_007:\n    name: \"build; PKG: pkg/web_css; `./build.sh`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_css;commands:command_1\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_css\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_web_css_pub_get\n        name: pkg/web_css; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_css\n      - name: pkg/web_css; ./build.sh\n        run: ./build.sh\n        if: \"always() && steps.pkg_web_css_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_css\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n  job_008:\n    name: \"build_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P build-only -j 1`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_00\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P build-only -j 1\"\n        run: dart test -P build-only -j 1\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n  job_009:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 0`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_01\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 0\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 0\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_010:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 1`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_02\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 1\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 1\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_011:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 2`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_03\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 2\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 2\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_012:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 3`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_04\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 3\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 3\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_013:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 4`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_05\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 4\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 4\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_014:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 5`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_06\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 5\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 5\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_015:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 6`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_07\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 6\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 6\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_016:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 7`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_08\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 7\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 7\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_017:\n    name: \"unit_test; PKG: pkg/_pub_shared; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/_pub_shared;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/_pub_shared\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg__pub_shared_pub_get\n        name: pkg/_pub_shared; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n      - name: \"pkg/_pub_shared; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg__pub_shared_pub_get.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_018:\n    name: \"unit_test; PKG: pkg/fake_gcloud; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/fake_gcloud;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/fake_gcloud\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_fake_gcloud_pub_get\n        name: pkg/fake_gcloud; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n      - name: \"pkg/fake_gcloud; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_fake_gcloud_pub_get.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_019:\n    name: \"unit_test; PKG: pkg/indexed_blob; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/indexed_blob;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/indexed_blob\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_indexed_blob_pub_get\n        name: pkg/indexed_blob; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n      - name: \"pkg/indexed_blob; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_indexed_blob_pub_get.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_020:\n    name: \"unit_test; PKG: pkg/pub_package_reader; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_package_reader;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_package_reader\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_package_reader_pub_get\n        name: pkg/pub_package_reader; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n      - name: \"pkg/pub_package_reader; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_pub_package_reader_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_021:\n    name: \"unit_test; PKG: pkg/web_app; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_app;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_web_app_pub_get\n        name: pkg/web_app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - name: \"pkg/web_app; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_web_app_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_022:\n    name: \"unit_test; PKG: pkg/web_css; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_css;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_css\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_web_css_pub_get\n        name: pkg/web_css; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_css\n      - name: \"pkg/web_css; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_web_css_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_css\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_023:\n    name: \"unit_test; PKG: pkg/pub_integration; `dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~4p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:test_10\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~4p'`\"\n        run: \"dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~4p'`\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_024:\n    name: \"unit_test; PKG: pkg/pub_integration; `dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~4p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:test_11\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~4p'`\"\n        run: \"dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~4p'`\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_025:\n    name: \"unit_test; PKG: pkg/pub_integration; `dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~4p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:test_12\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~4p'`\"\n        run: \"dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~4p'`\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_026:\n    name: \"unit_test; PKG: pkg/pub_integration; `dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '3~4p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:test_13\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '3~4p'`\"\n        run: \"dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '3~4p'`\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_027:\n    name: \"unit_test; PKG: pkg/pub_worker; `dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~3p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker;commands:test_14\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_worker_pub_get\n        name: pkg/pub_worker; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~3p'`\"\n        run: \"dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~3p'`\"\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_028:\n    name: \"unit_test; PKG: pkg/pub_worker; `dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~3p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker;commands:test_15\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_worker_pub_get\n        name: pkg/pub_worker; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~3p'`\"\n        run: \"dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~3p'`\"\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_029:\n    name: \"unit_test; PKG: pkg/pub_worker; `dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~3p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker;commands:test_16\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_worker_pub_get\n        name: pkg/pub_worker; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~3p'`\"\n        run: \"dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~3p'`\"\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n",
    "source": "dart-lang/pub-dev",
    "path": ".github/workflows/dart.yml",
    "url": "https://github.com/dart-lang/pub-dev/blob/59af9a05fb70e699e7caa325d43f60649a3f431a/.github/workflows/dart.yml",
    "retrieved_at": "2025-08-14T12:24:50.273229Z"
  },
  {
    "question": "What i18n-related checks are performed by this workflow on push and pull requests to the main branch?",
    "answer": "name: Check i18n\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  RAILS_ENV: test\n\njobs:\n  check-i18n:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libicu-dev libidn11-dev libprotobuf-dev protobuf-compiler\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: '2.7'\n        bundler-cache: true\n    - name: Check locale file normalization\n      run: bundle exec i18n-tasks check-normalized\n    - name: Check for unused strings\n      run: bundle exec i18n-tasks unused -l en\n    - name: Check for wrong string interpolations\n      run: bundle exec i18n-tasks check-consistent-interpolations\n    - name: Check that all required locale files exist\n      run: bundle exec rake repo:check_locales_files\n",
    "source": "magicstone-dev/ecko",
    "path": ".github/workflows/check-i18n.yml",
    "url": "https://github.com/magicstone-dev/ecko/blob/a11982ed5341fb447ac3f65543488a014aa319d5/.github/workflows/check-i18n.yml",
    "retrieved_at": "2025-08-14T12:24:51.039771Z"
  },
  {
    "question": "What specific end-to-end tests are executed by this workflow?",
    "answer": "# Runs randomly generated E2E testnets nightly on the v0.37.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-37x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01', '02', '03', \"04\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"branch=`git branch --show-current`\" >> $GITHUB_OUTPUT\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner tests\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 5 -d networks/nightly/ -p\n\n      - name: Run ${{ matrix.p2p }} p2p testnets\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ needs.e2e-nightly-test.outputs.git-branch }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/e2e-nightly-37x.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/e2e-nightly-37x.yml",
    "retrieved_at": "2025-08-14T12:24:51.947805Z"
  },
  {
    "question": "Under what condition is the distribution artifact uploaded in this workflow?",
    "answer": "# This workflow will build a Java project with Gradle\n# For more information see: https://help.github.com/actions/language-and-framework-guides/building-and-testing-java-with-gradle\n\nname: Java CI\n\non: [ push, pull_request ]\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        env:\n          # see https://www.jetbrains.com/idea/download/previous.html\n          # and https://www.jetbrains.com/intellij-repository/snapshots/\n          - IDEA_VERSION: IC-2022.2.5\n          - IDEA_VERSION: IC-2022.3.1\n          - IDEA_VERSION: IC-2023.2.2\n          - IDEA_VERSION: IU-LATEST-EAP-SNAPSHOT\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up JDK 17\n        uses: actions/setup-java@v1\n        with:\n          java-version: 17\n      - name: Build with Gradle\n        run: |\n          ./gradlew -PideaVersion=${IDEA_VERSION} buildPlugin\n          echo \"DIST_FILE=$(find build/distributions/ -name 'jetbrains-plugin-st4-*.zip' -exec basename {} .zip \\;)\" >> $GITHUB_ENV\n        env: ${{ matrix.env }}\n      - name: Archive distribution artifact\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{env.DIST_FILE}}\n          path: build/distributions/jetbrains-plugin-st4-*.zip\n        if: matrix.env.IDEA_VERSION == 'IC-2022.2.5'\n",
    "source": "antlr/jetbrains-plugin-st4",
    "path": ".github/workflows/gradle.yml",
    "url": "https://github.com/antlr/jetbrains-plugin-st4/blob/1f8246e9d47f13c1e052ddd27232e8d4afaf67d9/.github/workflows/gradle.yml",
    "retrieved_at": "2025-08-14T12:24:53.834344Z"
  },
  {
    "question": "Does this workflow check if the code in a pull request branch is correctly formatted according to scalafmt, comparing it to the `origin/main` branch?",
    "answer": "name: Scalafmt\n\npermissions: read-all\n\non:\n  pull_request:\n    branches: ['**']\n\njobs:\n  build:\n    name: Code is formatted\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout current branch (full)\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n        with:\n          fetch-depth: 0\n          fetch-tags: true\n          persist-credentials: false\n\n      - name: Check project is formatted\n        uses: jrouly/scalafmt-native-action@v4\n        with:\n          arguments: '--list --mode diff-ref=origin/main'\n",
    "source": "apache/pekko-persistence-dynamodb",
    "path": ".github/workflows/format.yml",
    "url": "https://github.com/apache/pekko-persistence-dynamodb/blob/f88b6b9b20b4d5cec7285a5f049da220998f2dcb/.github/workflows/format.yml",
    "retrieved_at": "2025-08-14T12:24:55.489823Z"
  },
  {
    "question": "What different configurations are tested for KLEE on Linux, based on the matrix strategy?",
    "answer": "name: CI\n\non:\n  pull_request:\n    branches: master\n  push:\n    branches: master\n\n# Defaults for building KLEE\nenv:\n  BASE_IMAGE: ubuntu:bionic-20200807\n  REPOSITORY: klee\n  COVERAGE: 0\n  DISABLE_ASSERTIONS: 0\n  ENABLE_DOXYGEN: 0\n  ENABLE_OPTIMIZED: 1\n  ENABLE_DEBUG: 1\n  GTEST_VERSION: 1.7.0\n  KLEE_RUNTIME_BUILD: \"Debug+Asserts\"\n  LLVM_VERSION: 9\n  METASMT_VERSION: qf_abv\n  MINISAT_VERSION: \"master\"\n  REQUIRES_RTTI: 0\n  SANITIZER_BUILD:\n  SOLVERS: STP:Z3\n  STP_VERSION: 2.3.3\n  TCMALLOC_VERSION: 2.7\n  UCLIBC_VERSION: klee_uclibc_v1.2\n  USE_TCMALLOC: 1\n  USE_LIBCXX: 1\n  Z3_VERSION: 4.8.4\n\njobs:\n  Linux:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        name: [\n                \"LLVM 11\",\n                \"LLVM 10\",\n                \"LLVM 9\",\n                \"LLVM 8\",\n                \"LLVM 7\",\n                \"LLVM 6\",\n                \"LLVM 5\",\n                \"LLVM 4\",\n                \"LLVM 3.9\",\n                \"LLVM 3.8\",\n                \"ASan\",\n                \"UBSan\",\n                \"MSan\",\n                \"Z3 only\",\n                \"metaSMT STP\",\n                \"metaSMT Boolector\",\n                \"STP master\",\n                \"Latest klee-uclibc\",\n                \"Asserts enabled\",\n                \"No TCMalloc, optimised runtime\",\n            ]\n        include:\n          - name: \"LLVM 11\"\n            env:\n              LLVM_VERSION: 11\n          - name: \"LLVM 10\"\n            env:\n              LLVM_VERSION: 10\n          - name: \"LLVM 9\"\n            env:\n              LLVM_VERSION: 9\n          - name: \"LLVM 8\"\n            env:\n              LLVM_VERSION: 8\n          - name: \"LLVM 7\"\n            env:\n              LLVM_VERSION: 7\n          - name: \"LLVM 6\"\n            env:\n              LLVM_VERSION: 6\n          - name: \"LLVM 5\"\n            env:\n              LLVM_VERSION: 5\n          - name: \"LLVM 4\"\n            env:\n              LLVM_VERSION: 4\n          - name: \"LLVM 3.9\"\n            env:\n              LLVM_VERSION: 3.9\n          - name: \"LLVM 3.8\"\n            env:\n              LLVM_VERSION: 3.8\n              USE_LIBCXX: 0\n          # Sanitizer builds. Do unoptimized build otherwise the optimizer might remove problematic code\n          - name: \"ASan\"\n            env:\n              SANITIZER_BUILD: address\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n          - name: \"UBSan\"\n            env:\n              SANITIZER_BUILD: undefined\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n          - name: \"MSan\"\n            env:\n              SANITIZER_BUILD: memory\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n              SOLVERS: STP\n            # Test just using Z3 only\n          - name: \"Z3 only\"\n            env:\n              SOLVERS: Z3\n          # Test just using metaSMT\n          - name: \"metaSMT STP\"\n            env:\n              SOLVERS: metaSMT\n              METASMT_DEFAULT: STP\n              REQUIRES_RTTI: 1\n          - name: \"metaSMT Boolector\"\n            env:\n              SOLVERS: metaSMT\n              METASMT_DEFAULT: BTOR\n              REQUIRES_RTTI: 1\n          # Test we can build against STP master\n          - name: \"STP master\"\n            env:\n              SOLVERS: STP\n              STP_VERSION: master\n          # Check we can build latest klee-uclibc branch\n          - name: \"Latest klee-uclibc\"\n            env:\n              UCLIBC_VERSION: klee_0_9_29\n          # Check at least one build with Asserts disabled.\n          - name: \"Asserts enabled\"\n            env:\n              SOLVERS: STP\n              DISABLE_ASSERTIONS: 1\n          # Check without TCMALLOC and with an optimised runtime library\n          - name: \"No TCMalloc, optimised runtime\"\n            env:\n              USE_TCMALLOC: 0\n              KLEE_RUNTIME_BUILD: \"Release+Debug+Asserts\"\n    steps:\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        env: ${{ matrix.env }}\n        run: scripts/build/build.sh klee --docker --create-final-image\n      - name: Run tests\n        run: scripts/build/run-tests.sh --run-docker --debug\n\n  macOS:\n    runs-on: macos-latest\n    env:\n      BASE: /tmp\n      SOLVERS: STP\n      UCLIBC_VERSION: 0\n      USE_TCMALLOC: 0\n      USE_LIBCXX: 0\n    steps:\n      - name: Install newer version of Bash\n        run: brew install bash\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        run: scripts/build/build.sh klee --debug --install-system-deps\n      - name: Run tests\n        run: scripts/build/run-tests.sh /tmp/klee_build* --debug\n\n  Docker:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout KLEE Code\n        uses: actions/checkout@v2\n      - name: Build Docker image\n        run: docker build .\n\n  Coverage:\n    runs-on: ubuntu-latest\n    env:\n      ENABLE_OPTIMIZED: 0\n      COVERAGE: 1\n    steps:\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        run: scripts/build/build.sh klee --docker --create-final-image\n      - name: Run tests\n        run: scripts/build/run-tests.sh --coverage --upload-coverage --run-docker --debug\n",
    "source": "COMSYS/SymbolicLivenessAnalysis",
    "path": ".github/workflows/build.yaml",
    "url": "https://github.com/COMSYS/SymbolicLivenessAnalysis/blob/69af518cdb6cc4323f2a639e0a15a265552e9df5/.github/workflows/build.yaml",
    "retrieved_at": "2025-08-14T12:24:56.586759Z"
  },
  {
    "question": "Under what conditions does the workflow publish the created NuGet package to NuGet.org?",
    "answer": "name: .NET Core\non:\n  push:\n  workflow_dispatch:\n  release:\n    types: [published]\n\nenv:\n  DEFAULT_VERSION: \"3.1.0.4-alpha\"\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup .NET Core 2.2\n        uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: 2.2.x\n      - name: Setup .NET Core 3.1\n        uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: 3.1.x\n      - name: Setup .NET 6\n        uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: 6.0.x\n      - name: Setup .NET 8\n        uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: 8.0.x\n      - name: Build with dotnet\n        run: dotnet build --configuration Release /p:ContinuousIntegrationBuild=true\n      - name: Test\n        run: dotnet test src/PostalCodes.UnitTests/PostalCodes.UnitTests.csproj\n      - name: Extract Version from Release Tag\n        id: get_version\n        run: echo \"VERSION=$(echo '${{ github.event.release.tag_name || env.DEFAULT_VERSION }}' | sed -e 's/^v//')\" >> $GITHUB_ENV\n      - name: Create nuget package\n        run: dotnet pack . -p:PackageVersion=${{ env.VERSION }} -o out --no-build\n      - name: Install dotnet-validate\n        run: dotnet tool install --global dotnet-validate --version 0.0.1-preview.304\n\n      - name: Validate NuGet package\n        run: dotnet-validate package local out/*.nupkg\n\n      - name: Publish\n        if: github.event_name == 'release'\n        run: |\n          dotnet nuget push out/PostalCodes.${{ env.VERSION }}.nupkg -k ${{ secrets.NUGET_API_KEY }} -s https://api.nuget.org/v3/index.json\n",
    "source": "Cimpress-MCP/PostalCodes.Net",
    "path": ".github/workflows/dotnetcore.yml",
    "url": "https://github.com/Cimpress-MCP/PostalCodes.Net/blob/1dea354096b5858e6584b8d233db1a88456a02f3/.github/workflows/dotnetcore.yml",
    "retrieved_at": "2025-08-14T12:24:57.771476Z"
  },
  {
    "question": "What specific tests are executed by the `npm test` command in this workflow?",
    "answer": "name: Tests\n\non: [push, pull_request]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@master\n      - name: Use Node.js 12.x\n        uses: actions/setup-node@v1\n        with:\n          node-version: 12.x\n      - name: npm install and test\n        run: |\n          npm install\n          npm test\n",
    "source": "nervetattoo/banner-card",
    "path": ".github/workflows/nodejs.yml",
    "url": "https://github.com/nervetattoo/banner-card/blob/554c77c4fca5019896d336492b81b1fc56f9a281/.github/workflows/nodejs.yml",
    "retrieved_at": "2025-08-14T12:24:58.621683Z"
  },
  {
    "question": "How does the `metadata2gha` command in the `setup_matrix` job determine the test matrix configurations?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "scibian/puppet-module-puppet-archive",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/scibian/puppet-module-puppet-archive/blob/56ac175624cd57ebd4245348165ea97895f5fd1c/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-14T12:24:59.444331Z"
  },
  {
    "question": "What specific files or directories are excluded from triggering the workflow on push and pull requests?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "DeepLink-org/ParrotsDL-mmpose",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/DeepLink-org/ParrotsDL-mmpose/blob/5734804f05fdcc8849bfee34d79f8bd47b9a9fdb/.github/workflows/build.yml",
    "retrieved_at": "2025-08-14T12:50:06.774488Z"
  },
  {
    "question": "Under what conditions does the \"prepare\" job not run, and how does that impact the \"release\" job?",
    "answer": "name: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  prepare:\n    runs-on: ubuntu-latest\n    if: \"! contains(github.event.head_commit.message, '[skip ci]')\"\n    steps:\n      - run: echo \"${{ github.event.head_commit.message }}\"\n  release:\n    needs: prepare\n    name: Release\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          persist-credentials: false\n      - name: Setup Node.js\n        uses: actions/setup-node@v1\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: npm ci\n      - name: Release\n        env:\n          GIT_AUTHOR_EMAIL: ${{ secrets.GIT_AUTHOR_EMAIL }}\n          GIT_AUTHOR_NAME: ${{ secrets.GIT_AUTHOR_NAME }}\n          GIT_COMMITTER_EMAIL: ${{ secrets.GIT_COMMITTER_EMAIL }}\n          GIT_COMMITTER_NAME: ${{ secrets.GIT_COMMITTER_NAME }}\n          GITHUB_TOKEN: ${{ secrets.PA_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n",
    "source": "americanexpress/jest-json-schema",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/americanexpress/jest-json-schema/blob/5bc73a66ea7e1b6164923c69d12103a36dfdd21c/.github/workflows/release.yml",
    "retrieved_at": "2025-08-14T12:50:07.442808Z"
  },
  {
    "question": "What actions are performed to lint the code and report the results when a pull request targets the main branch?",
    "answer": "name: Lint\non:\n  pull_request_target:\n    branches:\n      - main\njobs:\n  lint:\n    strategy:\n      matrix:\n        node: [\"14.x\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v2\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n          fetch-depth: 2\n\n      - name: Use Node.js 14.x\n        uses: actions/setup-node@v2\n        with:\n          node-version: ${{ matrix.node }}\n          # cache: \"yarn\"\n          # cache-dependency-path: yarn.lock\n\n      - name: Install deps\n        if: steps.yarn-cache.outputs.cache-hit != 'true'\n        run: yarn\n\n      - name: Lint\n        run: yarn lint:report\n        continue-on-error: true\n\n      - name: Merge lint reports\n        run: jq -s '[.[]]|flatten' lint-results/*.json &> lint-results/eslint_report.json\n\n      - name: Annotate Code Linting Results\n        uses: ataylorme/eslint-annotate-action@1.2.0\n        with:\n          repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          report-json: \"lint-results/eslint_report.json\"\n\n      - name: Upload ESLint report\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: lint-results\n          path: lint-results\n",
    "source": "meruhealth/calendso",
    "path": ".github/workflows/lint.yml",
    "url": "https://github.com/meruhealth/calendso/blob/d88af28e441320b9d1e32babb2bd9d11bdb0834d/.github/workflows/lint.yml",
    "retrieved_at": "2025-08-14T12:50:08.214848Z"
  },
  {
    "question": "What triggers this workflow, and what distinguishes those triggers?",
    "answer": "name: 'build and deploy Speckle functions'\non:\n  workflow_dispatch:\n  push:\n    tags:\n      - '*'\n\njobs:\n  publish-automate-function-version: # make sure the action works on a clean machine without building\n    env:\n      FUNCTION_SCHEMA_FILE_NAME: functionSchema.json\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.1.1\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install and configure Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: 1.3.2\n          virtualenvs-create: false\n          virtualenvs-in-project: false\n          installer-parallel: true\n      - name: Restore dependencies\n        run: poetry install --no-root\n      - name: Extract functionInputSchema\n        id: extract_schema\n        run: |\n          python main.py generate_schema ${HOME}/${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n      - name: Speckle Automate Function - Build and Publish\n        uses: specklesystems/speckle-automate-github-composite-action@0.8.0\n        with:\n          speckle_automate_url: ${{ env.SPECKLE_AUTOMATE_URL || 'https://automate.speckle.dev' }} \n          speckle_token: ${{ secrets.SPECKLE_FUNCTION_TOKEN }}\n          speckle_function_id: ${{ secrets.SPECKLE_FUNCTION_ID }}\n          speckle_function_input_schema_file_path: ${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n          speckle_function_command: 'python -u main.py run'\n",
    "source": "specklesystems/Mott-mac-demo",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/specklesystems/Mott-mac-demo/blob/d08b92f1948bf28ab8a39f2c3b2c75e56bf297fc/.github/workflows/main.yml",
    "retrieved_at": "2025-08-14T12:50:08.879430Z"
  },
  {
    "question": "What happens when a push event triggers this workflow with a tag?",
    "answer": "---\nname: release\n\non:\n  push:\n    tags: [\"*\"]\n\npermissions:\n  id-token: write\n  contents: write\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out code\n        uses: actions/checkout@v2\n\n      - name: version\n        run: echo \"version=$(/usr/bin/basename ${{ github.ref }})\" >> $GITHUB_OUTPUT\n        id: version\n\n      - name: release\n        uses: actions/create-release@v1\n        id: create_release\n        with:\n          draft: false\n          prerelease: false\n          release_name: ${{ steps.version.outputs.version }}\n          tag_name: ${{ github.ref }}\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n\n      - name: Package\n        uses: a7ul/tar-action@v1.1.2\n        with:\n          files: .\n          command: c\n          outPath: ${{ steps.version.outputs.version }}.tar.gz\n\n      - name: Sign release with Sigstore\n        continue-on-error: true\n        uses: sigstore/gh-action-sigstore-python@v0.0.9\n        with:\n          inputs: ${{ steps.version.outputs.version }}.tar.gz\n          release-signing-artifacts: true\n          upload-signing-artifacts: true\n\n      - name: upload signed asset\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz\n          asset_content_type: application/gzip\n\n      - name: upload sigstore certificate\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz.crt\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz.crt\n          asset_content_type: application/x-x509-ca-cert\n\n      - name: upload sigstore signature\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz.sig\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz.sig\n          asset_content_type: application/octet-stream\n\n      - name: Build and Deploy Collection\n        uses: 0x022b/galaxy-role-import-action@1.0.0\n        with:\n          galaxy_api_key: \"${{ secrets.ANSIBLE_GALAXY_TOKEN }}\"\n",
    "source": "ipr-cnrs/glpi-agent",
    "path": ".github/workflows/release.yaml",
    "url": "https://github.com/ipr-cnrs/glpi-agent/blob/55a7d8b68c8dee8b30b2c2f65fbc9d66ad7eabba/.github/workflows/release.yaml",
    "retrieved_at": "2025-08-14T12:50:09.700604Z"
  },
  {
    "question": "What automated code formatting and fixing tasks are executed by the `autofix` job in this workflow?",
    "answer": "name: autofix.ci # needed to securely identify the workflow\n\non:\n  pull_request:\n  push:\n    branches: [\"main\"]\n\npermissions:\n  contents: read\n\njobs:\n  autofix:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: pnpm/action-setup@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: 20\n          cache: \"pnpm\"\n      - run: pnpm install\n      - run: pnpm automd\n      - run: pnpm lint:fix\n      - uses: autofix-ci/action@635ffb0c9798bd160680f18fd73371e355b85f27\n        with:\n          commit-message: \"chore: apply automated updates\"\n",
    "source": "unjs/runtime-compat",
    "path": ".github/workflows/autofix.yml",
    "url": "https://github.com/unjs/runtime-compat/blob/9a2246b51cb25292975a24619394bc2c1754a445/.github/workflows/autofix.yml",
    "retrieved_at": "2025-08-14T12:50:10.490003Z"
  },
  {
    "question": "What specific compliance checks are performed by the `check_compliance` job, and how are the results reported?",
    "answer": "name: Compliance Checks\n\non: pull_request\n\njobs:\n  maintainer_check:\n    runs-on: ubuntu-latest\n    name: Check MAINTAINERS file\n    steps:\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n    - name: Run Maintainers Script\n      id: maintainer\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        python3 ./scripts/get_maintainer.py path CMakeLists.txt\n\n  check_compliance:\n    runs-on: ubuntu-latest\n    name: Run compliance checks on patch series (PR)\n    steps:\n    - name: Update PATH for west\n      run: |\n        echo \"$HOME/.local/bin\" >> $GITHUB_PATH\n\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n\n    - name: cache-pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-doc-pip\n\n    - name: Install python dependencies\n      run: |\n        pip3 install setuptools\n        pip3 install wheel\n        pip3 install python-magic junitparser==1.6.3 gitlint pylint pykwalify\n        pip3 install west\n\n    - name: west setup\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        git config --global user.email \"you@example.com\"\n        git config --global user.name \"Your Name\"\n        git remote -v\n        git rebase origin/${BASE_REF}\n        # debug\n        git log  --pretty=oneline | head -n 10\n        west init -l . || true\n        west update 2>&1 1> west.update.log || west update 2>&1 1> west.update2.log\n\n    - name: Run Compliance Tests\n      continue-on-error: true\n      id: compliance\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        export ZEPHYR_BASE=$PWD\n        # debug\n        ls -la\n        git log  --pretty=oneline | head -n 10\n        ./scripts/ci/check_compliance.py -m Devicetree -m Gitlint -m Identity -m Nits -m pylint -m checkpatch -m Kconfig -c origin/${BASE_REF}..\n\n    - name: upload-results\n      uses: actions/upload-artifact@master\n      continue-on-error: True\n      with:\n        name: compliance.xml\n        path: compliance.xml\n\n    - name: check-warns\n      run: |\n        if [[ ! -s \"compliance.xml\" ]]; then\n          exit 1;\n        fi\n\n        for file in Nits.txt checkpatch.txt Identity.txt Gitlint.txt pylint.txt Devicetree.txt Kconfig.txt; do\n          if [[ -s $file ]]; then\n            errors=$(cat $file)\n            errors=\"${errors//'%'/'%25'}\"\n            errors=\"${errors//$'\\n'/'%0A'}\"\n            errors=\"${errors//$'\\r'/'%0D'}\"\n            echo \"::error file=${file}::$errors\"\n            exit=1\n          fi\n        done\n\n        if [ \"${exit}\" == \"1\" ]; then\n          exit 1;\n        fi\n",
    "source": "sofarocean/zephyr",
    "path": ".github/workflows/compliance.yml",
    "url": "https://github.com/sofarocean/zephyr/blob/429954f467b388b839c8bfedc51f39a5fb3609d1/.github/workflows/compliance.yml",
    "retrieved_at": "2025-08-14T12:50:11.384065Z"
  },
  {
    "question": "Under what conditions will the \"bulk-import\" job actually run, given the `if` condition?",
    "answer": "name: \"bulk quest import\"\non:\n  schedule:\n    - cron: '0 10 * * *' # UTC time, that's 5:00 am EST, 2:00 am PST.\n  workflow_dispatch:\n    inputs:\n      reason:\n        description: \"The reason for running the bulk import workflow\"\n        required: true\n        default: \"Initial import into Quest (Azure DevOps)\"\n\njobs:\n  bulk-import:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n    if: ${{ github.repository_owner == 'dotnet' }}\n\n    steps:\n      - name: \"Print manual bulk import run reason\"\n        if: ${{ github.event_name == 'workflow_dispatch' }}\n        run: |\n          echo \"Reason: ${{ github.event.inputs.reason }}\"\n\n      - name: bulk-sequester\n        id: bulk-sequester\n        uses: dotnet/docs-tools/actions/sequester@main\n        env:\n          ImportOptions__ApiKeys__GitHubToken: ${{ secrets.GITHUB_TOKEN }}\n          ImportOptions__ApiKeys__OSPOKey: ${{ secrets.OSPO_KEY }}\n          ImportOptions__ApiKeys__QuestKey: ${{ secrets.QUEST_KEY }}\n        with:\n          org: ${{ github.repository_owner }}\n          repo: ${{ github.repository }}\n          issue: '-1'\n",
    "source": "void0620/ASP.NET-Core",
    "path": ".github/workflows/quest-bulk.yml",
    "url": "https://github.com/void0620/ASP.NET-Core/blob/d56a806f9d8c99ba375db558c49a38faf7aa09ec/.github/workflows/quest-bulk.yml",
    "retrieved_at": "2025-08-14T12:50:12.324009Z"
  },
  {
    "question": "For the Windows MSVC builds, what environment variables are being set using the Visual Studio batch script?",
    "answer": "name: range-v3 CI\n\n# Trigger on pushes to all branches and for all pull-requests\non: [push, pull_request]\n\nenv:\n  CMAKE_VERSION: 3.16.2\n  NINJA_VERSION: 1.9.0\n\njobs:\n  build:\n    name: ${{ matrix.config.name }}\n    runs-on: ${{ matrix.config.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        config:\n        # GCC-6\n        - {\n            name: \"Linux GCC 6 Debug (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 6 Release (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 6 Debug (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 6 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 6 Release (C++17, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 17,\n          }\n\n        # GCC-7\n        - {\n            name: \"Linux GCC 7 Debug (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 7 Release (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 7 Debug (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 7 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 7 Release (C++17, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 17,\n          }\n\n        # GCC-8\n        - {\n            name: \"Linux GCC 8 Debug (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 8 Release (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 8 Debug (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 8 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 8 Release (C++17, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 17,\n          }\n\n        # GCC-9\n        - {\n            name: \"Linux GCC 9 Debug (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 9 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 9 Debug (C++20)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 20,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 9 Release (C++20)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 20,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 9 Release (C++20, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 20,\n          }\n\n        # GCC-10\n        - {\n            name: \"Linux GCC 10 Debug (C++20, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"gcc-10\", cxx: \"g++-10\",\n            cxx_standard: 20\n          }\n        - {\n            name: \"Linux GCC 10 Release (C++20, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-10\", cxx: \"g++-10\",\n            cxx_standard: 20\n          }\n\n        # Clang-5.0\n        - {\n            name: \"Linux Clang 5.0 Debug (C++14 / libc++ / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"clang-5.0\", cxx: \"clang++-5.0\",\n            cxx_standard: 14,\n            cxx_asan: true,\n            libcxx: true\n          }\n        - {\n            name: \"Linux Clang 5.0 Debug (C++17 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"clang-5.0\", cxx: \"clang++-5.0\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n\n        # Clang-6.0\n        - {\n            name: \"Linux Clang 6.0 Debug (C++14 / libc++ / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"clang-6.0\", cxx: \"clang++-6.0\",\n            cxx_standard: 14,\n            cxx_asan: true,\n            libcxx: true\n          }\n        - {\n            name: \"Linux Clang 6.0 Debug (C++17 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"clang-6.0\", cxx: \"clang++-6.0\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n\n        # Clang-8\n        - {\n            name: \"Linux Clang 8 Debug (C++14 / libc++ / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"clang-8\", cxx: \"clang++-8\",\n            cxx_standard: 14,\n            cxx_asan: true,\n            libcxx: true\n          }\n        - {\n            name: \"Linux Clang 8 Debug (C++17 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"clang-8\", cxx: \"clang++-8\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n\n        # Clang-9\n        - {\n            name: \"Linux Clang 9 Debug (C++17 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"clang-9\", cxx: \"clang++-9\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n        - {\n            name: \"Linux Clang 9 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"clang-9\", cxx: \"clang++-9\",\n            cxx_standard: 17,\n          }\n\n        # Clang-10\n        - {\n            name: \"Linux Clang 10 Debug (C++20 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"clang-10\", cxx: \"clang++-10\",\n            cxx_standard: 20,\n            cxx_asan: true,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux Clang 10 Release (C++20 / Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"clang-10\", cxx: \"clang++-10\",\n            cxx_standard: 20,\n          }\n\n        # AppleClang\n        - {\n            name: \"macOS Clang Debug (C++17)\", artifact: \"macOS.tar.xz\",\n            os: macos-latest,\n            build_type: Debug,\n            cc: \"clang\", cxx: \"clang++\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n        - {\n            name: \"macOS Clang Release (C++17)\", artifact: \"macOS.tar.xz\",\n            os: macos-latest,\n            build_type: RelWithDebInfo,\n            cc: \"clang\", cxx: \"clang++\",\n            cxx_standard: 17,\n          }\n        - {\n            name: \"macOS Clang Debug (C++20 / ASAN)\", artifact: \"macOS.tar.xz\",\n            os: macos-latest,\n            build_type: Debug,\n            cc: \"clang\", cxx: \"clang++\",\n            cxx_standard: 20,\n            cxx_asan: true,\n          }\n        - {\n            name: \"macOS Clang Release (C++20)\", artifact: \"macOS.tar.xz\",\n            os: macos-latest,\n            build_type: RelWithDebInfo,\n            cc: \"clang\", cxx: \"clang++\",\n            cxx_standard: 20,\n          }\n\n        # MSVC 2022\n        - {\n            name: \"Windows MSVC 2022 Debug (C++17)\", artifact: \"Windows-MSVC.tar.xz\",\n            os: windows-latest,\n            build_type: Debug,\n            cc: \"cl\", cxx: \"cl\",\n            environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n            cxx_standard: 17,\n          }\n        - {\n            name: \"Windows MSVC 2022 Release (C++17)\", artifact: \"Windows-MSVC.tar.xz\",\n            os: windows-latest,\n            build_type: RelWithDebInfo,\n            cc: \"cl\", cxx: \"cl\",\n            environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n            cxx_standard: 17,\n          }\n        - {\n            name: \"Windows MSVC 2022 Debug (C++20)\", artifact: \"Windows-MSVC.tar.xz\",\n            os: windows-latest,\n            build_type: Debug,\n            cc: \"cl\", cxx: \"cl\",\n            environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n            cxx_standard: 20,\n          }\n        - {\n            name: \"Windows MSVC 2022 Release (C++20)\", artifact: \"Windows-MSVC.tar.xz\",\n            os: windows-latest,\n            build_type: RelWithDebInfo,\n            cc: \"cl\", cxx: \"cl\",\n            environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n            cxx_standard: 20,\n          }\n\n    steps:\n    - uses: actions/checkout@v1\n\n    - name: Download Ninja and CMake\n      id: cmake_and_ninja\n      shell: cmake -P {0}\n      run: |\n        set(cmake_version $ENV{CMAKE_VERSION})\n        set(ninja_version $ENV{NINJA_VERSION})\n\n        message(STATUS \"Using host CMake version: ${CMAKE_VERSION}\")\n\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n          set(ninja_suffix \"win.zip\")\n          set(cmake_suffix \"win64-x64.zip\")\n          set(cmake_dir \"cmake-${cmake_version}-win64-x64/bin\")\n        elseif (\"${{ runner.os }}\" STREQUAL \"Linux\")\n          set(ninja_suffix \"linux.zip\")\n          set(cmake_suffix \"Linux-x86_64.tar.gz\")\n          set(cmake_dir \"cmake-${cmake_version}-Linux-x86_64/bin\")\n        elseif (\"${{ runner.os }}\" STREQUAL \"macOS\")\n          set(ninja_suffix \"mac.zip\")\n          set(cmake_suffix \"Darwin-x86_64.tar.gz\")\n          set(cmake_dir \"cmake-${cmake_version}-Darwin-x86_64/CMake.app/Contents/bin\")\n        endif()\n\n        set(ninja_url \"https://github.com/ninja-build/ninja/releases/download/v${ninja_version}/ninja-${ninja_suffix}\")\n        file(DOWNLOAD \"${ninja_url}\" ./ninja.zip SHOW_PROGRESS)\n        execute_process(COMMAND ${CMAKE_COMMAND} -E tar xvf ./ninja.zip)\n\n        set(cmake_url \"https://github.com/Kitware/CMake/releases/download/v${cmake_version}/cmake-${cmake_version}-${cmake_suffix}\")\n        file(DOWNLOAD \"${cmake_url}\" ./cmake.zip SHOW_PROGRESS)\n        execute_process(COMMAND ${CMAKE_COMMAND} -E tar xvf ./cmake.zip)\n\n        # Save the path for other steps\n        file(TO_CMAKE_PATH \"$ENV{GITHUB_WORKSPACE}/${cmake_dir}\" cmake_dir)\n        message(\"::set-output name=cmake_dir::${cmake_dir}\")\n\n        if (NOT \"${{ runner.os }}\" STREQUAL \"Windows\")\n          execute_process(\n            COMMAND chmod +x ninja\n            COMMAND chmod +x ${cmake_dir}/cmake\n          )\n        endif()\n\n    - name: Install GCC or Clang\n      id: install_gcc_clang\n      if: startsWith(matrix.config.os, 'ubuntu')\n      shell: bash\n      working-directory: ${{ env.HOME }}\n      run: |\n        sudo apt-get install -y ${{matrix.config.cc}} ${{matrix.config.cxx}}\n\n    - name: Install libc++\n      id: install_libcxx\n      if: matrix.config.libcxx\n      shell: bash\n      working-directory: ${{ env.HOME }}\n      env:\n        CC: ${{ matrix.config.cc }}\n        CXX: ${{ matrix.config.cxx }}\n      run: |\n        $GITHUB_WORKSPACE/install_libcxx.sh\n\n    - name: Configure\n      shell: cmake -P {0}\n      run: |\n        set(ENV{CC} ${{ matrix.config.cc }})\n        set(ENV{CXX} ${{ matrix.config.cxx }})\n\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\" AND NOT \"x${{ matrix.config.environment_script }}\" STREQUAL \"x\")\n          execute_process(\n            COMMAND \"${{ matrix.config.environment_script }}\" && set\n            OUTPUT_FILE environment_script_output.txt\n          )\n          set(cxx_flags \"/permissive- /EHsc\")\n          file(STRINGS environment_script_output.txt output_lines)\n          foreach(line IN LISTS output_lines)\n            if (line MATCHES \"^([a-zA-Z0-9_-]+)=(.*)$\")\n              set(ENV{${CMAKE_MATCH_1}} \"${CMAKE_MATCH_2}\")\n            endif()\n          endforeach()\n        endif()\n\n        set(path_separator \":\")\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n          set(path_separator \";\")\n        endif()\n        set(ENV{PATH} \"$ENV{GITHUB_WORKSPACE}${path_separator}$ENV{PATH}\")\n\n        if (\"x${{ matrix.config.libcxx }}\" STREQUAL \"xtrue\")\n          set(cxx_flags \"${cxx_flags} -stdlib=libc++ -nostdinc++ -cxx-isystem $ENV{GITHUB_WORKSPACE}/llvm/include/c++/v1/ -Wno-unused-command-line-argument\")\n          set(link_flags \"${link_flags} -L $ENV{GITHUB_WORKSPACE}/llvm/lib -Wl,-rpath,$ENV{GITHUB_WORKSPACE}/llvm/lib -lc++abi\")\n        endif()\n\n        if (\"${{ matrix.config.cxx }}\" MATCHES \"clang.*\")\n            # clang spurious warning on <=> use\n            set(cxx_flags \"${cxx_flags} -Wno-zero-as-null-pointer-constant\")\n        endif()\n\n        if (\"x${{ matrix.config.cxx_asan }}\" STREQUAL \"xtrue\")\n          set(cxx_flags \"${cxx_flags} -fsanitize=address -fno-omit-frame-pointer\")\n        endif()\n\n        set(cxx_concepts ON)\n        if (\"x${{ matrix.config.cxx_concepts }}\" STREQUAL \"xfalse\")\n          set(cxx_concepts OFF)\n        endif()\n\n        execute_process(\n          COMMAND ${{ steps.cmake_and_ninja.outputs.cmake_dir }}/cmake\n            -S .\n            -B build\n            -G Ninja\n            -D CMAKE_BUILD_TYPE=${{ matrix.config.build_type }}\n            -D CMAKE_MAKE_PROGRAM:STRING=ninja\n            -D CMAKE_CXX_STANDARD:STRING=${{ matrix.config.cxx_standard }}\n            -D \"CMAKE_CXX_FLAGS:STRING=${cxx_flags}\"\n            -D \"CMAKE_EXE_LINKER_FLAGS:STRING=${link_flags}\"\n            -D CMAKE_VERBOSE_MAKEFILE:BOOL=ON\n            -D RANGE_V3_HEADER_CHECKS:BOOL=ON\n            -D RANGES_PREFER_REAL_CONCEPTS:BOOL=${cxx_concepts}\n            ${{ matrix.config.cmake_args }}\n            ${extra_cmake_args}\n          RESULT_VARIABLE result\n        )\n        if (NOT result EQUAL 0)\n          message(FATAL_ERROR \"Bad exit status\")\n        endif()\n\n    - name: Build\n      shell: cmake -P {0}\n      continue-on-error: ${{ matrix.config.experimental || false }}\n      run: |\n        set(ENV{NINJA_STATUS} \"[%f/%t %o/sec] \")\n\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\" AND NOT \"x${{ matrix.config.environment_script }}\" STREQUAL \"x\")\n          file(STRINGS environment_script_output.txt output_lines)\n          foreach(line IN LISTS output_lines)\n            if (line MATCHES \"^([a-zA-Z0-9_-]+)=(.*)$\")\n              set(ENV{${CMAKE_MATCH_1}} \"${CMAKE_MATCH_2}\")\n            endif()\n          endforeach()\n        endif()\n\n        set(path_separator \":\")\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n          set(path_separator \";\")\n        endif()\n        set(ENV{PATH} \"$ENV{GITHUB_WORKSPACE}${path_separator}$ENV{PATH}\")\n\n        execute_process(\n          COMMAND ${{ steps.cmake_and_ninja.outputs.cmake_dir }}/cmake --build build\n          RESULT_VARIABLE result\n        )\n        if (NOT result EQUAL 0)\n          message(FATAL_ERROR \"Bad exit status\")\n        endif()\n\n    - name: Run tests\n      shell: cmake -P {0}\n      continue-on-error: ${{ matrix.config.experimental || false }}\n      run: |\n        include(ProcessorCount)\n        ProcessorCount(N)\n\n        set(ENV{CTEST_OUTPUT_ON_FAILURE} \"ON\")\n\n        execute_process(\n          COMMAND ${{ steps.cmake_and_ninja.outputs.cmake_dir }}/ctest --verbose -j ${N}\n          WORKING_DIRECTORY build\n          RESULT_VARIABLE result\n        )\n        if (NOT result EQUAL 0)\n          message(FATAL_ERROR \"Running tests failed!\")\n        endif()\n",
    "source": "MrE-Fog/range-v3",
    "path": ".github/workflows/range-v3-ci.yml",
    "url": "https://github.com/MrE-Fog/range-v3/blob/689b4f3da769fb21dd7acf62550a038242d832e5/.github/workflows/range-v3-ci.yml",
    "retrieved_at": "2025-08-14T12:50:13.870317Z"
  },
  {
    "question": "How does the `concurrency` group ensure only one workflow runs at a time for the same pull request or branch?",
    "answer": "name: trunk\n\non:\n  push:\n    branches:\n      - main\n      - release/*\n      - landchecks/*\n    tags:\n      - ciflow/trunk/*\n  workflow_dispatch:\n  schedule:\n    - cron: 29 8 * * *  # about 1:29am PDT\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref_name }}-${{ github.ref_type == 'branch' && github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}\n  cancel-in-progress: true\n\npermissions: read-all\n\njobs:\n  target-determination:\n    name: before-test\n    uses: ./.github/workflows/target_determination.yml\n    permissions:\n      id-token: write\n      contents: read\n\n  # Build PyTorch with BUILD_CAFFE2=ON\n  caffe2-linux-jammy-py3_8-gcc11-build:\n    name: caffe2-linux-jammy-py3.8-gcc11\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: caffe2-linux-jammy-py3.8-gcc11\n      docker-image-name: pytorch-linux-jammy-py3.8-gcc11\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1 },\n        ]}\n\n  linux-focal-cuda12_1-py3_10-gcc9-build:\n    name: linux-focal-cuda12.1-py3.10-gcc9\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      test-matrix: |\n        { include: [\n          { config: \"nogpu_AVX512\", shard: 1, num_shards: 1, runner: \"linux.2xlarge\" },\n          { config: \"nogpu_NO_AVX2\", shard: 1, num_shards: 1, runner: \"linux.2xlarge\" },\n          { config: \"jit_legacy\", shard: 1, num_shards: 1, runner: \"linux.4xlarge.nvidia.gpu\" },\n        ]}\n\n  linux-focal-cuda12_1-py3_10-gcc9-test:\n    name: linux-focal-cuda12.1-py3.10-gcc9\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-focal-cuda12_1-py3_10-gcc9-build\n      - target-determination\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9\n      docker-image: ${{ needs.linux-focal-cuda12_1-py3_10-gcc9-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-cuda12_1-py3_10-gcc9-build.outputs.test-matrix }}\n\n  libtorch-linux-focal-cuda12_1-py3_7-gcc9-debug-build:\n    name: libtorch-linux-focal-cuda12.1-py3.7-gcc9-debug\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: libtorch-linux-focal-cuda12.1-py3.7-gcc9\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      build-generates-artifacts: false\n      runner: linux.4xlarge\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1 },\n        ]}\n\n  # no-ops builds test USE_PER_OPERATOR_HEADERS=0 where ATen/ops is not generated\n  linux-focal-cuda12_1-py3_10-gcc9-no-ops-build:\n    name: linux-focal-cuda12.1-py3.10-gcc9-no-ops\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9-no-ops\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1 },\n        ]}\n\n  pytorch-linux-focal-py3-clang9-android-ndk-r21e-build:\n    name: pytorch-linux-focal-py3-clang9-android-ndk-r21e-build\n    uses: ./.github/workflows/_android-full-build-test.yml\n    with:\n      build-environment: pytorch-linux-focal-py3-clang9-android-ndk-r21e-build\n      docker-image-name: pytorch-linux-focal-py3-clang9-android-ndk-r21e\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1, runner: \"linux.2xlarge\" },\n        ]}\n\n  macos-12-py3-arm64-build:\n    name: macos-12-py3-arm64\n    uses: ./.github/workflows/_mac-build.yml\n    with:\n      sync-tag: macos-12-py3-arm64-build\n      build-environment: macos-12-py3-arm64\n      runner-type: macos-m1-stable\n      build-generates-artifacts: true\n      # To match the one pre-installed in the m1 runners\n      python-version: 3.9.12\n      # We need to set the environment file here instead of trying to detect it automatically because\n      # MacOS arm64 is cross-compiled from x86-64. Specifically, it means that arm64 conda environment\n      # is needed when building PyTorch MacOS arm64 from x86-64\n      environment-file: .github/requirements/conda-env-macOS-ARM64\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 3, runner: \"macos-m1-stable\" },\n          { config: \"default\", shard: 2, num_shards: 3, runner: \"macos-m1-stable\" },\n          { config: \"default\", shard: 3, num_shards: 3, runner: \"macos-m1-stable\" },\n        ]}\n\n  macos-12-py3-arm64-mps-test:\n    name: macos-12-py3-arm64-mps\n    uses: ./.github/workflows/_mac-test-mps.yml\n    needs: macos-12-py3-arm64-build\n    if: needs.macos-12-py3-arm64-build.outputs.build-outcome == 'success'\n    with:\n      sync-tag: macos-12-py3-arm64-mps-test\n      build-environment: macos-12-py3-arm64\n      # Same as the build job\n      python-version: 3.9.12\n      test-matrix: |\n        { include: [\n          { config: \"mps\", shard: 1, num_shards: 1, runner: \"macos-m1-stable\" },\n        ]}\n\n  macos-12-py3-arm64-test:\n    name: macos-12-py3-arm64\n    uses: ./.github/workflows/_mac-test.yml\n    needs:\n      - macos-12-py3-arm64-build\n      - target-determination\n    with:\n      build-environment: macos-12-py3-arm64\n      # Same as the build job\n      python-version: 3.9.12\n      test-matrix: ${{ needs.macos-12-py3-arm64-build.outputs.test-matrix }}\n      arch: arm64\n\n  win-vs2019-cpu-py3-build:\n    name: win-vs2019-cpu-py3\n    uses: ./.github/workflows/_win-build.yml\n    with:\n      build-environment: win-vs2019-cpu-py3\n      cuda-version: cpu\n      sync-tag: win-cpu-build\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 3, runner: \"windows.4xlarge.nonephemeral\" },\n          { config: \"default\", shard: 2, num_shards: 3, runner: \"windows.4xlarge.nonephemeral\" },\n          { config: \"default\", shard: 3, num_shards: 3, runner: \"windows.4xlarge.nonephemeral\" },\n        ]}\n\n  win-vs2019-cpu-py3-test:\n    name: win-vs2019-cpu-py3\n    uses: ./.github/workflows/_win-test.yml\n    needs:\n      - win-vs2019-cpu-py3-build\n      - target-determination\n    with:\n      build-environment: win-vs2019-cpu-py3\n      cuda-version: cpu\n      test-matrix: ${{ needs.win-vs2019-cpu-py3-build.outputs.test-matrix }}\n\n  win-vs2019-cuda11_8-py3-build:\n    name: win-vs2019-cuda11.8-py3\n    uses: ./.github/workflows/_win-build.yml\n    with:\n      build-environment: win-vs2019-cuda11.8-py3\n      cuda-version: \"11.8\"\n      sync-tag: win-cuda-build\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 2, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 3, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 4, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 5, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 6, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"force_on_cpu\", shard: 1, num_shards: 1, runner: \"windows.4xlarge.nonephemeral\" },\n        ]}\n\n  linux-focal-rocm6_0-py3_8-build:\n    name: linux-focal-rocm6.0-py3.8\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-rocm6.0-py3.8\n      docker-image-name: pytorch-linux-focal-rocm-n-py3\n      sync-tag: rocm-build\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1, runner: \"linux.rocm.gpu\" },\n        ]}\n\n  linux-focal-rocm6_0-py3_8-test:\n    permissions:\n      id-token: write\n      contents: read\n    name: linux-focal-rocm6.0-py3.8\n    uses: ./.github/workflows/_rocm-test.yml\n    needs:\n      - linux-focal-rocm6_0-py3_8-build\n      - target-determination\n    with:\n      build-environment: linux-focal-rocm6.0-py3.8\n      docker-image: ${{ needs.linux-focal-rocm6_0-py3_8-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-rocm6_0-py3_8-build.outputs.test-matrix }}\n      tests-to-include: \"test_nn test_torch test_cuda test_ops test_unary_ufuncs test_binary_ufuncs test_autograd inductor/test_torchinductor\"\n",
    "source": "zal-orz/pytorch",
    "path": ".github/workflows/trunk.yml",
    "url": "https://github.com/zal-orz/pytorch/blob/78b4793c965fe640e37d80530eb78f07d67492e8/.github/workflows/trunk.yml",
    "retrieved_at": "2025-08-14T12:50:14.763071Z"
  },
  {
    "question": "What specific files are linted by the workflow when a pull request is made?",
    "answer": "name: Lint\n\non:\n  pull_request:\n\nenv:\n  PYTHON_VERSION: \"3.8\"\n\ndefaults:\n  run:\n    shell: bash -l -eo pipefail {0}\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repo\n        uses: actions/checkout@v3\n      - name: Setup python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Update pip\n        run: python -m pip install --upgrade pip\n      - name: Install lint utilities\n        run: |\n          python -m pip install pre-commit\n          pre-commit install-hooks\n      - id: file_changes\n        uses: trilom/file-changes-action@v1.2.4\n        with:\n          prNumber: ${{ github.event.number }}\n          output: ' '\n      - name: Lint modified files\n        run: pre-commit run --files ${{ steps.file_changes.outputs.files }}\n  type_checking:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repo\n        uses: actions/checkout@v3\n      - name: Setup conda env\n        uses: conda-incubator/setup-miniconda@v2\n        with:\n          auto-update-conda: true\n          miniconda-version: \"latest\"\n          activate-environment: lint\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Install dependencies\n        run: |\n          conda install pytorch torchvision torchtext cpuonly -c pytorch-nightly\n          pip install -e \".[dev]\"\n      - name: Type check all files\n        run: python -m mypy --install-types --non-interactive --config-file mypy.ini\n",
    "source": "ravich3373/flava",
    "path": ".github/workflows/lint.yaml",
    "url": "https://github.com/ravich3373/flava/blob/8c0e16dd6b8b944ca718655d772850f27de1f4b8/.github/workflows/lint.yaml",
    "retrieved_at": "2025-08-14T12:51:44.555655Z"
  },
  {
    "question": "What CSS styling rules and conventions are enforced by this workflow?",
    "answer": "# @Author: Roni Laukkarinen\n# @Date:   2023-02-15 17:39:37\n# @Last Modified by:   Roni Laukkarinen\n# @Last Modified time: 2023-03-03 20:01:10\nname: CSS\n\non: [push, pull_request]\n\njobs:\n  build:\n    name: Test styles\n    runs-on: ubuntu-20.04\n\n    steps:\n    - name: Checkout the repository\n      uses: actions/checkout@v3\n\n    - name: Read .nvmrc\n      run: echo ::set-output name=NVMRC::$(cat .nvmrc)\n      id: nvm\n\n    - name: Setup node\n      uses: actions/setup-node@v1\n      with:\n        node-version: '${{ steps.nvm.outputs.NVMRC }}'\n\n    - name: Get package.json from devpackages\n      run: |\n        rm package.json\n        wget https://raw.githubusercontent.com/digitoimistodude/devpackages/master/package.json\n        sed -i 's/PROJECTNAME/air-light/g' package.json\n        npm install --global\n\n    - name: Install stylelint packages\n      run: |\n        npm install --global \\\n        postcss@^8.4.21 \\\n        postcss-scss@^4.0.6 \\\n        stylelint@^15.1.0 \\\n        stylelint-config-recommended@^10.0.1 \\\n        stylelint-config-recommended-scss@^9.0.0 \\\n        stylelint-config-standard@^30.0.1 \\\n        stylelint-config-standard-scss@^7.0.0 \\\n        stylelint-file-max-lines@^1.0.0 \\\n        stylelint-order@^6.0.2 \\\n        stylelint-rem-over-px@^0.0.4 \\\n        stylelint-scss@^4.4.0 \\\n        @ronilaukkarinen/stylelint-a11y@^1.2.7 \\\n        @ronilaukkarinen/stylelint-declaration-strict-value@^1.9.2 \\\n        @ronilaukkarinen/stylelint-value-no-unknown-custom-properties@^4.0.1\n\n    - name: Run stylelint\n      run: |\n        npx stylelint . --max-warnings 0 --config .stylelintrc\n",
    "source": "raikasdev/hydrogen-starter",
    "path": ".github/workflows/styles.yml",
    "url": "https://github.com/raikasdev/hydrogen-starter/blob/72d41cebb6124e1cb192c681fff61e6f7fd0eb96/.github/workflows/styles.yml",
    "retrieved_at": "2025-08-14T12:51:45.263631Z"
  },
  {
    "question": "What types of changes trigger this workflow to run?",
    "answer": "name: Linter\n\non: [push]\n\nenv:\n  NODE_VERSION: 20\n\njobs:\n  build:\n    name: JS & CSS & TS & D.TS\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # https://github.com/actions/checkout/releases/tag/v4.1.1\n\n      - name: Use Node.js ${{ env.NODE_VERSION }}\n        uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8 # https://github.com/actions/setup-node/releases/tag/v4.0.2\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: |\n          npm ci --no-audit\n\n      - name: JavaScript lint\n        run: |\n          npm run lint\n\n      - name: CSS lint\n        run: |\n          npm run in handsontable stylelint\n",
    "source": "Slimua/handsontable",
    "path": ".github/workflows/linter.yml",
    "url": "https://github.com/Slimua/handsontable/blob/8595b3432cbe4ce05393661f225db5d68cac2bce/.github/workflows/linter.yml",
    "retrieved_at": "2025-08-14T12:51:45.956441Z"
  },
  {
    "question": "Under what conditions will the \"Build and publish\" job execute?",
    "answer": "name: Publish prerelease in NPM\n\non:\n  workflow_dispatch:\n  workflow_run:\n    workflows: [Tests]\n    types: [completed]\n    branches:\n      - 'develop'\n      - 'release/**'\n      - '!release/**-**'\n\nenv:\n  NODE_VERSION: 20\n\njobs:\n  build:\n    name: Build and publish\n    runs-on: ubuntu-latest\n    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}\n    steps:\n      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # https://github.com/actions/checkout/releases/tag/v4.1.1\n        with:\n          ref: ${{ github.event.workflow_run.head_branch }}\n\n      - name: Use Node.js ${{ env.NODE_VERSION }}\n        uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8 # https://github.com/actions/setup-node/releases/tag/v4.0.2\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          registry-url: https://registry.npmjs.org/\n          cache: 'npm'\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPM_PUBLISH_AUTOMATION }}\n      - run: npm install\n      - run: npm run pre-release\n",
    "source": "Slimua/handsontable",
    "path": ".github/workflows/publish.yml",
    "url": "https://github.com/Slimua/handsontable/blob/8595b3432cbe4ce05393661f225db5d68cac2bce/.github/workflows/publish.yml",
    "retrieved_at": "2025-08-14T12:51:46.657106Z"
  },
  {
    "question": "What are the different build configurations and compiler versions used within the compatibility job matrix?",
    "answer": "name: Windows\n\non:\n  push:\n    paths-ignore:\n      - '**/*.md'\n  pull_request:\n    paths-ignore:\n      - '**/*.md'\n\njobs:\n  compatibility:\n    runs-on: windows-latest\n    strategy:\n      matrix:\n        VER: [v142, v143]\n        EXT: [ON, OFF]\n        GEN: [Visual Studio 17 2022]\n        BIN: [x64, x86]\n        STD: [99, 11, 17]\n        include:\n          - VER: v141\n            EXT: OFF\n            GEN: Ninja Multi-Config\n            BIN: x64\n            STD: 89 # /Za\n    env:\n      NINJA_URL: https://github.com/ninja-build/ninja/releases/download/v1.10.2/ninja-win.zip\n      NINJA_ROOT: C:\\Tools\\Ninja\n      VS_ROOT: 'C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise'\n      UseMultiToolTask: true # Better parallel MSBuild execution\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Cache Ninja install\n      if: matrix.GEN == 'Ninja Multi-Config'\n      id: ninja-install\n      uses: actions/cache@v2\n      with:\n        path: |\n          C:\\Tools\\Ninja\n        key: ${{runner.os}}-ninja-${{env.NINJA_URL}}\n\n    - name: Install Ninja\n      if: matrix.GEN == 'Ninja Multi-Config' && steps.ninja-install.outputs.cache-hit != 'true'\n      shell: pwsh\n      run: |\n        Invoke-WebRequest ${env:NINJA_URL} -OutFile ~\\Downloads\\ninja-win.zip\n        Expand-Archive ~\\Downloads\\ninja-win.zip -DestinationPath ${env:NINJA_ROOT}\\\n        Remove-Item ~\\Downloads\\*\n\n    - name: Configure (MSBuild)\n      if: matrix.GEN == 'Visual Studio 17 2022'\n      shell: pwsh\n      run: |\n        $BIN = if('${{matrix.BIN}}' -eq 'x86') {'Win32'} else {'x64'}\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -A $BIN `\n          -T ${{matrix.VER}} `\n          -D BUILD_TESTING=ON `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -S \"${env:GITHUB_WORKSPACE}\" `\n          -B \"${env:GITHUB_WORKSPACE}\\build\"\n\n    - name: Configure (Ninja Multi-Config)\n      if: matrix.GEN == 'Ninja Multi-Config'\n      shell: pwsh\n      run: |\n        $VER = switch ('${{matrix.VER}}') { `\n          'v141' {'14.1'} `\n          'v142' {'14.2'} `\n          'v143' {'14.3'} }\n        Import-Module \"${env:VS_ROOT}\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\"\n        Enter-VsDevShell -VsInstallPath ${env:VS_ROOT} -SkipAutomaticLocation -DevCmdArguments \"-host_arch=x64 -arch=${{matrix.BIN}} -vcvars_ver=${VER}\"\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -D CMAKE_MAKE_PROGRAM=\"${env:NINJA_ROOT}\\ninja.exe\" `\n          -D BUILD_TESTING=ON `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_EXE_LINKER_FLAGS='/INCREMENTAL' `\n          -S \"${env:GITHUB_WORKSPACE}\" `\n          -B \"${env:GITHUB_WORKSPACE}\\build\"\n\n    - name: Build (MSBuild)\n      if: matrix.GEN == 'Visual Studio 17 2022'\n      shell: pwsh\n      run: |\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\build\" `\n            --config ${Config} `\n            -- `\n            /verbosity:minimal `\n            /maxCpuCount `\n            /noLogo\n        }\n\n    - name: Build (Ninja)\n      if: matrix.GEN == 'Ninja Multi-Config'\n      shell: pwsh\n      run: |\n        $VER = switch ('${{matrix.VER}}') { `\n          'v141' {'14.1'} `\n          'v142' {'14.2'} `\n          'v143' {'14.3'} }\n        Import-Module \"${env:VS_ROOT}\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\"\n        Enter-VsDevShell -VsInstallPath ${env:VS_ROOT} -SkipAutomaticLocation -DevCmdArguments \"-host_arch=x64 -arch=${{matrix.BIN}} -vcvars_ver=${VER}\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\build\" `\n            --config ${Config} `\n            -- `\n            -j ${env:NUMBER_OF_PROCESSORS}\n        }\n\n    - name: Test\n      shell: pwsh\n      run: |\n        foreach ($Config in 'Release','Debug') { `\n          & ctest `\n            --test-dir \"${env:GITHUB_WORKSPACE}\\build\" `\n            --build-config ${Config} `\n            --output-on-failure `\n            --parallel ${env:NUMBER_OF_PROCESSORS}\n        }\n\n    - name: Install\n      shell: pwsh\n      run: |\n        & cmake `\n          --install \"${env:GITHUB_WORKSPACE}\\build\" `\n          --prefix \"${env:GITHUB_WORKSPACE}\\install\" `\n          --config Release\n\n    - name: Consume (PkgConfig - bare MSBuild)\n      if: matrix.GEN == 'Visual Studio 17 2022'\n      shell: pwsh\n      run: |\n        $BIN = if('${{matrix.BIN}}' -eq 'x86') {'Win32'} else {'x64'}\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -A $BIN `\n          -T ${{matrix.VER}} `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_PREFIX_PATH=\"${env:GITHUB_WORKSPACE}\\install\" `\n          -S \"${env:GITHUB_WORKSPACE}\\tests\\pkgconfig\\bare\" `\n          -B \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\bare\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\bare\" `\n            --config ${Config} `\n            -- `\n            /verbosity:minimal `\n            /maxCpuCount `\n            /noLogo `\n        }\n\n    - name: Consume (PkgConfig - bare Ninja)\n      if: matrix.GEN == 'Ninja Multi-Config'\n      shell: pwsh\n      run: |\n        $VER = switch ('${{matrix.VER}}') { `\n          'v141' {'14.1'} `\n          'v142' {'14.2'} `\n          'v143' {'14.3'} }\n        Import-Module \"${env:VS_ROOT}\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\"\n        Enter-VsDevShell -VsInstallPath ${env:VS_ROOT} -SkipAutomaticLocation -DevCmdArguments \"-host_arch=x64 -arch=${{matrix.BIN}} -vcvars_ver=${VER}\"\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -D CMAKE_MAKE_PROGRAM=\"${env:NINJA_ROOT}\\ninja.exe\" `\n          -D BUILD_TESTING=ON `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_EXE_LINKER_FLAGS='/INCREMENTAL' `\n          -D CMAKE_PREFIX_PATH=\"${env:GITHUB_WORKSPACE}\\install\" `\n          -S \"${env:GITHUB_WORKSPACE}\\tests\\pkgconfig\\bare\" `\n          -B \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\bare\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\bare\" `\n            --config ${Config} `\n            -- `\n            -j ${env:NUMBER_OF_PROCESSORS} `\n        }\n\n    - name: Consume (Emulate SDK presence)\n      shell: pwsh\n      run: |\n        New-Item -Type Directory -Path ${env:GITHUB_WORKSPACE}\\install\\share\\cmake\\OpenCL\n        New-Item -Type File -Path ${env:GITHUB_WORKSPACE}\\install\\share\\cmake\\OpenCL\\OpenCLConfig.cmake -Value 'include(\"${CMAKE_CURRENT_LIST_DIR}/../OpenCLHeaders/OpenCLHeadersTargets.cmake\")'\n\n    - name: Consume (PkgConfig - SDK MSBuild)\n      if: matrix.GEN == 'Visual Studio 17 2022'\n      shell: pwsh\n      run: |\n        $BIN = if('${{matrix.BIN}}' -eq 'x86') {'Win32'} else {'x64'}\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -A $BIN `\n          -T ${{matrix.VER}} `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_PREFIX_PATH=\"${env:GITHUB_WORKSPACE}\\install\" `\n          -S \"${env:GITHUB_WORKSPACE}\\tests\\pkgconfig\\sdk\" `\n          -B \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\sdk\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\sdk\" `\n            --config ${Config} `\n            -- `\n            /verbosity:minimal `\n            /maxCpuCount `\n            /noLogo `\n        }\n\n    - name: Consume (PkgConfig - SDK Ninja)\n      if: matrix.GEN == 'Ninja Multi-Config'\n      shell: pwsh\n      run: |\n        $VER = switch ('${{matrix.VER}}') { `\n          'v141' {'14.1'} `\n          'v142' {'14.2'} `\n          'v143' {'14.3'} }\n        Import-Module \"${env:VS_ROOT}\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\"\n        Enter-VsDevShell -VsInstallPath ${env:VS_ROOT} -SkipAutomaticLocation -DevCmdArguments \"-host_arch=x64 -arch=${{matrix.BIN}} -vcvars_ver=${VER}\"\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -D CMAKE_MAKE_PROGRAM=\"${env:NINJA_ROOT}\\ninja.exe\" `\n          -D BUILD_TESTING=ON `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_EXE_LINKER_FLAGS='/INCREMENTAL' `\n          -D CMAKE_PREFIX_PATH=\"${env:GITHUB_WORKSPACE}\\install\" `\n          -S \"${env:GITHUB_WORKSPACE}\\tests\\pkgconfig\\sdk\" `\n          -B \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\sdk\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\sdk\" `\n            --config ${Config} `\n            -- `\n            -j ${env:NUMBER_OF_PROCESSORS} `\n        }",
    "source": "msft-mirror-aosp/platform.external.OpenCL-Headers",
    "path": ".github/workflows/windows.yml",
    "url": "https://github.com/msft-mirror-aosp/platform.external.OpenCL-Headers/blob/dceb741b02c8788107db18f343b46d542406467a/.github/workflows/windows.yml",
    "retrieved_at": "2025-08-14T12:51:47.502862Z"
  },
  {
    "question": "Under what conditions does the `build-protocol` job automatically commit changes to the repository?",
    "answer": "name: Protocol\n\non:\n  pull_request:\n    types: [opened, synchronize, reopened, ready_for_review]\n    paths:\n      - \"packages/protocol/**\"\n      - \"!packages/protocol/contract_layout_*\"\n      - \"!packages/protocol/audit/**\"\n      - \"!packages/protocol/docs/**\"\n      - \"!packages/protocol/simulation/**\"\n      - \"!packages/protocol/deployments/**\"\n      - \"!packages/protocol/script/layer2/hekla/**\"\n      - \"!packages/protocol/script/layer2/mainnet/**\"\n      - \"!packages/protocol/script/layer1/hekla/**\"\n      - \"!packages/protocol/script/layer1/mainnet/**\"\n      - \"!packages/protocol/script/layer1/preconf/**\"\n      - \"!packages/protocol/script/layer1/provers/**\"\n      - \"!packages/protocol/script/layer1/team/**\"\n    branches-ignore:\n      - release-please--branches--**\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build-protocol:\n    if: ${{ github.event.pull_request.draft == false  && !startsWith(github.head_ref, 'release-please') && github.actor != 'dependabot[bot]' }}\n    runs-on: [arc-runner-set]\n    permissions:\n      # Give the necessary permissions for stefanzweifel/git-auto-commit-action.\n      contents: write\n    steps:\n      - name: Cancel previous runs\n        uses: styfle/cancel-workflow-action@0.12.1\n        with:\n          access_token: ${{ github.token }}\n\n      - name: Prepare environment\n        continue-on-error: true\n        run: sudo apt-get update && sudo apt-get install -y git wget\n\n      - name: Checkout repository\n        uses: actions/checkout@v4\n        with:\n          submodules: recursive\n\n      - name: Install Foundry\n        uses: foundry-rs/foundry-toolchain@v1.2.0\n        with:\n          version: stable\n\n      - name: Install pnpm dependencies\n        uses: ./.github/actions/install-pnpm-dependencies\n\n      - name: Clean up and fmt\n        working-directory: ./packages/protocol\n        run: pnpm clean && forge fmt\n\n      - name: L2-Unit tests\n        working-directory: ./packages/protocol\n        run: pnpm compile:l2 && pnpm test:l2 && pnpm layout:l2\n\n      - name: L1-Unit tests\n        working-directory: ./packages/protocol\n        run: pnpm compile:l1 && pnpm snapshot:l1 && pnpm layout:l1\n\n      - name: Check for changes\n        id: git_status\n        run: |\n          git add -N .  # Simulate staging to detect untracked files\n          if [ -n \"$(git status --porcelain)\" ]; then\n            echo \"changes=true\" >> $GITHUB_ENV\n          else\n            echo \"changes=false\" >> $GITHUB_ENV\n          fi\n\n      - name: Commit contract layout table\n        if: env.changes == 'true'\n        uses: stefanzweifel/git-auto-commit-action@v5\n        with:\n          commit_message: \"forge fmt & update contract layout tables\"\n\n      - name: L1-Deploy contracts\n        working-directory: ./packages/protocol\n        timeout-minutes: 2\n        run: |\n          anvil --hardfork cancun &\n          until cast chain-id --rpc-url \"http://localhost:8545\" 2> /dev/null; do\n            sleep 1\n          done\n          pnpm test:deploy:l1\n\n  genesis-docker:\n    if: ${{ github.event.pull_request.draft == false  && !startsWith(github.head_ref, 'release-please') && github.actor != 'dependabot[bot]' }}\n    runs-on: [taiko-runner]\n    permissions:\n      # Give the necessary permissions for stefanzweifel/git-auto-commit-action.\n      contents: write\n    steps:\n      - name: Cancel previous runs\n        uses: styfle/cancel-workflow-action@0.12.1\n        with:\n          access_token: ${{ github.token }}\n\n      - name: Checkout repository\n        uses: actions/checkout@v4\n        with:\n          submodules: recursive\n\n      - name: Install Foundry\n        uses: foundry-rs/foundry-toolchain@v1.3.1\n\n      - name: Install pnpm dependencies\n        uses: ./.github/actions/install-pnpm-dependencies\n\n      - name: Compile\n        working-directory: ./packages/protocol\n        run: pnpm clean && pnpm compile\n\n      - name: L2-Generate Genesis (using docker)\n        working-directory: ./packages/protocol\n        run: pnpm genesis:test\n",
    "source": "NethermindEth/preconf-taiko-mono",
    "path": ".github/workflows/protocol.yml",
    "url": "https://github.com/NethermindEth/preconf-taiko-mono/blob/5ac7eda8202115e23bf76fd554c0a62b5aee765c/.github/workflows/protocol.yml",
    "retrieved_at": "2025-08-14T12:51:48.275454Z"
  },
  {
    "question": "What branches, besides `trying`, `trying.tmp`, `staging`, and `staging.tmp`, trigger this workflow?",
    "answer": "on:\n  push:\n    branches:\n      - '!trying'\n      - '!trying.tmp'\n      - '!staging'\n      - '!staging.tmp'\n\nname: Coverage\n\nenv:\n  RUST_BACKTRACE: 1\n  RUSTFLAGS: \"-Ccodegen-units=1 -Clink-dead-code -Coverflow-checks=off\"\n\njobs:\n  coverage:\n    name: Coverage\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          profile: minimal\n          override: true\n      - name: Install LLVM (Linux)\n        run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz -L -o llvm.tar.xz\n          mkdir -p /opt/llvm-10\n          tar xf llvm.tar.xz --strip-components=1 -C /opt/llvm-10\n          echo '/opt/llvm-10/bin' >> $GITHUB_PATH\n          echo 'LLVM_SYS_100_PREFIX=/opt/llvm-10' >> $GITHUB_ENV\n      - name: Generate Coverage Report\n        run: |\n          cargo install cargo-tarpaulin\n          cargo tarpaulin --forward --release -t120 --out Xml --ignore-tests --workspace --exclude wasmer-wasi-experimental-io-devices --exclude wasmer-c-api -- --skip traps:: --skip spec::linking --test-threads=1\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v1\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          file: ./cobertura.xml\n",
    "source": "ailisp/near-wasmer",
    "path": ".github/workflows/coverage.yaml",
    "url": "https://github.com/ailisp/near-wasmer/blob/b75a0b651abd1b271142dc6ebc870b0090434972/.github/workflows/coverage.yaml",
    "retrieved_at": "2025-08-14T12:51:49.118560Z"
  },
  {
    "question": "Under what conditions will Dependabot pull requests be automatically merged by this workflow?",
    "answer": "name: Dependabot Auto Merge\n\non:\n  pull_request_target:\n\njobs:\n  auto-merge:\n    timeout-minutes: 5\n    runs-on: ubuntu-latest\n    if: github.actor == 'dependabot[bot]'\n    steps:\n      - uses: actions/checkout@v2\n      - uses: ahmadnassri/action-dependabot-auto-merge@v2\n        with:\n          target: minor\n          github-token: ${{ secrets.DEP_AUTOMERGE }}\n",
    "source": "ttttonyhe/ouorz-mono",
    "path": ".github/workflows/auto-merge.yml",
    "url": "https://github.com/ttttonyhe/ouorz-mono/blob/19ccccede781c71f3a3c5c4bbfc27368cc43be03/.github/workflows/auto-merge.yml",
    "retrieved_at": "2025-08-14T12:51:49.878293Z"
  },
  {
    "question": "Under what conditions will the code formatting check be executed?",
    "answer": "name: '🔎 Check Code Formatting'\n\non:\n  push:\n    branches:\n      - 'master'\n    paths:\n      - 'src/**'\n      - '!**/README.md'\n      - '!**.rst'\n\n  pull_request:\n    types:\n      - opened\n      - edited\n      - reopened\n      - synchronize\n    branches:\n      - 'master'\n\njobs:\n  formatting-check:\n    runs-on: ubuntu-20.04\n    steps:\n    - name: '⏳ Checkout repository'\n      uses: actions/checkout@v4\n      with:\n        submodules: false\n        persist-credentials: false\n\n    - name: '♻ Caching dependencies'\n      uses: actions/cache@v4.0.2\n      id: cache\n      with:\n        path: ~/cache/deps/bin\n        key: 'uncrustify'\n\n    - name: '🛠 Install dependencies'\n      if: steps.cache.outputs.cache-hit != 'true'\n      run: source tools/ci.sh && ci_install_code_format_deps\n\n    - name: '📜 Get list of changed files'\n      id: changed-files\n      uses: tj-actions/changed-files@v44\n      with:\n        files: |\n            src/**/*.c\n            src/**/*.h\n            !src/hal/**\n            !src/uvc/**\n            !src/lib/**\n            !src/drivers/**\n            !src/micropython/**\n            !src/stm32cubeai/**\n\n    - name: '📜 Show list of changed files'\n      run: |\n        echo \"${{ toJSON(steps.changed-files.outputs) }}\"\n      shell:\n        bash\n\n    - name: '🔎 Check code formatting'\n      if: steps.changed-files.outputs.any_changed == 'true'\n      run: |\n        source tools/ci.sh && ci_run_code_format_check ${{ steps.changed-files.outputs.all_changed_files }}\n",
    "source": "Omarsevenway/openmv-master",
    "path": ".github/workflows/codeformat.yml",
    "url": "https://github.com/Omarsevenway/openmv-master/blob/8e3c88119e4b813a2abceddcfb30370d9db5f36c/.github/workflows/codeformat.yml",
    "retrieved_at": "2025-08-14T12:51:50.622680Z"
  },
  {
    "question": "What specific benchmarks are run by the workflow, and how are their results stored and sent to Datadog?",
    "answer": "name: Haystack 1.x Benchmarks\n\non:\n  workflow_dispatch:\n\npermissions:\n  id-token: write\n  contents: read\n\nenv:\n  AWS_REGION: eu-central-1\n\njobs:\n  deploy-runner:\n    runs-on: ubuntu-latest\n    outputs:\n      cml_runner_id: ${{ steps.deploy.outputs.cml_runner_id }}\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: iterative/setup-cml@v3\n\n      - name: AWS authentication\n        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}\n\n      - name: Launch EC2 instance and deploy runner\n        id: deploy\n        env:\n          repo_token: ${{ secrets.HAYSTACK_BOT_TOKEN }}\n        run: |\n          OUTPUT=$(cml runner launch \\\n          --cloud aws \\\n          --cloud-region ${{ env.AWS_REGION }} \\\n          --cloud-type=p3.2xlarge \\\n          --cloud-hdd-size=64 \\\n          --labels=cml 2>&1 | tee /dev/fd/2)\n          # Extract 'id' from the log and set it as an environment variable\n          ID_VALUE=$(echo \"$OUTPUT\" | jq -r '.message? | fromjson? | select(.id != null) | .id // empty')\n          echo \"cml_runner_id=$ID_VALUE\" >> \"$GITHUB_OUTPUT\"\n\n  run-reader-benchmarks:\n    needs: deploy-runner\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/reader/*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-reader\n          path: test/benchmarks/out/\n\n  run-elasticsearch-benchmarks:\n    needs:\n      - deploy-runner\n      - run-reader-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      elasticsearch:\n        image: elasticsearch:7.17.6\n        env:\n          discovery.type: \"single-node\"\n        ports:\n          - 9201:9200\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,elasticsearch,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-elasticsearch-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-elasticsearch\n          path: test/benchmarks/out/\n\n  run-weaviate-benchmarks:\n    needs:\n      - deploy-runner\n      - run-elasticsearch-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      weaviate:\n        image: semitechnologies/weaviate:1.17.2\n        env:\n          AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: \"true\"\n          PERSISTENCE_DATA_PATH: \"/var/lib/weaviate\"\n        ports:\n          - 8080:8080\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,weaviate,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-weaviate-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-weaviate\n          path: test/benchmarks/out/\n\n  run-opensearch-benchmarks:\n    needs:\n      - deploy-runner\n      - run-weaviate-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      opensearch:\n        image: opensearchproject/opensearch:1.3.5\n        env:\n          discovery.type: \"single-node\"\n          OPENSEARCH_JAVA_OPTS: \"-Xms4096m -Xmx4096m\"\n        ports:\n          - 9200:9200\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,opensearch,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-opensearch-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-opensearch\n          path: test/benchmarks/out/\n\n  terminate-runner:\n    if: always()\n    needs:\n      - deploy-runner\n      - run-opensearch-benchmarks\n    runs-on: ubuntu-latest\n    steps:\n      - name: AWS authentication\n        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}\n\n      - name: Terminate EC2 instance\n        env:\n          CML_RUNNER_ID: ${{needs.deploy-runner.outputs.cml_runner_id}}\n        run: |\n          # Get the instance ID using its Name tag and terminate the instance\n          INSTANCE_ID=$(aws ec2 describe-instances --filters \"Name=tag:Name,Values=${{ env.CML_RUNNER_ID }}\" --query \"Reservations[*].Instances[*].[InstanceId]\" --output text)\n          aws ec2 terminate-instances --instance-ids \"$INSTANCE_ID\"\n",
    "source": "SAKTHICPT/deepset-ai-haystack",
    "path": ".github/workflows/benchmarks.yml",
    "url": "https://github.com/SAKTHICPT/deepset-ai-haystack/blob/9e18c822e07f2038397aabeff9a176aa69995f4b/.github/workflows/benchmarks.yml",
    "retrieved_at": "2025-08-14T12:51:51.333091Z"
  },
  {
    "question": "Under what conditions does the `delete-preview` job execute and what artifact does it upload?",
    "answer": "name: Preview (build)\non:\n  pull_request:\n    types: [opened, synchronize, reopened, closed]\n    paths-ignore:\n      - 'microsite/**'\n      - '*.md'\n\njobs:\n  build-backstage:\n    env:\n      NODE_OPTIONS: --max-old-space-size=4096\n      UFFIZZI_URL: https://uffizzi.com\n    name: Build PR image\n    runs-on: ubuntu-latest\n    if: ${{ github.event_name != 'pull_request' || github.event.action != 'closed' }}\n    outputs:\n      tags: ${{ steps.meta.outputs.tags }}\n    steps:\n      - name: checkout\n        uses: actions/checkout@v3\n\n      - name: setup-node\n        uses: actions/setup-node@v3\n        with:\n          node-version: 16.x\n          registry-url: https://registry.npmjs.org/\n\n      - name: yarn install\n        uses: backstage/actions/yarn-install@v0.6.4\n        with:\n          cache-prefix: linux-v16\n\n      - name: Use Uffizzi's backstage app config\n        run: |\n          cp -f ./.github/uffizzi/uffizzi.production.app-config.yaml ./app-config.yaml\n\n      - name: typescript build\n        run: |\n          yarn tsc\n\n      - name: backstage build\n        run: |\n          yarn workspace example-backend build\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n\n      - name: Generate UUID image name\n        id: uuid\n        run: echo \"UUID_TAG_APP=$(uuidgen)\" >> $GITHUB_ENV\n\n      - name: Docker metadata\n        id: meta\n        uses: docker/metadata-action@v4\n        with:\n          images: registry.uffizzi.com/${{ env.UUID_TAG_APP }}\n          tags: type=raw,value=60d\n\n      - name: Build Image\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          file: packages/backend/Dockerfile\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n          push: true\n\n  render-compose-file:\n    name: Render Docker Compose File\n    runs-on: ubuntu-latest\n    needs:\n      - build-backstage\n    outputs:\n      compose-file-cache-key: ${{ steps.hash.outputs.hash }}\n    steps:\n      - name: Checkout git repo\n        uses: actions/checkout@v3\n      - name: Render Compose File\n        run: |\n          BACKSTAGE_IMAGE=$(echo ${{ needs.build-backstage.outputs.tags }})\n          export BACKSTAGE_IMAGE\n          # Render simple template from environment variables.\n          envsubst '$BACKSTAGE_IMAGE $GITHUB_SHA' < .github/uffizzi/docker-compose.uffizzi.yml > docker-compose.rendered.yml\n          cat docker-compose.rendered.yml\n      - name: Upload Rendered Compose File as Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: preview-spec\n          path: docker-compose.rendered.yml\n          retention-days: 2\n      - name: Upload PR Event as Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: preview-spec\n          path: ${{ github.event_path }}\n          retention-days: 2\n\n  delete-preview:\n    name: Call for Preview Deletion\n    runs-on: ubuntu-latest\n    if: ${{ github.event.action == 'closed' }}\n    steps:\n      # If this PR is closing, we will not render a compose file nor pass it to the next workflow.\n      - name: Upload PR Event as Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: preview-spec\n          path: ${{ github.event_path }}\n          retention-days: 2\n",
    "source": "davidosantos/backstage",
    "path": ".github/workflows/uffizzi-build.yml",
    "url": "https://github.com/davidosantos/backstage/blob/d820584c6df4aa66c927622f86898aa2c1a4fc3f/.github/workflows/uffizzi-build.yml",
    "retrieved_at": "2025-08-15T01:51:56.361055Z"
  },
  {
    "question": "What criteria determine the folders tested in the `run_tests_gpu` job based on the `setup` job's output?",
    "answer": "name: Self-hosted runner (scheduled)\n\non:\n  repository_dispatch:\n  schedule:\n    - cron: \"0 2 * * *\"\n\nenv:\n  HF_HOME: /mnt/cache\n  TRANSFORMERS_IS_CI: yes\n  OMP_NUM_THREADS: 8\n  MKL_NUM_THREADS: 8\n  RUN_SLOW: yes\n  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n  TF_FORCE_GPU_ALLOW_GROWTH: true\n  RUN_PT_TF_CROSS_TESTS: 1\n\njobs:\n  setup:\n    name: Setup\n    strategy:\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: |\n          git fetch && git checkout ${{ github.sha }}\n\n      - name: Cleanup\n        working-directory: /transformers\n        run: |\n          rm -rf tests/__pycache__\n          rm -rf reports\n\n      - id: set-matrix\n        name: Identify models to test\n        working-directory: /transformers/tests\n        run: |\n          echo \"::set-output name=matrix::$(python3 -c 'import os; x = list(filter(os.path.isdir, os.listdir(os.getcwd()))); x.sort(); print(x)')\"\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: GPU visibility\n        working-directory: /transformers\n        run: |\n          utils/print_env_pt.py\n          TF_CPP_MIN_LOG_LEVEL=3 python3 -c \"import tensorflow as tf; print('TF GPUs available:', bool(tf.config.list_physical_devices('GPU')))\"\n          TF_CPP_MIN_LOG_LEVEL=3 python3 -c \"import tensorflow as tf; print('Number of TF GPUs available:', len(tf.config.list_physical_devices('GPU')))\"\n\n  run_tests_gpu:\n    name: Model tests\n    strategy:\n      fail-fast: false\n      matrix:\n        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Echo folder ${{ matrix.folders }}\n        run: echo \"${{ matrix.folders }}\"\n\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all non-slow tests on GPU\n        working-directory: /transformers\n        run: python3 -m pytest -v --make-reports=${{ matrix.machines }}_tests_gpu_${{ matrix.folders }} tests/${{ matrix.folders }}\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/${{ matrix.machines }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_all_tests_gpu_${{ matrix.folders }}_test_reports\n          path: /transformers/reports/${{ matrix.machines }}_tests_gpu_${{ matrix.folders }}\n\n  run_examples_gpu:\n    name: Examples directory\n    runs-on: [self-hosted, single-gpu-docker]\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run examples tests on GPU\n        working-directory: /transformers\n        run: |\n          pip install -r examples/pytorch/_tests_requirements.txt\n          python3 -m pytest -v --make-reports=examples_gpu examples/pytorch\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/examples_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_examples_gpu\n          path: /transformers/reports/examples_gpu\n\n  run_pipelines_torch_gpu:\n    name: PyTorch pipelines\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-pytorch-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all pipeline tests on GPU\n        working-directory: /transformers\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python3 -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=${{ matrix.machines }}_tests_torch_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/${{ matrix.machines }}_tests_torch_pipeline_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_torch_pipeline_gpu\n          path: /transformers/reports/${{ matrix.machines }}_tests_torch_pipeline_gpu\n\n  run_pipelines_tf_gpu:\n    name: TensorFlow pipelines\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-tensorflow-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: |\n          git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all pipeline tests on GPU\n        working-directory: /transformers\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python3 -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=${{ matrix.machines }}_tests_tf_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: |\n          cat /transformers/reports/${{ matrix.machines }}_tests_tf_pipeline_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_tf_pipeline_gpu\n          path: /transformers/reports/${{ matrix.machines }}_tests_tf_pipeline_gpu\n\n  run_all_tests_torch_cuda_extensions_gpu:\n    name: Torch CUDA extension tests\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    needs: setup\n    container:\n      image: huggingface/transformers-pytorch-deepspeed-latest-gpu\n      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Update clone\n        working-directory: /workspace/transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all tests on GPU\n        working-directory: /workspace/transformers\n        run: |\n          python -m pytest -v --make-reports=${{ matrix.machines }}_tests_torch_cuda_extensions_gpu tests/deepspeed tests/extended\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /workspace/transformers/reports/${{ matrix.machines }}_tests_torch_cuda_extensions_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_torch_cuda_extensions_gpu_test_reports\n          path: /workspace/transformers/reports/${{ matrix.machines }}_tests_torch_cuda_extensions_gpu\n\n\n  send_results:\n    name: Send results to webhook\n    runs-on: ubuntu-latest\n    if: always()\n    needs: [setup, run_tests_gpu, run_examples_gpu, run_pipelines_tf_gpu, run_pipelines_torch_gpu, run_all_tests_torch_cuda_extensions_gpu]\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/download-artifact@v2\n      - name: Send message to Slack\n        env:\n          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n        run: |\n          pip install slack_sdk\n          python utils/notification_service.py \"${{ needs.setup.outputs.matrix }}\"\n",
    "source": "bhargaviparanjape/robust-transformers",
    "path": ".github/workflows/self-scheduled.yml",
    "url": "https://github.com/bhargaviparanjape/robust-transformers/blob/7195cc405c3339d1ed58895cc0eecc229fa49d22/.github/workflows/self-scheduled.yml",
    "retrieved_at": "2025-08-15T01:51:57.297370Z"
  },
  {
    "question": "What triggers this workflow to run and check the status of endpoints?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "caliwyr/Software",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/caliwyr/Software/blob/b98328872a986013090c7f130f3cb9eb56c82fe2/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-15T01:51:57.912738Z"
  },
  {
    "question": "What triggers this workflow to run and deploy updates?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Updates CI\non:\n  schedule:\n    - cron: \"0 3 * * *\"\n  repository_dispatch:\n    types: [updates]\n  workflow_dispatch:\njobs:\n  release:\n    name: Deploy updates\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Update code\n        uses: upptime/updates@master\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n",
    "source": "caliwyr/Software",
    "path": ".github/workflows/updates.yml",
    "url": "https://github.com/caliwyr/Software/blob/b98328872a986013090c7f130f3cb9eb56c82fe2/.github/workflows/updates.yml",
    "retrieved_at": "2025-08-15T01:51:58.669791Z"
  },
  {
    "question": "Under what conditions will the \"Analytics App Tests\" workflow be triggered?",
    "answer": "name: Analytics App Tests\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'apps/analytics/**'\n      - '.github/workflows/**'\n  pull_request:\n    branches:\n      - main\n    paths:\n      - 'apps/analytics/**'\n      - '.github/workflows/**'\n\njobs:\n  run-smoke-test:\n    name: Smoke test\n    runs-on: ubuntu-latest\n    env:\n      TURBO_TOKEN: ${{ secrets.TURBO_TOKEN }}\n      TURBO_TEAM: ${{ secrets.TURBO_TEAM }}\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Install Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n\n      - uses: pnpm/action-setup@v2\n        name: Install pnpm\n        id: pnpm-install\n        with:\n          version: 8\n          run_install: false\n\n      - name: Get pnpm store directory\n        id: pnpm-cache\n        shell: bash\n        run: |\n          echo \"STORE_PATH=$(pnpm store path)\" >> $GITHUB_OUTPUT\n\n      - uses: actions/cache@v3\n        name: Setup pnpm cache\n        with:\n          path: |\n            ~/.cache/Cypress\n            ${{ steps.pnpm-cache.outputs.STORE_PATH }}\n          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}\n          restore-keys: |\n            ${{ runner.os }}-pnpm-store-\n\n      - name: Install dependencies\n        run: pnpm install\n\n      - name: Setup environment variables\n        run: |\n          touch apps/analytics/.env\n          echo DATABASE_URL=${{ secrets.DATABASE_URL }} >> apps/analytics/.env\n          echo HASH_SALT=${{ secrets.HASH_SALT }} >> apps/analytics/.env\n          echo MAXMIND_LICENSE_KEY=${{ secrets.MAXMIND_LICENSE_KEY }} >> apps/analytics/.env\n\n      - name: Build Prisma client\n        run: cd apps/analytics && pnpm run build-postgresql-client\n\n      - name: Build app\n        run: pnpm run build:analytics\n",
    "source": "ttttonyhe/ouorz-mono",
    "path": ".github/workflows/analytics-app-testing.yml",
    "url": "https://github.com/ttttonyhe/ouorz-mono/blob/19ccccede781c71f3a3c5c4bbfc27368cc43be03/.github/workflows/analytics-app-testing.yml",
    "retrieved_at": "2025-08-15T01:51:59.300015Z"
  },
  {
    "question": "Under what conditions are the \"-push-ci\" tagged images built and pushed to DockerHub?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "kssteven418/SqueezeLLM-gradients",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/kssteven418/SqueezeLLM-gradients/blob/5f2a16698b93cddddf858a78bef61fd5c6271055/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-15T01:52:00.230533Z"
  },
  {
    "question": "Under what conditions does the workflow execute `terraform apply -auto-approve`?",
    "answer": "# This workflow installs the latest version of Terraform CLI and configures the Terraform CLI configuration file\n# with an API token for Terraform Cloud (app.terraform.io). On pull request events, this workflow will run\n# `terraform init`, `terraform fmt`, and `terraform plan` (speculative plan via Terraform Cloud). On push events\n# to the main branch, `terraform apply` will be executed.\n#\n# Documentation for `hashicorp/setup-terraform` is located here: https://github.com/hashicorp/setup-terraform\n#\n# To use this workflow, you will need to complete the following setup steps.\n#\n# 1. Create a `main.tf` file in the root of this repository with the `remote` backend and one or more resources defined.\n#   Example `main.tf`:\n#     # The configuration for the `remote` backend.\n#     terraform {\n#       backend \"remote\" {\n#         # The name of your Terraform Cloud organization.\n#         organization = \"example-organization\"\n#\n#         # The name of the Terraform Cloud workspace to store Terraform state files in.\n#         workspaces {\n#           name = \"example-workspace\"\n#         }\n#       }\n#     }\n#\n#     # An example resource that does nothing.\n#     resource \"null_resource\" \"example\" {\n#       triggers = {\n#         value = \"A example resource that does nothing!\"\n#       }\n#     }\n#\n#\n# 2. Generate a Terraform Cloud user API token and store it as a GitHub secret (e.g. TF_API_TOKEN) on this repository.\n#   Documentation:\n#     - https://www.terraform.io/docs/cloud/users-teams-organizations/api-tokens.html\n#     - https://help.github.com/en/actions/configuring-and-managing-workflows/creating-and-storing-encrypted-secrets\n#\n# 3. Reference the GitHub secret in step using the `hashicorp/setup-terraform` GitHub Action.\n#   Example:\n#     - name: Setup Terraform\n#       uses: hashicorp/setup-terraform@v1\n#       with:\n#         cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\nname: 'Terraform'\n\non:\n  push:\n    branches:\n    - main\n  pull_request:\n\njobs:\n  terraform:\n    name: 'Terraform'\n    runs-on: ubuntu-latest\n    environment: production\n\n    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest\n    defaults:\n      run:\n        shell: bash\n\n    steps:\n    # Checkout the repository to the GitHub Actions runner\n    - name: Checkout\n      uses: actions/checkout@v2\n\n    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token\n    - name: Setup Terraform\n      uses: hashicorp/setup-terraform@v1\n      with:\n        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\n    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.\n    - name: Terraform Init\n      run: terraform init\n\n    # Checks that all Terraform configuration files adhere to a canonical format\n    - name: Terraform Format\n      run: terraform fmt -check\n\n    # Generates an execution plan for Terraform\n    - name: Terraform Plan\n      run: terraform plan\n\n      # On push to main, build or change infrastructure according to Terraform configuration files\n      # Note: It is recommended to set up a required \"strict\" status check in your repository for \"Terraform Cloud\". See the documentation on \"strict\" required status checks for more information: https://help.github.com/en/github/administering-a-repository/types-of-required-status-checks\n    - name: Terraform Apply\n      if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n      run: terraform apply -auto-approve\n",
    "source": "pabranch/sandbox-github-actions",
    "path": ".github/workflows/install-terraform.yml",
    "url": "https://github.com/pabranch/sandbox-github-actions/blob/b8607ccea20c8e2fa5c4b73374108404710003de/.github/workflows/install-terraform.yml",
    "retrieved_at": "2025-08-15T01:52:00.838262Z"
  },
  {
    "question": "Under what conditions does the workflow execute, given the `on` trigger configuration?",
    "answer": "name: rAPId Dev Deployment\n\non:\n  push:\n    branches:\n      - '**'\n\n  workflow_dispatch:\n\njobs:\n  setup:\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Log commit SHA\n        run: echo $GITHUB_SHA\n\n  security-check:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - run: pip install -r requirements.txt\n\n      - name: Run security checks\n        run: make security-check\n\n  api-dev:\n    needs:\n      - setup\n      - security-check\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          cp ./.github/.github.env .env\n          echo AWS_ACCOUNT=${{ secrets.AWS_ACCOUNT }} >> .env\n          echo AWS_REGION=${{ secrets.AWS_REGION }} >> .env\n          echo AWS_DEFAULT_REGION=${{ secrets.AWS_REGION }} >> .env\n\n      - name: Build API Image\n        run: make api/create-image\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - name: Setup API environment\n        run: make api/setup\n\n      - name: API Static Analysis\n        run: make api/lint\n\n      - name: API Tests\n        run: make api/test\n\n      - name: API Tag and Upload\n        run: make api/tag-and-upload\n\n  sdk-dev:\n    needs:\n      - setup\n      - security-check\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          echo \"TWINE_USERNAME=${{ secrets.TWINE_USERNAME_TEST }}\" >> .env\n          echo \"TWINE_PASSWORD=${{ secrets.TWINE_PASSWORD_TEST }}\" >> .env\n          echo \"TWINE_NON_INTERACTIVE=true\" >> .env\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - name: Setup Python Environment\n        run: |\n          make sdk/setup\n\n      - name: SDK Test\n        run: make sdk/test\n\n      - name: Set env variable\n        run: echo \"TEST_SDK_VERSION=$(date +%Y%m%d%H%M%S)\" >> $GITHUB_ENV\n\n      - name: SDK Test Deploy\n        run: make sdk/release-test\n\n  ui-dev:\n    needs:\n      - setup\n      - security-check\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20\n\n      - name: Install UI Packages\n        run: make ui/setup\n\n      - name: UI Test\n        run: make ui/test\n\n  cleanup:\n    needs:\n      - setup\n      - security-check\n      - api-dev\n      - sdk-dev\n      - ui-dev\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Clean Docker Context\n        if: always()\n        run: make api/clean-docker\n",
    "source": "no10ds/rapid",
    "path": ".github/workflows/dev.yml",
    "url": "https://github.com/no10ds/rapid/blob/c763da3d92b4371effa90f6df460c4d027b899c6/.github/workflows/dev.yml",
    "retrieved_at": "2025-08-15T01:52:01.597911Z"
  },
  {
    "question": "Under what conditions does the `setup` job run?",
    "answer": "name: rAPId Release\n\non:\n  release:\n    types: [released]\n\njobs:\n  setup:\n    if: \"${{ startsWith(github.event.release.name, 'API: ') }}\"\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Log commit SHA\n        run: echo $GITHUB_SHA\n\n  api-release:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          cp ./.github/.github.env .env\n          echo AWS_ACCOUNT=${{ secrets.AWS_ACCOUNT }} >> .env\n          echo AWS_REGION=${{ secrets.AWS_REGION }} >> .env\n\n      - name: Build API Image\n        run: make api/create-image\n\n      - name: API Tag and Upload Release Image\n        run: make api/tag-and-upload-release-image\n\n  ui-release:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20\n\n      - name: Install UI Packages\n        run: make ui/setup\n\n      - name: UI Build Static Files\n        run: make ui/create-static-out\n\n      - name: UI Zip and Release\n        env:\n          TAG: ${{ github.event.release.tag_name }}\n          GH_TOKEN: ${{ github.token }}\n        run: make ui/zip-and-release tag=$TAG\n\n  cleanup:\n    needs:\n      - setup\n      - api-release\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Clean Docker Context\n        if: always()\n        run: make api/clean-docker\n",
    "source": "no10ds/rapid",
    "path": ".github/workflows/release_api.yml",
    "url": "https://github.com/no10ds/rapid/blob/c763da3d92b4371effa90f6df460c4d027b899c6/.github/workflows/release_api.yml",
    "retrieved_at": "2025-08-15T01:52:02.117771Z"
  },
  {
    "question": "On what branches will this workflow run when a push or pull request is made?",
    "answer": "name: Clippy\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - dev\n\njobs:\n  clippy:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y webkit2gtk-4.0\n\n      - name: Install clippy with stable toolchain\n        uses: dtolnay/rust-toolchain@stable\n        with:\n          components: clippy\n\n      - uses: Swatinem/rust-cache@v2\n\n      - run: cargo clippy --manifest-path=Cargo.toml --all-targets --all-features -- -D warnings\n",
    "source": "bagindo/tauri-plugin-keygen",
    "path": ".github/workflows/clippy.yml",
    "url": "https://github.com/bagindo/tauri-plugin-keygen/blob/e0de03a76a5c82c36adb347e43e4bff4f78c87de/.github/workflows/clippy.yml",
    "retrieved_at": "2025-08-15T01:52:02.822724Z"
  },
  {
    "question": "What Docker images are built and pushed by this workflow, and under what conditions does the workflow trigger?",
    "answer": "name: push-poetry-container\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}-ci\n  POETRY_VERSION: 1.3.2\n  PYTHON_PRIMARY_VERSION: 3.10.15\n  PYTHON_PRIMARY_TAG: py310\n  PYTHON_SECONDARY_VERSION: 3.12.4\n  PYTHON_SECONDARY_TAG: py312\n  DEBIAN_VERSION: bullseye\n\non:\n  push:\n    paths:\n      - .devcontainer/Dockerfile\n    branches:\n      - devel\n  workflow_dispatch:\n\njobs:\n  push-poetry-container:\n    runs-on: ubuntu-24.04\n    permissions: \n      contents: read\n      packages: write \n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n    - name: Login to GCR\n      uses: docker/login-action@v3\n      with:\n        registry: ghcr.io\n        username: ${{ github.actor }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n    - name: Metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n    - name: Echo\n      run: |\n        echo \"USER: ${{ github.actor }}\"\n        echo \"REPOSITORY: ${{ github.repository }}\"\n        echo \"POETRY_VERSION: ${POETRY_VERSION}\"\n        echo \"PYTHON_PRIMARY_VERSION: ${PYTHON_PRIMARY_VERSION}\"\n        echo \"PYTHON_SECONDARY_VERSION: ${PYTHON_SECONDARY_VERSION}\"\n        echo \"TAGS: ${{ steps.meta.outputs.tags }}\"\n        echo \"LABELS: ${{ steps.meta.outputs.labels }}\"\n    - name: Image - poetry${{ env.POETRY_VERSION }}-python${{ env.PYTHON_PRIMARY_VERSION }}\n      uses: docker/build-push-action@v6\n      with:\n        file: ./.devcontainer/Dockerfile\n        push: true\n        build-args: |\n          PYTHON_VERSION=${{ env.PYTHON_PRIMARY_VERSION }}\n          POETRY_VERSION=${{ env.POETRY_VERSION }}\n          DEBIAN_VERSION=${{ env.DEBIAN_VERSION }}\n        tags: |\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.PYTHON_PRIMARY_TAG }}-poetry-${{ env.DEBIAN_VERSION }}\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:python${{ env.PYTHON_PRIMARY_VERSION }}-poetry${{ env.POETRY_VERSION }}-${{ env.DEBIAN_VERSION }}\n        labels: ${{ steps.meta.outputs.labels }}\n    - name: Image - poetry${{ env.POETRY_VERSION }}-python${{ env.PYTHON_SECONDARY_VERSION }}\n      uses: docker/build-push-action@v6\n      with:\n        file: ./.devcontainer/Dockerfile\n        push: true\n        build-args: |\n          PYTHON_VERSION=${{ env.PYTHON_SECONDARY_VERSION }}\n          POETRY_VERSION=${{ env.POETRY_VERSION }}\n          DEBIAN_VERSION=${{ env.DEBIAN_VERSION }}\n        tags: |\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.PYTHON_SECONDARY_TAG }}-poetry-${{ env.DEBIAN_VERSION }}\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:python${{ env.PYTHON_SECONDARY_VERSION }}-poetry${{ env.POETRY_VERSION }}-${{ env.DEBIAN_VERSION }}\n        labels: ${{ steps.meta.outputs.labels }}",
    "source": "splintered-reality/py_trees",
    "path": ".github/workflows/push_poetry_container.yaml",
    "url": "https://github.com/splintered-reality/py_trees/blob/d998975a302f0466405aaf7aef185ff5a4a57e3f/.github/workflows/push_poetry_container.yaml",
    "retrieved_at": "2025-08-16T01:46:30.022488Z"
  },
  {
    "question": "How does the workflow ensure consistent versioning across different platforms and build configurations?",
    "answer": "name: Build\n\non: [push, pull_request]\n\njobs:\n  Build:\n    name: ${{ matrix.platform.name }}\n    runs-on: ${{ matrix.platform.os }}\n\n    defaults:\n      run:\n        shell: ${{ matrix.platform.shell }}\n\n    strategy:\n      fail-fast: false\n      matrix:\n        platform:\n        - { name: Windows (mingw32),        os: windows-latest, shell: 'msys2 {0}', msystem: mingw32, msys-env: mingw-w64-i686 }\n        - { name: Windows (mingw64),        os: windows-latest, shell: 'msys2 {0}', msystem: mingw64, msys-env: mingw-w64-x86_64 }\n        - { name: Windows (clang32),        os: windows-latest, shell: 'msys2 {0}', msystem: clang32, msys-env: mingw-w64-clang-i686 }\n        - { name: Windows (clang64),        os: windows-latest, shell: 'msys2 {0}', msystem: clang64, msys-env: mingw-w64-clang-x86_64 }\n        - { name: Windows (ucrt64),         os: windows-latest, shell: 'msys2 {0}', msystem: ucrt64,  msys-env: mingw-w64-ucrt-x86_64 }\n        - { name: Ubuntu 20.04 (CMake),     os: ubuntu-20.04,   shell: sh }\n        - { name: Ubuntu 20.04 (autotools), os: ubuntu-20.04,   shell: sh,    autotools: true }\n        - { name: Ubuntu 22.04 (CMake),     os: ubuntu-22.04,   shell: sh }\n        - { name: Ubuntu 22.04 (autotools), os: ubuntu-22.04,   shell: sh,    autotools: true }\n        - { name: MacOS (CMake),            os: macos-latest,   shell: sh,    cmake: '-DCMAKE_OSX_ARCHITECTURES=\"x86_64;arm64\"' }\n        - { name: MacOS (autotools),        os: macos-latest,   shell: sh,    autotools: true }\n\n    steps:\n    - name: Set up MSYS2\n      if: matrix.platform.shell == 'msys2 {0}'\n      uses: msys2/setup-msys2@v2\n      with:\n        msystem: ${{ matrix.platform.msystem }}\n        install: >-\n          ${{ matrix.platform.msys-env }}-cc\n          ${{ matrix.platform.msys-env }}-cmake\n          ${{ matrix.platform.msys-env }}-ninja\n          ${{ matrix.platform.msys-env }}-pkg-config\n\n    - name: Setup Linux dependencies\n      if: runner.os == 'Linux'\n      run: |\n        sudo apt-get update\n        sudo apt-get install build-essential git make autoconf automake libtool \\\n            pkg-config cmake ninja-build gnome-desktop-testing libasound2-dev libpulse-dev \\\n            libaudio-dev libjack-dev libsndio-dev libsamplerate0-dev libx11-dev libxext-dev \\\n            libxrandr-dev libxcursor-dev libxfixes-dev libxi-dev libxss-dev libwayland-dev \\\n            libxkbcommon-dev libdrm-dev libgbm-dev libgl1-mesa-dev libgles2-mesa-dev \\\n            libegl1-mesa-dev libdbus-1-dev libibus-1.0-dev libudev-dev fcitx-libs-dev\n\n    - name: Setup extra Ubuntu 22.04 dependencies\n      if: matrix.platform.os == 'ubuntu-22.04'\n      run: |\n        sudo apt-get install libpipewire-0.3-dev libdecor-0-dev\n\n    - name: Setup Macos dependencies\n      if: runner.os == 'macOS'\n      run: |\n        brew install \\\n          ninja\n    - uses: actions/checkout@v3\n    - name: Check that versioning is consistent\n      # We only need to run this once: arbitrarily use the Linux/CMake build\n      if: \"runner.os == 'Linux' && ! matrix.platform.autotools\"\n      run: ./build-scripts/test-versioning.sh\n    - name: Configure (CMake)\n      if: \"! matrix.platform.autotools\"\n      run: |\n        cmake -S . -B build -G Ninja \\\n        -DSDL_TESTS=ON \\\n        -DSDL_WERROR=ON \\\n        -DSDL_INSTALL_TESTS=ON \\\n        -DSDL_VENDOR_INFO=\"Github Workflow\" \\\n        -DCMAKE_INSTALL_PREFIX=cmake_prefix \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        ${{ matrix.platform.cmake }}\n    - name: Build (CMake)\n      if: \"! matrix.platform.autotools\"\n      run: |\n        cmake --build build/ --config Release --verbose --parallel\n    - name: Run build-time tests (CMake)\n      if: \"! matrix.platform.autotools\"\n      run: |\n        set -eu\n        export SDL_TESTS_QUICK=1\n        ctest -VV --test-dir build/\n        if test \"${{ runner.os }}\" = \"Linux\"; then\n          # This should show us the SDL_REVISION\n          strings build/libSDL2-2.0.so.0 | grep SDL-\n        fi\n    - name: Install (CMake)\n      if: \"! matrix.platform.autotools\"\n      run: |\n        set -eu\n        cmake --install build/ --config Release\n        echo \"SDL2_DIR=$(pwd)/cmake_prefix\" >> $GITHUB_ENV\n        ( cd cmake_prefix; find ) | LC_ALL=C sort -u\n    - name: Configure (Autotools)\n      if: matrix.platform.autotools\n      run: |\n        set -eu\n        rm -fr build-autotools\n        mkdir build-autotools\n        ./autogen.sh\n        (\n          cd build-autotools\n          ${{ github.workspace }}/configure \\\n            --enable-vendor-info=\"Github Workflow\" \\\n            --enable-werror \\\n            --prefix=${{ github.workspace }}/autotools_prefix \\\n        )\n        if test \"${{ runner.os }}\" != \"macOS\" ; then\n          curdir=\"$(pwd)\"\n          multiarch=\"$(dpkg-architecture -qDEB_HOST_MULTIARCH)\"\n          (\n            mkdir -p build-autotools/test\n            cd build-autotools/test\n            ${{ github.workspace }}/test/configure \\\n              --enable-werror \\\n              --x-includes=/usr/include \\\n              --x-libraries=\"/usr/lib/${multiarch}\" \\\n              --prefix=${{ github.workspace }}/autotools_prefix \\\n              SDL_CFLAGS=\"-I${curdir}/include\" \\\n              SDL_LIBS=\"-L${curdir}/build-autotools/build/.libs -lSDL2\" \\\n              ac_cv_lib_SDL2_ttf_TTF_Init=no \\\n              ${NULL+}\n          )\n        fi\n    - name: Build (Autotools)\n      if: matrix.platform.autotools\n      run: |\n        set -eu\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        make -j\"${parallel}\" -C build-autotools V=1\n        if test \"${{ runner.os }}\" != \"macOS\" ; then\n          make -j\"${parallel}\" -C build-autotools/test V=1\n        fi\n    - name: Run build-time tests (Autotools)\n      if: ${{ matrix.platform.autotools && (runner.os != 'macOS') }}\n      run: |\n        set -eu\n        curdir=\"$(pwd)\"\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        export SDL_TESTS_QUICK=1\n        make -j\"${parallel}\" -C build-autotools/test check LD_LIBRARY_PATH=\"${curdir}/build-autotools/build/.libs\"\n        if test \"${{ runner.os }}\" = \"Linux\"; then\n          # This should show us the SDL_REVISION\n          strings \"${curdir}/build-autotools/build/.libs/libSDL2-2.0.so.0\" | grep SDL-\n        fi\n    - name: Install (Autotools)\n      if: matrix.platform.autotools\n      run: |\n        set -eu\n        curdir=\"$(pwd)\"\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        make -j\"${parallel}\" -C build-autotools install V=1\n        if test \"${{ runner.os }}\" != \"macOS\" ; then\n          make -j\"${parallel}\" -C build-autotools/test install V=1\n        fi\n        ( cd autotools_prefix; find . ) | LC_ALL=C sort -u\n        echo \"SDL2_DIR=$(pwd)/autotools_prefix\" >> $GITHUB_ENV\n    - name: Verify CMake configuration files\n      run: |\n        cmake -S cmake/test -B cmake_config_build -G Ninja \\\n          -DCMAKE_BUILD_TYPE=Release \\\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }}\n        cmake --build cmake_config_build --verbose\n    - name: Verify sdl2-config\n      run: |\n        export PATH=${{ env.SDL2_DIR }}/bin:$PATH\n        cmake/test/test_sdlconfig.sh\n    - name: Verify sdl2.pc\n      run: |\n        export PKG_CONFIG_PATH=${{ env.SDL2_DIR }}/lib/pkgconfig\n        cmake/test/test_pkgconfig.sh\n    - name: Distcheck (Autotools)\n      if: matrix.platform.autotools\n      run: |\n        set -eu\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        make -j\"${parallel}\" -C build-autotools dist V=1\n        # Similar to Automake `make distcheck`: check that the tarball\n        # release is sufficient to do a new build\n        mkdir distcheck\n        tar -C distcheck -zxf build-autotools/SDL2-*.tar.gz\n        ( cd distcheck/SDL2-* && ./configure )\n        make -j\"${parallel}\" -C distcheck/SDL2-*\n    - name: Run installed-tests (Autotools)\n      if: \"runner.os == 'Linux' && matrix.platform.autotools\"\n      run: |\n        set -eu\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        sudo make -j\"${parallel}\" -C build-autotools install\n        sudo make -j\"${parallel}\" -C build-autotools/test install\n        export SDL_TESTS_QUICK=1\n        # We need to set LD_LIBRARY_PATH because it isn't in the default\n        # linker search path. We don't need to set XDG_DATA_DIRS for\n        # ginsttest-runner, because /usr/local/share *is* in the default\n        # search path for that.\n        env --chdir=/ \\\n            LD_LIBRARY_PATH=/usr/local/lib \\\n            SDL_AUDIODRIVER=dummy \\\n            SDL_VIDEODRIVER=dummy \\\n            ginsttest-runner --tap SDL2\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/main.yml",
    "retrieved_at": "2025-08-16T01:46:30.986219Z"
  },
  {
    "question": "What triggers this workflow to run, and what specific event type is it looking for?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "Guizimo/d2-manage-client",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/Guizimo/d2-manage-client/blob/260866da20e5e876de825e6e3fadc8b5b7736ce6/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-16T01:46:31.623756Z"
  },
  {
    "question": "What specific action does the `notify_simulator_failure.py` script perform using the provided environment variables and payload data?",
    "answer": "name: Notify Notifications chat\n\non:\n  repository_dispatch:\n    types: [notify-simulator-failure]\n\njobs:\n  send-chat-message:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v2\n\n      - name: Set up JDK 17.0.7\n        uses: actions/setup-java@v2\n        with:\n          distribution: 'temurin'\n          java-version: '17.0.7'\n\n      - name: Set up Python 3.x\n        uses: actions/setup-python@v1\n        with:\n          python-version: '3.x'\n\n      - name: Install Python packages\n        run: |\n          pip install cryptography\n          pip install httplib2\n          pip install PyGithub\n\n      - name: Wget required files\n        run: |\n          python3 dependabot/notify_simulator_failure.py ${{ github.event.client_payload.branch }} ${{ github.event.client_payload.runId }} \n        env:\n          CHAT_ID: ${{ secrets.NOTIFICATIONS_CHAT_ID }}\n          CHAT_KEY: ${{ secrets.NOTIFICATIONS_CHAT_KEY }}\n          CHAT_TOKEN: ${{ secrets.NOTIFICATIONS_CHAT_TOKEN }}\n          ENV_USER_ENCRYPTION_KEY: ${{secrets.USER_ENCRYPTION_KEY}}\n          BALLERINA_BOT_TOKEN: ${{ secrets.BALLERINA_BOT_TOKEN }}\n",
    "source": "ballerina-platform/ballerina-release",
    "path": ".github/workflows/notify-simulator-failure.yml",
    "url": "https://github.com/ballerina-platform/ballerina-release/blob/fe827e460fa2d4cf8bd36a15fe14035fea3c78a2/.github/workflows/notify-simulator-failure.yml",
    "retrieved_at": "2025-08-16T01:46:32.516931Z"
  },
  {
    "question": "What specific static checks and security checks are performed by the `staticcheck` job?",
    "answer": "on:\n  push:\n    branches:\n      - 'master'\n      - 'develop'\nname: push_master_develop\njobs:\n  staticcheck:\n    runs-on: ubuntu-latest\n    steps:\n    - name: install Go\n      uses: actions/setup-go@v2\n      with:\n        go-version: 1.18.x\n    - name: checkout code\n      uses: actions/checkout@v2\n      with:\n        fetch-depth: 0\n    - uses: actions/cache@v2\n      with:\n        path: |\n          ~/go/pkg/mod\n          ~/.cache/go-build\n          ~/Library/Caches/go-build\n          %LocalAppData%\\go-build\n        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}\n        restore-keys: |\n          ${{ runner.os }}-go-\n    - name: install deps\n      run: go install golang.org/x/tools/cmd/goimports@latest && go install github.com/klauspost/asmfmt/cmd/asmfmt@latest\n    - name: gofmt\n      run: if [[ -n $(gofmt -l .) ]]; then echo \"please run gofmt\"; exit 1; fi\n    - name: go vet\n      run: go vet ./...\n    - name: staticcheck\n      run: |\n        go install honnef.co/go/tools/cmd/staticcheck@23e1086441d24fed9f668ad1cd4374245118b590\n        staticcheck ./...\n    - name: gosec\n      run: |\n        go install github.com/securego/gosec/v2/cmd/gosec@latest\n        gosec -exclude G204 ./...\n    - name: generated files should not be modified\n      run: |\n        go generate ./...\n        git update-index --assume-unchanged go.mod\n        git update-index --assume-unchanged go.sum\n        if [[ -n $(git status --porcelain) ]]; then echo \"git repo is dirty after runing go generate -- please don't modify generated files\"; echo $(git diff);echo $(git status --porcelain); exit 1; fi\n  \n  test:\n    strategy:\n      matrix:\n        go-version: [1.17.x, 1.18.x]\n        os: [ubuntu-latest, windows-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    needs:\n      - staticcheck\n    steps:\n    - name: install Go\n      uses: actions/setup-go@v2\n      with:\n        go-version: ${{ matrix.go-version }}\n    - name: checkout code\n      uses: actions/checkout@v2\n    - uses: actions/cache@v2\n      with:\n        path: |\n          ~/go/pkg/mod\n          ~/.cache/go-build\n          ~/Library/Caches/go-build\n          %LocalAppData%\\go-build\n        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}\n        restore-keys: |\n          ${{ runner.os }}-go-\n    - name: install deps\n      run: go install golang.org/x/tools/cmd/goimports@latest && go install github.com/klauspost/asmfmt/cmd/asmfmt@latest\n    - name: Test\n      run: |\n        go test -v -timeout=30m ./...\n    - name: Test (race)\n      if: matrix.os == 'ubuntu-latest'\n      run: |\n        go test -v -timeout=50m -race -short ./...\n  \n  slack-workflow-status:\n    if: always()\n    name: post workflow status to slack\n    needs:\n      - staticcheck\n      - test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Build notification\n        uses: Gamesight/slack-workflow-status@master\n        with:\n          repo_token: ${{secrets.GITHUB_TOKEN}}\n          slack_webhook_url: ${{secrets.SLACK_WEBHOOK_URL}}\n          channel: '#team-gnark-build'",
    "source": "dreamATD/pianist-gnark",
    "path": ".github/workflows/push.yml",
    "url": "https://github.com/dreamATD/pianist-gnark/blob/5685973eda77c38fbf2a4d58168e4ec531474b86/.github/workflows/push.yml",
    "retrieved_at": "2025-08-16T01:46:33.214967Z"
  },
  {
    "question": "Under what conditions will the \"Snifftest\" workflow be triggered?",
    "answer": "name: Snifftest\n\non:\n  push:\n    tags:\n      - v*\n    branches:\n      - main\n\npermissions:\n  contents: read\n  pull-requests: read\n\njobs:\n  Snifftest:\n    name: Run Snifftest\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/setup-go@v5\n        with:\n          go-version: \"1.22\"\n      - uses: actions/checkout@v4\n      - name: Run Snifftest\n        run: make snifftest\n",
    "source": "Marshal-EASM/webssi",
    "path": ".github/workflows/snifftest.yml",
    "url": "https://github.com/Marshal-EASM/webssi/blob/25053d6b6cd204954d4caf23a5fabbca2d8324b0/.github/workflows/snifftest.yml",
    "retrieved_at": "2025-08-16T01:46:33.998778Z"
  },
  {
    "question": "Under what conditions will this workflow automatically approve a pull request?",
    "answer": "name: Auto approve\n\non:\n  pull_request_target:\n    types: [labeled]\n\njobs:\n  # Auto-approve dependabot PRs since this repo requires at least one approving review.\n  # Dependabot will automatically merge minor version upgrades\n  # (see .dependabot/config.yml for more info).\n  auto-approve-dependabot:\n    runs-on: ubuntu-latest\n    if: github.actor == 'dependabot[bot]' && contains(github.event.pull_request.labels.*.name, 'dependencies')\n    steps:\n      - uses: hmarr/auto-approve-action@v2.1.0\n        with:\n          github-token: \"${{ secrets.GITHUB_TOKEN }}\"\n",
    "source": "aws-actions/action-cloudwatch-metrics",
    "path": ".github/workflows/autoapprove.yml",
    "url": "https://github.com/aws-actions/action-cloudwatch-metrics/blob/36cfe8e2237e43b1129fbd3807621508657ff430/.github/workflows/autoapprove.yml",
    "retrieved_at": "2025-08-16T01:46:34.711437Z"
  },
  {
    "question": "What tracking methods and YOLO models are tested in the \"Tests all tracking options\" step?",
    "answer": "# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.9', '3.10']\n        #model: ['yolov8n', 'yolo_nas_s', yolox_n]  # yolo models to test\n        #tracking-methods: ['deepocsort', 'ocsort', 'botsort', 'strongsort', 'bytetrack']  # tracking methods to  test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n\n      - uses: actions/checkout@v4  # Check out the repository\n      - uses: actions/setup-python@v4  # Prepare environment with python 3.9\n        with:\n          python-version: ${{ matrix.python-version }}\n          cache: 'pip' # caching pip dependencies\n      - name: Install requirements\n        shell: bash  # for Windows compatibility\n        run: |\n          python -m pip install --upgrade pip setuptools wheel\n          pip install -e . pytest pytest-cov --extra-index-url https://download.pytorch.org/whl/cpu\n          python --version\n          pip --version\n          pip list\n\n      - name: Tests all tracking options\n        shell: bash  # for Windows compatibility\n        env:\n          IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n        run: |\n          # deepocsort fro all supported yolo models\n          python examples/track.py --tracking-method deepocsort --source $IMG --imgsz 320 --reid-model examples/weights/clip_market1501.pt\n          python examples/track.py --yolo-model yolo_nas_s --tracking-method deepocsort --source $IMG --imgsz 320\n          python examples/track.py --yolo-model yolox_n --tracking-method deepocsort --source $IMG --imgsz 320\n\n          # hybridsort\n          python examples/track.py --tracking-method hybridsort --source $IMG --imgsz 320\n\n          # botsort\n          python examples/track.py --tracking-method botsort --source $IMG --imgsz 320\n\n          # strongsort\n          python examples/track.py --tracking-method strongsort --source $IMG --imgsz 320\n\n          # ocsort\n          python examples/track.py --tracking-method ocsort --source $IMG --imgsz 320\n\n          # bytetrack\n          python examples/track.py --tracking-method bytetrack --source $IMG --imgsz 320\n\n      - name: Pytest tests  # after tracking options as this does not download models\n        shell: bash  # for Windows compatibility\n        run: |\n\n          # needed in TFLite export\n          wget https://github.com/PINTO0309/onnx2tf/releases/download/1.7.3/flatc.tar.gz\n          tar -zxvf flatc.tar.gz\n          sudo chmod +x flatc\n          sudo mv flatc /usr/bin/\n\n          pytest --cov=$PACKAGE_DIR --cov-report=html -v tests\n          coverage report --fail-under=$COVERAGE_FAIL_UNDER\n\n      - name: Tests exported reid models\n        env:\n            IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n        shell: bash  # for Windows compatibility\n        run: |\n\n          # test exported reid model\n          python examples/track.py --reid-model examples/weights/osnet_x0_25_msmt17.torchscript                                   --source $IMG --imgsz 320\n          python examples/track.py --reid-model examples/weights/osnet_x0_25_msmt17.onnx                                          --source $IMG --imgsz 320\n          python examples/track.py --reid-model examples/weights/osnet_x0_25_msmt17_saved_model/osnet_x0_25_msmt17_float16.tflite --source $IMG --imgsz 320\n          python examples/track.py --reid-model examples/weights/osnet_x0_25_msmt17_openvino_model                                --source $IMG --imgsz 320\n\n      - name: Test tracking with seg models\n        env:\n            IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n        shell: bash  # for Windows compatibility\n        run: |\n          # tracking with SEG models\n          python examples/track.py --tracking-method deepocsort --yolo-model yolov8n-seg.pt --source $IMG\n\n      - name: Test tracking with pose models\n        env:\n          IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n        shell: bash  # for Windows compatibility\n        run: |\n          # tracking with POSE models\n          python3 examples/track.py --yolo-model weights/yolov8n.pt --source $IMG --imgsz 320\n\n      - name: Test validation on MOT17 subset\n        shell: bash  # for Windows compatibility\n        run: |\n          # validation on a few MOT17 imges\n          python examples/val.py --tracking-method deepocsort --yolo-model yolov8n.pt --benchmark MOT17-mini --imgsz 320 --conf 0.2\n\n      - name: Test evolution on MOT17 subset\n        shell: bash  # for Windows compatibility\n        run: |\n          # evolve a for a single set of parameters\n          python examples/evolve.py --objectives HOTA,MOTA,IDF1 --benchmark MOT17-mini --n-trials 1 --imgsz 320 --conf 0.2\n",
    "source": "rustoneee/Yolo-Tracking",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/rustoneee/Yolo-Tracking/blob/a64e0c5886927c3c3d6080e50a5cffd63a343e52/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-16T01:46:35.410384Z"
  },
  {
    "question": "What platforms are targeted by this workflow and what artifacts are produced for each?",
    "answer": "# This is a basic workflow to help you get started with Actions\n\nname: Build\n\n# Controls when the action will run.\non:\n  # Triggers the workflow on push or pull request events but only for the master branch\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n  buildLinux:\n    # The type of runner that the job will run on\n    runs-on: ubuntu-latest\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      - uses: krdlab/setup-haxe@master\n        with:\n          haxe-version: 4.2.5\n      # Runs a set of commands using the runners shell\n      - name: Install Haxelib\n        run: |\n          sudo apt-get install libvlc-dev\n          sudo apt-get install libvlccore-dev\n          haxelib setup ~/haxelib\n          haxelib install hxcpp > /dev/null\n          haxelib install lime\n          haxelib install openfl\n          haxelib --never install flixel\n          haxelib run lime setup flixel\n          haxelib run lime setup\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install flixel-addons\n          haxelib install tjson\n          haxelib install hxjsonast\n          haxelib install hxCodec\n          haxelib git linc_luajit https://github.com/nebulazorua/linc_luajit\n          haxelib install hscript\n          haxelib git hscript-ex https://github.com/ianharrigan/hscript-ex\n          haxelib git discord_rpc https://github.com/Aidan63/linc_discord-rpc\n          haxelib install hxcpp-debug-server\n          haxelib list\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n      - name: Compile\n        run: haxelib run lime build Project.xml linux --app-version=\"4.0.0-${{ github.run_id}}\"\n      - name: Publish Artifact\n        uses: actions/upload-artifact@v2.2.4\n        with:\n          name: linuxBuild\n          path: 'export/release/linux/bin'\n  buildWindows:\n    runs-on: windows-latest\n\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2.3.0\n\n      - uses: krdlab/setup-haxe@master\n        with:\n          haxe-version: 4.2.5\n      # Runs a set of commands using the runners shell\n      - name: Install Haxelib\n        run: |\n          haxelib setup C:/haxelib\n          haxelib install hxcpp > nul\n          haxelib install lime\n          haxelib install openfl\n          haxelib --never install flixel\n          haxelib run lime setup flixel\n          haxelib run lime setup\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install flixel-addons\n          haxelib install tjson\n          haxelib install hxjsonast\n          haxelib install hxCodec\n          haxelib git linc_luajit https://github.com/nebulazorua/linc_luajit\n          haxelib install hscript\n          haxelib git hscript-ex https://github.com/ianharrigan/hscript-ex\n          haxelib git discord_rpc https://github.com/Aidan63/linc_discord-rpc\n          haxelib install hxcpp-debug-server\n          haxelib list\n        shell: cmd\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n      - name: Compile\n        run: haxelib run lime build windows --app-version=\"4.0.0-${{ github.run_id}}\"\n      - name: Publish Artifact\n        uses: actions/upload-artifact@v2.2.4\n        with:\n          name: windowsBuild\n          path: export/release/windows/bin\n  buildMac:\n    runs-on: macos-latest\n\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      - uses: krdlab/setup-haxe@master\n        with:\n          haxe-version: 4.2.5\n      # Runs a set of commands using the runners shell\n      - name: Install Haxelib\n        run: |\n          haxelib setup ~/haxelib\n          haxelib install hxcpp > /dev/null\n          haxelib install lime\n          haxelib install openfl\n          haxelib --never install flixel\n          haxelib run lime setup flixel\n          haxelib run lime setup\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install flixel-addons\n          haxelib install tjson\n          haxelib install hxjsonast\n          haxelib install hxCodec\n          haxelib git linc_luajit https://github.com/nebulazorua/linc_luajit\n          haxelib install hscript\n          haxelib git hscript-ex https://github.com/ianharrigan/hscript-ex\n          haxelib git discord_rpc https://github.com/Aidan63/linc_discord-rpc\n          haxelib install hxcpp-debug-server\n          haxelib list\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n      - name: Compile\n        run: haxelib run lime build mac --app-version=\"4.0.0-${{ github.run_id}}\"\n      - name: Publish Artifact\n        uses: actions/upload-artifact@v2.2.4\n        with:\n          name: macBuild\n          path: export/release/macos/bin\n  buildAndroid:\n    name: buildAndroid\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.1\n\n      - name: Setup Android NDK\n        uses: nttld/setup-ndk@v1\n        id: setup-ndk\n        with:\n          ndk-version: r21e\n\n      - name: Setup Java JDK\n        uses: actions/setup-java@v1\n        with:\n          java-version: 11\n\n      - name: Setup Android SDK\n        uses: android-actions/setup-android@v2\n\n      - name: Setup Haxe\n        uses: krdlab/setup-haxe@v1.1.5\n        with:\n          haxe-version: 4.2.5\n\n      - name: Install Haxelib\n        run: |\n          haxelib setup ~/haxelib\n          haxelib install hxcpp > /dev/null\n          haxelib install lime\n          haxelib install openfl\n          haxelib --never install flixel\n          haxelib run lime setup flixel\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install hscript\n          haxelib install flixel-addons\n          haxelib install hxCodec\n          haxelib git linc_luajit https://github.com/jigsaw-4277821/linc_luajit.git\n          haxelib git extension-androidtools https://github.com/InsKal/extension-androidtools.git\n          haxelib list\n\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n\n      - name: Setup Lime\n        run: |\n          haxelib run lime setup -alias -y\n          haxelib run lime config ANDROID_SDK $ANDROID_HOME\n          haxelib run lime config ANDROID_NDK_ROOT $ANDROID_NDK_HOME\n          haxelib run lime config JAVA_HOME $JAVA_HOME\n          haxelib run lime config ANDROID_SETUP true\n        env:\n          ANDROID_NDK_HOME: ${{ steps.setup-ndk.outputs.ndk-path }}\n\n      - name: Compile\n        run: haxelib run lime build android --app-version=\"4.0.0-${{ github.run_id}}\"\n\n      - name: Publish Artifact\n        uses: actions/upload-artifact@v2.2.4\n        with:\n          name: androidBuild\n          path: export/release/android/bin/app/build/outputs/apk/debug\n\n",
    "source": "InsKal/Psych-Engine-With-Android-Shaders-And-More-Callbacks",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/InsKal/Psych-Engine-With-Android-Shaders-And-More-Callbacks/blob/e2e219d9e6c51bf17faa07fac99f7eb5d5106ad9/.github/workflows/main.yml",
    "retrieved_at": "2025-08-16T01:46:36.367065Z"
  },
  {
    "question": "What is the purpose of the `build_cpu`, `build_cu101`, and `build_cu102` jobs, and how do they differ?",
    "answer": "name: build\n\non: [push, pull_request]\n\njobs:\n  lint:\n    runs-on: ubuntu-18.04\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python 3.7\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.7\n      - name: Install pre-commit hook\n        run: |\n          pip install pre-commit\n          pre-commit install\n      - name: Linting\n        run: pre-commit run --all-files\n      - name: Check docstring coverage\n        run: |\n          pip install interrogate\n          interrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex \"__repr__\" --fail-under 80 mmaction\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.7.0, 1.9.0]\n        include:\n          - torch: 1.5.0\n            torchvision: 0.6.0\n          - torch: 1.7.0\n            torchvision: 0.8.1\n          - torch: 1.9.0\n            torchvision: 0.10.0\n            python-version: 3.7\n          - torch: 1.9.0\n            torchvision: 0.10.0\n            python-version: 3.8\n          - torch: 1.9.0\n            torchvision: 0.10.0\n            python-version: 3.9\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install soundfile lib\n        run: sudo apt-get install -y libsndfile1\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install librosa and soundfile\n        run: pip install librosa soundfile\n      - name: Install lmdb\n        run: pip install lmdb\n      - name: Install TurboJpeg lib\n        run: sudo apt-get install -y libturbojpeg\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch${{matrix.torch}}/index.html\n      - name: Install MMDet\n        run: pip install git+https://github.com/open-mmlab/mmdetection/\n      - name: Install MMCls\n        run: pip install git+https://github.com/open-mmlab/mmclassification/\n      - name: Install unittest dependencies\n        run: pip install -r requirements/tests.txt -r requirements/optional.txt\n      - name: Install PytorchVideo\n        run: pip install pytorchvideo\n        if: ${{matrix.torchvision == '0.10.0'}}\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --source mmaction -m pytest tests/\n          coverage xml\n          coverage report -m\n  build_cu101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0+cu101, 1.6.0+cu101, 1.7.0+cu101]\n        include:\n          - torch: 1.5.0+cu101\n            torch_version: torch1.5.0\n            torchvision: 0.6.0+cu101\n          - torch: 1.6.0+cu101\n            torch_version: torch1.6.0\n            torchvision: 0.7.0+cu101\n          - torch: 1.7.0+cu101\n            torch_version: torch1.7.0\n            torchvision: 0.8.1+cu101\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install CUDA\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libturbojpeg libsndfile1 libsm6 libxrender-dev libxext6 python${{matrix.python-version}}-dev\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Install librosa and soundfile\n        run: python -m pip install librosa soundfile\n      - name: Install lmdb\n        run: python -m pip install lmdb\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmaction dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -q git+https://github.com/open-mmlab/mmdetection/\n          python -m pip install -q git+https://github.com/open-mmlab/mmclassification/\n          python -m pip install -r requirements.txt\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --source mmaction -m pytest tests/\n          coverage xml\n          coverage report -m\n      # Only upload coverage report for python3.7 && pytorch1.5\n      - name: Upload coverage to Codecov\n        if: ${{matrix.torch == '1.5.0+cu101' && matrix.python-version == '3.7'}}\n        uses: codecov/codecov-action@v1.0.14\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cu102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.9.0+cu102]\n        include:\n          - torch: 1.9.0+cu102\n            torch_version: torch1.9.0\n            torchvision: 0.10.0+cu102\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install CUDA\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libturbojpeg libsndfile1 libsm6 libxrender-dev libxext6 python${{matrix.python-version}}-dev\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Install librosa and soundfile\n        run: python -m pip install librosa soundfile\n      - name: Install lmdb\n        run: python -m pip install lmdb\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmaction dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -q git+https://github.com/open-mmlab/mmdetection/\n          python -m pip install -q git+https://github.com/open-mmlab/mmclassification/\n          python -m pip install -r requirements.txt\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install PytorchVideo\n        run: python -m pip install pytorchvideo\n        if: ${{matrix.torchvision == '0.10.0+cu102'}}\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --source mmaction -m pytest tests/\n          coverage xml\n          coverage report -m\n",
    "source": "rehohoho/mmaction2",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/rehohoho/mmaction2/blob/b1e962ab97d4844452c90d57800e733f3425128a/.github/workflows/build.yml",
    "retrieved_at": "2025-08-16T01:46:37.485900Z"
  },
  {
    "question": "What specific tests are executed by the `run-multiple.sh` script?",
    "answer": "# Runs randomly generated E2E testnets nightly on the 0.34.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-34x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01']\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.18'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.34.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"::set-output name=branch::`git branch --show-current`\"\n          echo \"::set-output name=commit::`git rev-parse HEAD`\"\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 2 -d networks/nightly -p\n\n      - name: Run testnets in group ${{ matrix.group }}\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n      git-commit: ${{ steps.git-info.outputs.commit }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMIT_URL: \"${{ github.server_url }}/${{ github.repository }}/commit/${{ needs.e2e-nightly-test.outputs.git-commit }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMIT_URL }}|commit> that caused the failure.\"\n                  }\n                }\n              ]\n            }\n\n  e2e-nightly-success:  # may turn this off once they seem to pass consistently\n    needs: e2e-nightly-test\n    if: ${{ success() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on success\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":white_check_mark: Nightly E2E tests for `${{ env.BRANCH }}` passed.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "lightmos/lightmosbft",
    "path": ".github/workflows/e2e-nightly-34x.yml",
    "url": "https://github.com/lightmos/lightmosbft/blob/6f48b680f6d1bc32abf7f2ad91e84d3d53e7f96d/.github/workflows/e2e-nightly-34x.yml",
    "retrieved_at": "2025-08-17T01:57:17.819463Z"
  },
  {
    "question": "Under what conditions will the workflow's build job be skipped?",
    "answer": "#name: Release\n#on:\n#  push:\n#    branches:\n#      - main\n#\n#jobs:\n#  build:\n#    if: \"!contains(github.event.commits[0].message, '[skip ci]')\"\n#    strategy:\n#      matrix:\n#        include:\n#          - os: macos-latest\n#            gradle_args: assemble publishIosX64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishIosArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishIosSimulatorArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishTvosX64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishTvosArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishTvosSimulatorArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishWatchosArm32PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishWatchosArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishWatchosX64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishWatchosSimulatorArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishMacosX64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishMacosArm64PublicationToSonatypeRepository\n#          - os: ubuntu-latest\n#            gradle_args: assemble publishKotlinMultiplatformPublicationToSonatypeRepository publishJvmPublicationToSonatypeRepository publishLinuxX64PublicationToSonatypeRepository publishLinuxArm64PublicationToSonatypeRepository publishMingwX64PublicationToSonatypeRepository\n#\n#    runs-on: ${{ matrix.os }}\n#    steps:\n#      - name: Checkout project sources\n#        uses: actions/checkout@v3\n#      - uses: actions/setup-java@v3\n#        with:\n#          distribution: liberica\n#          java-version: 8\n#      - name: Build using gradle\n#        uses: gradle/gradle-build-action@v2\n#        with:\n#          arguments: ${{ matrix.gradle_args }}\n#        env:\n#          SIGNING_SECRET_KEY: ${{ secrets.SIGNING_SECRET_KEY }}\n#          SIGNING_PASSWORD: ${{ secrets.SIGNING_PASSWORD }}\n#          OSSRH_USERNAME: ${{ secrets.OSSRH_USERNAME }}\n#          OSSRH_PASSWORD: ${{ secrets.OSSRH_PASSWORD }}\n#          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "source": "ton-blockchain/ton-kotlin",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/ton-blockchain/ton-kotlin/blob/29b1b1d3cc91ebafab70af54cc4a19b7ec81ca5a/.github/workflows/release.yml",
    "retrieved_at": "2025-08-17T01:57:18.530171Z"
  },
  {
    "question": "What triggers the `Mac MPS` workflow, and how does it handle concurrent runs?",
    "answer": "name: Mac MPS\n\non:\n  push:\n    tags:\n      - ciflow/mps/*\n  workflow_dispatch:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref_name }}-${{ github.ref_type == 'branch' && github.sha }}-${{ github.event_name == 'workflow_dispatch' }}\n  cancel-in-progress: true\n\njobs:\n  macos-12-py3-arm64-build:\n    name: macos-12-py3-arm64\n    uses: ./.github/workflows/_mac-build.yml\n    with:\n      sync-tag: macos-12-py3-arm64-build\n      build-environment: macos-12-py3-arm64\n      xcode-version: \"13.3.1\"\n      runner-type: macos-12-xl\n      build-generates-artifacts: true\n      # To match the one pre-installed in the m1 runners\n      python_version: 3.9.12\n      # We need to set the environment file here instead of trying to detect it automatically because\n      # MacOS arm64 is cross-compiled from x86-64. Specifically, it means that arm64 conda environment\n      # is needed when building PyTorch MacOS arm64 from x86-64\n      environment-file: .github/requirements/conda-env-macOS-ARM64\n    secrets:\n      MACOS_SCCACHE_S3_ACCESS_KEY_ID: ${{ secrets.MACOS_SCCACHE_S3_ACCESS_KEY_ID }}\n      MACOS_SCCACHE_S3_SECRET_ACCESS_KEY: ${{ secrets.MACOS_SCCACHE_S3_SECRET_ACCESS_KEY }}\n\n  macos-12-py3-arm64-mps-test:\n    name: macos-12-py3-arm64-mps\n    uses: ./.github/workflows/_mac-test-mps.yml\n    needs: macos-12-py3-arm64-build\n    with:\n      sync-tag: macos-12-py3-arm64-mps-test\n      build-environment: macos-12-py3-arm64\n\n  macos-13-py3-arm64-mps-test:\n    name: macos-13-py3-arm64-mps\n    uses: ./.github/workflows/_mac-test-mps.yml\n    needs: macos-12-py3-arm64-build\n    with:\n      build-environment: macos-12-py3-arm64\n      runs-on: macos-m1-13\n",
    "source": "UEFI-code/PyTorch_For_PoorGuys",
    "path": ".github/workflows/mac-mps.yml",
    "url": "https://github.com/UEFI-code/PyTorch_For_PoorGuys/blob/a66ed97b99aa81e94e5700e495e62fdaa97ecf00/.github/workflows/mac-mps.yml",
    "retrieved_at": "2025-08-17T01:57:19.222156Z"
  },
  {
    "question": "What actions are triggered when a pull request is made to the `main` branch?",
    "answer": "name: Release Charts\n\non:\n  pull_request:\n    branches:\n      - main\n\njobs:\n  lint-test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: Set up Helm\n        uses: azure/setup-helm@v3\n        with:\n          version: v3.9.0\n\n      - name: Add Bitnami\n        run: helm repo add bitnami https://charts.bitnami.com/bitnami\n\n      - uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n\n      - name: Set up chart-testing\n        uses: helm/chart-testing-action@v2.3.0\n\n      - name: Run chart-testing (list-changed)\n        id: list-changed\n        run: |\n          changed=$(ct list-changed)\n          if [[ -n \"$changed\" ]]; then\n            echo \"::set-output name=changed::true\"\n          fi\n\n      - name: Run chart-testing (lint)\n        run: ct lint --check-version-increment=false\n\n      - name: Create kind cluster\n        uses: helm/kind-action@v1.3.0\n        # if: steps.list-changed.outputs.changed == 'true'\n        # TODO for some reason, the earlier chart-testing logic is never seeing any changes, though running it locally\n        # does show changes. This remains a mystery between a lack of any debug logging, but simply running tests\n        # always is fine\n\n      - name: Run chart-testing (install)\n        run: ct install --all\n\n  integration-test:\n    # Not a real requirement, but due to worker CPU contention, this can fail if it runs concurrent with lint-test\n    needs: lint-test\n    runs-on: ubuntu-latest\n    steps:\n      - name: checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: setup helm\n        uses: azure/setup-helm@v3\n        with:\n          version: v3.9.0\n\n      - name: setup testing environment (kind-cluster)\n        run: ./scripts/test-env.sh\n\n      - name: run integration tests (integration)\n        run: ./scripts/test-run.sh\n\n      - name: run upgrade integration tests (integration-upgrade)\n        run: ./scripts/test-upgrade.sh\n\n      - name: cleanup integration tests (cleanup)\n        run: ./scripts/test-env.sh cleanup\n",
    "source": "harmonicai/kong-fork",
    "path": ".github/workflows/main-pr.yaml",
    "url": "https://github.com/harmonicai/kong-fork/blob/b43e8e3cd9683644b22afe50d7a84e277090d6a3/.github/workflows/main-pr.yaml",
    "retrieved_at": "2025-08-17T01:57:19.783047Z"
  },
  {
    "question": "Does this workflow post a comment on new pull requests if their descriptions are empty or consist only of specific characters and formatting?",
    "answer": "name: Lint new PR\n\non:\n    pull_request:\n        types: [opened]\n\njobs:\n    check-description:\n        name: Check that PR has description\n        runs-on: ubuntu-20.04\n\n        steps:\n            - name: Check if PR is shame-worthy\n              id: is-shame-worthy\n              run: |\n                  FILTERED_BODY=$( \\\n                      sed -r -e \\\n                      '/^(\\.\\.\\.)|(\\*)|(#+ )|(- )|(<!--)|(👉)/d' \\\n                      <<< $RAW_BODY \\\n                  )\n                  echo \"::debug::Filtered PR body to $FILTERED_BODY\"\n                  if [[ -z \"${FILTERED_BODY//[[:space:]]/}\" ]]; then\n                      echo \"is-shame-worthy=true\" >> $GITHUB_OUTPUT\n                  else\n                      echo \"is-shame-worthy=false\" >> $GITHUB_OUTPUT\n                  fi\n              env:\n                  RAW_BODY: ${{ github.event.pull_request.body }}\n\n            - name: Shame if PR has no description\n              if: steps.is-shame-worthy.outputs.is-shame-worthy == 'true'\n              run: |\n                  SHAME_BODY=\"Hey @${{ github.actor }}! 👋\\nThis pull request seems to contain no description. Please add useful context, rationale, and/or any other information that will help make sense of this change now and in the distant Mars-based future.\"\n                  curl -s -u posthog-bot:${{ secrets.POSTHOG_BOT_GITHUB_TOKEN || secrets.GITHUB_TOKEN }} -X POST -d \"{ \\\"body\\\": \\\"$SHAME_BODY\\\" }\" \"https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments\"\n",
    "source": "gooditworks/posthog",
    "path": ".github/workflows/lint-new-pr.yml",
    "url": "https://github.com/gooditworks/posthog/blob/3d6709ee73ddfb3f0d0c23bdde2da1ced904285b/.github/workflows/lint-new-pr.yml",
    "retrieved_at": "2025-08-17T01:57:20.464698Z"
  },
  {
    "question": "What is the purpose of this workflow and how frequently does it run?",
    "answer": "name: 'Lock threads'\n\non:\n  schedule:\n    - cron: '0 9 * * *'\n\njobs:\n  lock:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: dessant/lock-threads@v2\n        with:\n          github-token: ${{ github.token }}\n          issue-lock-inactive-days: '90'\n          issue-lock-reason: ''\n          pr-lock-inactive-days: '90'\n          pr-lock-reason: ''\n",
    "source": "authok/node-oidc-provider",
    "path": ".github/workflows/lock.yml",
    "url": "https://github.com/authok/node-oidc-provider/blob/492c5b897f1eac8616e397148a12ee5fdd2ac1ae/.github/workflows/lock.yml",
    "retrieved_at": "2025-08-17T01:57:21.173846Z"
  },
  {
    "question": "What actions are performed when a pull request targets the `master` branch?",
    "answer": "---\nname: Check\n\non:\n  pull_request:\n    branches: [\"master\"]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    env:\n      HUGO_VERSION: 0.102.3\n    steps:\n      - name: Install Hugo CLI\n        run: |\n          wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_Linux-64bit.deb \\\n          && sudo dpkg -i ${{ runner.temp }}/hugo.deb\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          submodules: recursive\n      - name: Build with Hugo\n        env:\n          HUGO_ENVIRONMENT: production\n          HUGO_ENV: production\n        run: |\n          set -x\n          pwd\n          ls\n          hugo \\\n            --minify \\\n            --baseURL grotius.example.com \\\n            --theme hugo-theme-notrack/ \\\n            --themesDir ../../ \\\n            --printUnusedTemplates \\\n        working-directory: ./exampleSite\n",
    "source": "jssuzuki1/assignment_2b",
    "path": ".github/workflows/check.yaml",
    "url": "https://github.com/jssuzuki1/assignment_2b/blob/33d32bc2cd39cd2d6364441b25948827fbdf5ede/.github/workflows/check.yaml",
    "retrieved_at": "2025-08-17T01:57:21.694065Z"
  },
  {
    "question": "What specific actions are performed by the `make` commands referenced throughout the workflow?",
    "answer": "name: rAPId Deployment\n\non:\n  push:\n    branches: [main]\n\n  workflow_dispatch:\n\njobs:\n  setup:\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Log commit SHA\n        run: echo $GITHUB_SHA\n\n  security-check:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - run: pip install -r requirements.txt\n\n      - name: Run security checks\n        run: make security-check\n\n  api-deployment:\n    needs:\n      - setup\n      - security-check\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          cp ./.github/.github.env .env\n          echo AWS_ACCOUNT=${{ secrets.AWS_ACCOUNT }} >> .env\n          echo AWS_REGION=${{ secrets.AWS_REGION }} >> .env\n          echo AWS_DEFAULT_REGION=${{ secrets.AWS_REGION }} >> .env\n\n      - name: Build API Image\n        run: make api/create-image\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - name: Setup API environment\n        run: make api/setup\n\n      - name: API Static Analysis\n        run: make api/lint\n\n      - name: API Tests\n        run: make api/test\n\n      - name: API Tag and Upload\n        run: make api/tag-and-upload\n\n      - name: API Check Image Scan for Vulnerabilities\n        run: make api/scan-for-vulns-and-tag\n\n      - name: API Tag PROD Candidate\n        run: make api/tag-prod-candidate\n\n      - name: API Deploy Image to Prod\n        run: make api/app-live-in-prod\n\n      - name: API Allow for Application to Start\n        run: sleep 120\n\n      - name: API Wait for Running Application\n        id: await-running-app\n        run: make api/check-app-is-running\n\n      - name: API E2E Tests\n        id: e2e-tests\n        env:\n          E2E_DOMAIN_NAME: ${{ secrets.E2E_DOMAIN_NAME }}\n          E2E_RESOURCE_PREFIX: ${{ secrets.E2E_RESOURCE_PREFIX }}\n        run: |\n          # Export AWS credentials to env for e2e tests\n          eval \"$(aws configure export-credentials --format env)\"\n          make api/test-e2e\n\n      - name: API Tag Image as Failure\n        if: always() && steps.await-running-app.conclusion == 'failure' || steps.e2e-tests.conclusion == 'failure'\n        run: make api/tag-prod-failure\n\n  cleanup:\n    needs:\n      - setup\n      - security-check\n      - api-deployment\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Clean Docker Context\n        if: always()\n        run: make api/clean-docker\n",
    "source": "no10ds/rapid",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/no10ds/rapid/blob/c763da3d92b4371effa90f6df460c4d027b899c6/.github/workflows/main.yml",
    "retrieved_at": "2025-08-17T01:57:22.380959Z"
  },
  {
    "question": "Under what conditions does the `deploy` job execute, and what deployment tool is used?",
    "answer": "name: deploy\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\non:\n  push:\n    branches:\n      - 'main'\n  pull_request:\n    branches:\n      - 'main'\njobs:\n  setup:\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - name: ⬇️ Checkout repo\n        uses: actions/checkout@v4\n\n      - name: ⎔ Setup node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20\n\n      - name: ▶️ Run setup script\n        run: npm run setup\n\n      - name: ʦ TypeScript\n        run: npm run typecheck\n\n      - name: ⬣ ESLint\n        run: npm run lint\n\n  deploy:\n    name: 🚀 Deploy\n    runs-on: ubuntu-latest\n    # only deploy main branch on pushes\n    if: ${{ github.ref == 'refs/heads/main' && github.event_name == 'push' }}\n\n    steps:\n      - name: ⬇️ Checkout repo\n        uses: actions/checkout@v4\n\n      - name: 🎈 Setup Fly\n        uses: superfly/flyctl-actions/setup-flyctl@1.5\n\n      - name: 🚀 Deploy\n        run: flyctl deploy --remote-only\n        working-directory: ./epicshop\n        env:\n          FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}\n",
    "source": "undyingkevin/react-api",
    "path": ".github/workflows/validate.yml",
    "url": "https://github.com/undyingkevin/react-api/blob/494643901e26d669e951f337fe927b29d460b2df/.github/workflows/validate.yml",
    "retrieved_at": "2025-08-17T01:57:23.082976Z"
  },
  {
    "question": "What specific code formatting and linting checks are performed by the `invoke project.pre-commit` command?",
    "answer": "# Check the code against the formatter and linter\nname: Code format and lint check\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\npermissions:\n  contents: read\n\njobs:\n  check:\n    name: Check Code\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n      fail-fast: false\n\n    steps:\n    - name: Check out from Git\n      uses: actions/checkout@v3\n    - name: Get history and tags for SCM versioning to work\n      run: |\n        git fetch --prune --unshallow\n        git fetch --depth=1 origin +refs/tags/*:refs/tags/*\n    - name: Set up Python\n      uses: actions/setup-python@v3\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        python -m pip install \".[build,test,development,pandora]\"\n    - name: Check\n      run: |\n        invoke project.pre-commit\n",
    "source": "msft-mirror-aosp/platform.external.python.bumble",
    "path": ".github/workflows/code-check.yml",
    "url": "https://github.com/msft-mirror-aosp/platform.external.python.bumble/blob/0d9d4b75e37ccc486b25830a3ef8f917730a0cb2/.github/workflows/code-check.yml",
    "retrieved_at": "2025-08-17T01:57:23.732725Z"
  },
  {
    "question": "Does this workflow update and push the index on every push or pull request to the master branch?",
    "answer": "name: CI\n\non:\n  # Trigger the workflow on push or pull request,\n  # but only for the master branch\n  push:\n    branches:\n      - master\n  pull_request:\n    branches:\n      - master\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n      with:\n        ref: ${{ github.head_ref }}\n\n    - name: Run gen index\n      run: \"./index_gen.sh\"\n      shell: bash\n\n    - name: Add & Commit\n      uses: github-actions-x/commit@v2.4\n      with:\n        commit-message: 'Index updated'\n        name: '${{ secrets.GH_USER }}'\n        email: '${{ secrets.GH_EMAIL }}'\n        push-branch: ${{ github.head_ref }}\n        github-token: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Push\n      uses: ad-m/github-push-action@v0.5.0\n      with:\n        github_token: ${{ secrets.GH_TOKEN }}\n        branch: ${{ github.head_ref }}\n",
    "source": "Yara-Rules/rules",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/Yara-Rules/rules/blob/0f93570194a80d2f2032869055808b0ddcdfb360/.github/workflows/main.yml",
    "retrieved_at": "2025-08-18T01:58:01.982381Z"
  },
  {
    "question": "Under what conditions does the `target-determination` job run within the workflow?",
    "answer": "# This workflow is dedicated to host slow jobs that are run only periodically because\n# they are too slow to run in every commit.  The list of slow tests can be found in\n# https://github.com/pytorch/test-infra/blob/generated-stats/stats/slow-tests.json\nname: slow\n\non:\n  schedule:\n    - cron: 45 0,4,8,12,16,20 * * *\n    - cron: 29 8 * * *  # about 1:29am PDT, for mem leak check and rerun disabled tests\n  push:\n    tags:\n      - ciflow/slow/*\n    branches:\n      - release/*\n  workflow_dispatch:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref_name }}-${{ github.ref_type == 'branch' && github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}-${{ github.event.schedule }}\n  cancel-in-progress: true\n\npermissions: read-all\n\njobs:\n  target-determination:\n    name: before-test\n    uses: ./.github/workflows/target_determination.yml\n    permissions:\n      id-token: write\n      contents: read\n\n  linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-build:\n    name: linux-focal-cuda12.1-py3-gcc9-slow-gradcheck\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-cuda12.1-py3-gcc9-slow-gradcheck\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      cuda-arch-list: 8.6\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 4, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 2, num_shards: 4, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 3, num_shards: 4, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 4, num_shards: 4, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n        ]}\n\n  linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-test:\n    name: linux-focal-cuda12.1-py3-gcc9-slow-gradcheck\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-build\n      - target-determination\n    with:\n      build-environment: linux-focal-cuda12.1-py3-gcc9-slow-gradcheck\n      docker-image: ${{ needs.linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-build.outputs.test-matrix }}\n      timeout-minutes: 300\n\n  linux-focal-cuda12_1-py3_10-gcc9-sm86-build:\n    name: linux-focal-cuda12.1-py3.10-gcc9-sm86\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9-sm86\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      cuda-arch-list: 8.6\n      test-matrix: |\n        { include: [\n          { config: \"slow\", shard: 1, num_shards: 2, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n          { config: \"slow\", shard: 2, num_shards: 2, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n        ]}\n\n  linux-focal-cuda12_1-py3_10-gcc9-sm86-test:\n    name: linux-focal-cuda12.1-py3.10-gcc9-sm86\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-focal-cuda12_1-py3_10-gcc9-sm86-build\n      - target-determination\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9-sm86\n      docker-image: ${{ needs.linux-focal-cuda12_1-py3_10-gcc9-sm86-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-cuda12_1-py3_10-gcc9-sm86-build.outputs.test-matrix }}\n\n  linux-focal-py3_8-clang10-build:\n    name: linux-focal-py3.8-clang10\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-py3.8-clang10\n      docker-image-name: pytorch-linux-focal-py3.8-clang10\n      test-matrix: |\n        { include: [\n          { config: \"slow\", shard: 1, num_shards: 1, runner: \"linux.2xlarge\" },\n        ]}\n\n  linux-focal-py3_8-clang10-test:\n    name: linux-focal-py3.8-clang10\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-focal-py3_8-clang10-build\n      - target-determination\n    with:\n      build-environment: linux-focal-py3.8-clang10\n      docker-image: ${{ needs.linux-focal-py3_8-clang10-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-py3_8-clang10-build.outputs.test-matrix }}\n\n  linux-focal-rocm6_0-py3_8-build:\n    name: linux-focal-rocm6.0-py3.8\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-rocm6.0-py3.8\n      docker-image-name: pytorch-linux-focal-rocm-n-py3\n      test-matrix: |\n        { include: [\n          { config: \"slow\", shard: 1, num_shards: 1, runner: \"linux.rocm.gpu\" },\n        ]}\n\n  linux-focal-rocm6_0-py3_8-test:\n    permissions:\n      id-token: write\n      contents: read\n    name: linux-focal-rocm6.0-py3.8\n    uses: ./.github/workflows/_rocm-test.yml\n    needs:\n      - linux-focal-rocm6_0-py3_8-build\n      - target-determination\n    with:\n      build-environment: linux-focal-rocm6.0-py3.8\n      docker-image: ${{ needs.linux-focal-rocm6_0-py3_8-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-rocm6_0-py3_8-build.outputs.test-matrix }}\n\n  linux-jammy-py3_10-clang15-asan-build:\n    name: linux-jammy-py3.10-clang15-asan\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-jammy-py3.10-clang15-asan\n      docker-image-name: pytorch-linux-jammy-py3-clang15-asan\n      test-matrix: |\n        { include: [\n          { config: \"slow\", shard: 1, num_shards: 2, runner: \"linux.4xlarge\" },\n          { config: \"slow\", shard: 2, num_shards: 2, runner: \"linux.4xlarge\" },\n        ]}\n      sync-tag: asan-build\n\n  linux-jammy-py3_10-clang15-asan-test:\n    name: linux-jammy-py3.10-clang15-asan\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-jammy-py3_10-clang15-asan-build\n      - target-determination\n    with:\n      build-environment: linux-jammy-py3.10-clang15-asan\n      docker-image: ${{ needs.linux-jammy-py3_10-clang15-asan-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-jammy-py3_10-clang15-asan-build.outputs.test-matrix }}\n      sync-tag: asan-test\n",
    "source": "zal-orz/pytorch",
    "path": ".github/workflows/slow.yml",
    "url": "https://github.com/zal-orz/pytorch/blob/78b4793c965fe640e37d80530eb78f07d67492e8/.github/workflows/slow.yml",
    "retrieved_at": "2025-08-18T01:58:02.922897Z"
  },
  {
    "question": "Under what conditions will the workflow upload coverage data to Codecov?",
    "answer": "# This is a basic workflow to help you get started with Actions\n\nname: CI\n\n# Controls when the action will run.\non:\n  # Triggers the workflow on push or pull request events but only for the master branch\n  push:\n    branches: [master]\n  pull_request:\n    branches: [master]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n  build:\n    # The type of runner that the job will run on\n    runs-on: macos-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        channel: [stable, beta, dev]\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      - name: Flutter action\n        uses: subosito/flutter-action@v1\n        with:\n          channel: ${{ matrix.channel }}\n\n      - name: Run Tests\n        run: |\n          flutter pub get\n          flutter format --dry-run --set-exit-if-changed .\n          flutter analyze --no-pub\n          flutter test --no-pub --coverage\n      - name: Upload coverage to Codecov\n        if: ${{ matrix.channel == 'stable' }}\n        uses: codecov/codecov-action@v1\n        with:\n          file: coverage/lcov.info\n",
    "source": "danvick/flutter_chips_input",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/danvick/flutter_chips_input/blob/4953c0f6ffcb35308738909aeab5d503a062c9be/.github/workflows/main.yml",
    "retrieved_at": "2025-08-18T01:58:03.768919Z"
  },
  {
    "question": "Under what conditions does the \"CLA Assistant\" step execute, and what actions does it perform?",
    "answer": "name: \"CLA Assistant\"\non:\n  issue_comment:\n    types: [created]\n  pull_request_target:\n    types: [opened,closed,synchronize]\n\njobs:\n  CLAssistant:\n    runs-on: ubuntu-latest\n    steps:\n      - name: \"CLA Assistant\"\n        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'\n        # Beta Release\n        uses: cla-assistant/github-action@v2.1.3-beta\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          # the below token should have repo scope and must be manually added by you in the repository's secret\n          PERSONAL_ACCESS_TOKEN : ${{ secrets.PERSONAL_ACCESS_TOKEN }}\n        with:\n          path-to-signatures: 'cla.json'\n          path-to-document: 'https://github.com/0xPolygon/polygon-edge/blob/develop/CLA.md'\n          branch: 'cla-signatures'\n          allowlist: dependabot[bot],dependabot-preview[bot]\n",
    "source": "blockstars-tech/konsta-core",
    "path": ".github/workflows/cla.yml",
    "url": "https://github.com/blockstars-tech/konsta-core/blob/c96334781453708dbdbabf2b221b63af2cdb05a1/.github/workflows/cla.yml",
    "retrieved_at": "2025-08-18T01:58:04.642762Z"
  },
  {
    "question": "What tests are performed by this workflow to ensure SDL2 compatibility for the Sony Playstation Vita?",
    "answer": "name: Build (Sony Playstation Vita)\n\non: [push, pull_request]\n\ndefaults:\n  run:\n    shell: sh\n\njobs:\n  vita:\n    runs-on: ubuntu-latest\n    container: \n      image: vitasdk/vitasdk:latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Install build requirements\n      run: |\n        apk update \n        apk add cmake ninja pkgconf bash\n    - name: Configure CMake\n      run: |\n        cmake -S . -B build -G Ninja \\\n          -DCMAKE_TOOLCHAIN_FILE=${VITASDK}/share/vita.toolchain.cmake \\\n          -DSDL_WERROR=ON \\\n          -DSDL_TESTS=ON \\\n          -DSDL_INSTALL_TESTS=ON \\\n          -DCMAKE_BUILD_TYPE=Release \\\n          -DCMAKE_INSTALL_PREFIX=prefix\n    - name: Build\n      run: cmake --build build --verbose\n    - name: Install CMake\n      run: |\n        echo \"SDL2_DIR=$(pwd)/prefix\" >> $GITHUB_ENV\n        cmake --install build/\n        ( cd prefix; find ) | LC_ALL=C sort -u\n    - name: Verify CMake configuration files\n      run: |\n        cmake -S cmake/test -B cmake_config_build -G Ninja \\\n          -DCMAKE_TOOLCHAIN_FILE=${VITASDK}/share/vita.toolchain.cmake \\\n          -DTEST_SHARED=FALSE \\\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }} \\\n          -DCMAKE_BUILD_TYPE=Release\n        cmake --build cmake_config_build --verbose\n    - name: Verify sdl2-config\n      run: |\n        export CC=arm-vita-eabi-gcc\n        export PATH=${{ env.SDL2_DIR }}/bin:$PATH\n        cmake/test/test_sdlconfig.sh\n    - name: Verify sdl2.pc\n      run: |\n        export CC=arm-vita-eabi-gcc\n        export PKG_CONFIG_PATH=${{ env.SDL2_DIR }}/lib/pkgconfig\n        cmake/test/test_pkgconfig.sh\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/vita.yaml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/vita.yaml",
    "retrieved_at": "2025-08-18T01:58:05.488972Z"
  },
  {
    "question": "Under what conditions does this workflow update PyTorch labels in S3?",
    "answer": "name: Update PyTorch Labels in S3\n\non:\n  label:\n  workflow_dispatch:\n\nconcurrency:\n  group: 1\n  cancel-in-progress: true\n\njobs:\n  update-labels-in-S3:\n    runs-on: ubuntu-22.04\n    if: ${{ github.repository == 'pytorch/pytorch' }}\n    steps:\n      - name: Checkout PyTorch\n        uses: pytorch/pytorch/.github/actions/checkout-pytorch@master\n        with:\n          fetch-depth: 1\n          submodules: false\n      - name: Update PyTorch labels list in S3\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_OSSCI_METRICS_V2_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_OSSCI_METRICS_V2_SECRET_ACCESS_KEY }}\n        run: |\n          python3 -m pip install boto3==1.19.12\n          .github/scripts/export_pytorch_labels.py\n",
    "source": "UEFI-code/PyTorch_For_PoorGuys",
    "path": ".github/workflows/update_pytorch_labels.yml",
    "url": "https://github.com/UEFI-code/PyTorch_For_PoorGuys/blob/a66ed97b99aa81e94e5700e495e62fdaa97ecf00/.github/workflows/update_pytorch_labels.yml",
    "retrieved_at": "2025-08-18T01:58:06.318803Z"
  },
  {
    "question": "What branches trigger this workflow on push events, and what other event triggers it?",
    "answer": "name: Secret Scan\n\non:\n  push:\n    branches:\n      - main\n      - master\n  pull_request:\n\njobs:\n  trufflehog:\n    name: Trufflehog\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    steps:\n      - name: Setup jq\n        uses: dcarbone/install-jq-action@v2.1.0\n\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: TruffleHog OSS\n        uses: trufflesecurity/trufflehog@main\n        with:\n          extra_args: --debug --only-verified",
    "source": "teknologi-umum/tgif",
    "path": ".github/workflows/Secret-Scan.yml",
    "url": "https://github.com/teknologi-umum/tgif/blob/83762eef67af2401fc8cb29c303569c083c6fac4/.github/workflows/Secret-Scan.yml",
    "retrieved_at": "2025-08-18T01:58:07.162858Z"
  },
  {
    "question": "What triggers this workflow to build and publish documentation?",
    "answer": "name: Docs\n\non:\n  push:\n    branches: [main]\n    paths-ignore:\n      - \".github/**\" # Ignore changes towards the .github directory\n  workflow_dispatch: # run on request (no need for PR)\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\n# Allow one concurrent deployment\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: true\n\njobs:\n  Build-and-Publish-Documentation:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: [docs]\n    steps:\n      - name: CHECKOUT REPOSITORY\n        uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n\n      - name: Install requirements\n        run: |\n          pip install -r requirements/docs.txt\n          pip install \".[full]\"\n\n      - name: Link dataset path to local directory as nbsphinx runs the notebook\n        run: ln -s $ANOMALIB_DATASET_PATH ./datasets\n      - name: Build and Commit Docs\n        run: |\n          cd docs\n          make html\n          cd ..\n\n      - name: Clean directory\n        run: |\n          mkdir -p /tmp/docs_build\n          cp -r docs/build/html/* /tmp/docs_build\n          rm -rf ./*\n          cp -r /tmp/docs_build/* ./\n          rm -rf /tmp/docs_build\n          touch .nojekyll\n\n      - name: Setup Pages\n        uses: actions/configure-pages@v2\n\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v1\n        with:\n          # Upload entire repository\n          path: \".\"\n\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v1\n",
    "source": "versusic/anomalib",
    "path": ".github/workflows/docs.yml",
    "url": "https://github.com/versusic/anomalib/blob/9287d3bb07f8569f3c1cb2e9212da12508929468/.github/workflows/docs.yml",
    "retrieved_at": "2025-08-18T01:58:07.992674Z"
  },
  {
    "question": "What specific code quality checks are performed by this workflow beyond formatting and bad word detection?",
    "answer": "name: CPP project with GTest CI\n\non: [push]\n\njobs:\n  build:\n\n    runs-on: ubuntu-20.04\n\n    env:\n      CC: clang\n      CXX: clang++\n\n    steps:\n    - name: Setup dependencies\n      run: |\n        sudo apt-get install -y clang-tidy-10\n        sudo update-alternatives --install /usr/bin/clang-tidy clang-tidy /usr/bin/clang-tidy-10 100\n    - name: Checkout submodules\n      uses: actions/checkout@v1\n      with:\n        submodules: recursive\n    - name: Check for bad words\n      run: \"! grep -R -n -w -f .bad_words src include\"\n    - name: Run clang format\n      uses: DoozyX/clang-format-lint-action@v0.11\n      with:\n        source: '.'\n        exclude: './googletest ./test'\n        extensions: 'h,cpp'\n        clangFormatVersion: 11\n    - name: Setup ICA\n      uses: actions/checkout@v2\n      with:\n        repository: 'Kurkin/ica-lint-action'\n        path: ica\n    - name: Prepare build dir\n      run: mkdir build\n    - name: Generate build files using cmake\n      run: cmake .. -DUSE_CLANG_TIDY=TRUE -DPATH_TO_ICA=\"${{ github.workspace }}/ica/ica/libica-plugin.so\"\n      working-directory: ./build\n    - name: Run make\n      run: make\n      working-directory: ./build\n    - name: Run tests\n      timeout-minutes: 3\n      run: ./test/runUnitTests\n      working-directory: ./build\n    - name: Prepare ASAN build dir\n      run: mkdir build_asan\n    - name: Generate ASAN build files using cmake\n      run: cmake .. -DCMAKE_BUILD_TYPE=ASAN\n      working-directory: ./build_asan\n    - name: Run ASAN make\n      run: make\n      working-directory: ./build_asan\n    - name: Run ASAN tests\n      timeout-minutes: 5\n      run: ./test/runUnitTests\n      working-directory: ./build_asan\n    - name: Prepare USAN build dir\n      run: mkdir build_usan\n    - name: Generate USAN build files using cmake\n      run: cmake .. -DCMAKE_BUILD_TYPE=USAN\n      working-directory: ./build_usan\n    - name: Run USAN make\n      run: make\n      working-directory: ./build_usan\n    - name: Run USAN tests\n      timeout-minutes: 3\n      run: ./test/runUnitTests\n      working-directory: ./build_usan\n",
    "source": "AgafonovVadim/ouch-executed-order",
    "path": ".github/workflows/cpp_project.yml",
    "url": "https://github.com/AgafonovVadim/ouch-executed-order/blob/0a9c529e3a19f8c77c5cc1b856130a02959c3b91/.github/workflows/cpp_project.yml",
    "retrieved_at": "2025-08-18T01:58:08.835981Z"
  },
  {
    "question": "Under what conditions will the \"Build, test, and publish unstable release\" job be skipped?",
    "answer": "name: push-master\n\non:\n  push:\n    branches:\n      - master\n\njobs:\n  build_test_publish:\n    name: Build, test, and publish unstable release\n    if: \"! contains(github.event.head_commit.message, 'chore(release): publish')\"\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [11.x]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Use Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n          registry-url: \"https://registry.npmjs.org\"\n      - run: yarn install --frozen-lockfile\n      - run: yarn build:release\n      - run: yarn test\n      - run: git config user.name \"Mattr CI\"\n      - run: git config user.email \"npmjs_ci_mattr_public@mattr.global\"\n      - run: npm whoami\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPMJS_PUBLIC_TOKEN }}\n      - run: yarn publish:unstable\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPMJS_PUBLIC_TOKEN }}\n      - name: Report Coverage\n        uses: codecov/codecov-action@v1\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n",
    "source": "rodrigodg1/broker-bbs",
    "path": ".github/workflows/push-master.yaml",
    "url": "https://github.com/rodrigodg1/broker-bbs/blob/58e27be9391bc0c6e585cd6824e150f8f897fae4/.github/workflows/push-master.yaml",
    "retrieved_at": "2025-08-18T01:58:09.736728Z"
  },
  {
    "question": "Under what conditions does the `docker_ephemeral_env` job, which pushes a Docker image to ECR, execute?",
    "answer": "name: Push ephemeral env image\n\non:\n  workflow_run:\n    workflows: [\"Docker\"]\n    types:\n      - completed\n\njobs:\n  config:\n    runs-on: \"ubuntu-latest\"\n    if: github.event.workflow_run.event == 'pull_request' && github.event.workflow_run.conclusion == 'success'\n    outputs:\n      has-secrets: ${{ steps.check.outputs.has-secrets }}\n    steps:\n      - name: \"Check for secrets\"\n        id: check\n        shell: bash\n        run: |\n          if [ -n \"${{ (secrets.AWS_ACCESS_KEY_ID != '' &&\n            secrets.AWS_SECRET_ACCESS_KEY != '') || '' }}\" ]; then\n            echo \"has-secrets=1\" >> \"$GITHUB_OUTPUT\"\n            echo \"has secrets!\"\n          else\n            echo \"has-secrets=0\" >> \"$GITHUB_OUTPUT\"\n            echo \"no secrets!\"\n          fi\n\n  docker_ephemeral_env:\n    needs: config\n    if: needs.config.outputs.has-secrets\n    name: Push ephemeral env Docker image to ECR\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: \"Download artifact\"\n        uses: actions/github-script@v3.1.0\n        with:\n          script: |\n            const artifacts = await github.actions.listWorkflowRunArtifacts({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                run_id: ${{ github.event.workflow_run.id }},\n            });\n\n            core.info('*** artifacts')\n            core.info(JSON.stringify(artifacts))\n\n            const matchArtifact = artifacts.data.artifacts.filter((artifact) => {\n              return artifact.name == \"build\"\n            })[0];\n            if(!matchArtifact) return core.setFailed(\"Build artifacts not found\")\n\n            const download = await github.actions.downloadArtifact({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                artifact_id: matchArtifact.id,\n                archive_format: 'zip',\n            });\n            var fs = require('fs');\n            fs.writeFileSync('${{github.workspace}}/build.zip', Buffer.from(download.data));\n\n      - run: unzip build.zip\n\n      - name: Display downloaded files (debug)\n        run: ls -la\n\n      - name: Get SHA\n        id: get-sha\n        run: echo \"::set-output name=sha::$(cat ./SHA)\"\n\n      - name: Get PR\n        id: get-pr\n        run: echo \"::set-output name=num::$(cat ./PR-NUM)\"\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v1\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-west-2\n\n      - name: Login to Amazon ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v1\n\n      - name: Load, tag and push image to ECR\n        id: push-image\n        env:\n          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}\n          ECR_REPOSITORY: superset-ci\n          SHA: ${{ steps.get-sha.outputs.sha }}\n          IMAGE_TAG: pr-${{ steps.get-pr.outputs.num }}\n        run: |\n          docker load < $SHA.tar.gz\n          docker tag $SHA $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\n          docker tag $SHA $ECR_REGISTRY/$ECR_REPOSITORY:$SHA\n          docker push -a $ECR_REGISTRY/$ECR_REPOSITORY\n",
    "source": "IsraelBoka/superset-engeem",
    "path": ".github/workflows/docker-ephemeral-env.yml",
    "url": "https://github.com/IsraelBoka/superset-engeem/blob/891dee627d506c89d7274c286489ad646ca4ba03/.github/workflows/docker-ephemeral-env.yml",
    "retrieved_at": "2025-08-19T01:45:31.497564Z"
  },
  {
    "question": "What specific changes to files under `csrc/` or `demo/csrc/`, or to `CMakeLists.txt`, will trigger this workflow?",
    "answer": "name: backend-coreml\n\non:\n  push:\n    paths:\n      - \"csrc/**\"\n      - \"demo/csrc/**\"\n      - \"CMakeLists.txt\"\n\n  pull_request:\n    paths:\n      - \"csrc/**\"\n      - \"demo/csrc/**\"\n      - \"CMakeLists.txt\"\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\nenv:\n  DEVELOPER_DIR: /Applications/Xcode_13.4.1.app/Contents/Developer\npermissions:\n  contents: read\n\njobs:\n  build_macos_arm64:\n    runs-on: macos-12\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v3\n        with:\n          submodules: 'recursive'\n      - name: install opencv\n        run: |\n          wget https://github.com/irexyc/mmdeploy-ci-resource/releases/download/opencv/opencv-osx-arm64-4.6.0.tar.gz\n          mkdir $GITHUB_WORKSPACE/opencv-install\n          tar xf opencv-osx-arm64-4.6.0.tar.gz -C $GITHUB_WORKSPACE/opencv-install\n      - name: install libtorch\n        run: |\n          wget https://github.com/irexyc/mmdeploy-ci-resource/releases/download/libtorch/libtorch-osx-arm64-1.8.0.tar.gz\n          mkdir $GITHUB_WORKSPACE/libtorch-install\n          tar xf libtorch-osx-arm64-1.8.0.tar.gz -C $GITHUB_WORKSPACE/libtorch-install\n      - name: build\n        run: |\n          mkdir build && cd build\n          cmake .. -DCMAKE_OSX_ARCHITECTURES=\"arm64\" \\\n            -DCMAKE_SYSTEM_PROCESSOR=\"arm64\" \\\n            -DMMDEPLOY_BUILD_SDK=ON \\\n            -DMMDEPLOY_TARGET_DEVICES=\"cpu\" \\\n            -DMMDEPLOY_CODEBASES=all \\\n            -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4 \\\n            -DTorch_DIR=$GITHUB_WORKSPACE/libtorch-install/share/cmake/Torch \\\n            -DMMDEPLOY_TARGET_BACKENDS=\"coreml\" \\\n            -DMMDEPLOY_BUILD_EXAMPLES=ON \\\n            -DMMDEPLOY_SHARED_LIBS=OFF\n          cmake --build . -j 3\n          cmake --build . --target install\n      - name: build-shared\n        run: |\n          mkdir build-shared && cd build-shared\n          cmake .. -DCMAKE_OSX_ARCHITECTURES=\"arm64\" \\\n            -DCMAKE_SYSTEM_PROCESSOR=\"arm64\" \\\n            -DMMDEPLOY_BUILD_SDK=ON \\\n            -DMMDEPLOY_TARGET_DEVICES=\"cpu\" \\\n            -DMMDEPLOY_CODEBASES=all \\\n            -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4 \\\n            -DTorch_DIR=$GITHUB_WORKSPACE/libtorch-install/share/cmake/Torch \\\n            -DMMDEPLOY_TARGET_BACKENDS=\"coreml\" \\\n            -DMMDEPLOY_BUILD_EXAMPLES=ON \\\n            -DMMDEPLOY_SHARED_LIBS=ON\n          cmake --build . -j 3\n          cmake --build . --target install\n",
    "source": "HuangJunJie2017/mmdeploy",
    "path": ".github/workflows/backend-coreml.yml",
    "url": "https://github.com/HuangJunJie2017/mmdeploy/blob/f70f3af35474929c1cc402c9d058cc40457e5188/.github/workflows/backend-coreml.yml",
    "retrieved_at": "2025-08-19T01:45:32.278070Z"
  },
  {
    "question": "For which Windows platforms does this workflow build and test SDL using CMake and MSBuild?",
    "answer": "name: Build (MSVC)\n\non: [push, pull_request]\n\njobs:\n  Build:\n    name: ${{ matrix.platform.name }}\n    runs-on: windows-latest\n\n    strategy:\n      fail-fast: false\n      matrix:\n        platform:\n        - { name: Windows (x64),          flags: -A x64,   project: VisualC/SDL.sln, projectflags: '/p:Platform=x64' }\n        - { name: Windows (x86),          flags: -A Win32, project: VisualC/SDL.sln, projectflags: '/p:Platform=Win32' }\n        - { name: Windows static VCRT (x64), flags: -A x64 -DSDL_FORCE_STATIC_VCRT=ON }\n        - { name: Windows static VCRT (x86), flags: -A Win32 -DSDL_FORCE_STATIC_VCRT=ON }\n        - { name: Windows (clang-cl x64), flags: -T ClangCL -A x64 }\n        - { name: Windows (clang-cl x86), flags: -T ClangCL -A Win32 }\n        - { name: Windows (ARM),          flags: -A ARM }\n        - { name: Windows (ARM64),        flags: -A ARM64 }\n        - { name: UWP (x64),              flags: -A x64 -DCMAKE_SYSTEM_NAME=WindowsStore -DCMAKE_SYSTEM_VERSION=\"10.0\" -DSDL_TESTS=OFF, nowerror: true,\n            project: VisualC-WinRT/SDL-UWP.sln, projectflags: '/p:Platform=x64 /p:WindowsTargetPlatformVersion=10.0.17763.0' }\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Create CMake project using SDL as a subproject\n      shell: python\n      run: |\n        import os\n        import textwrap\n        srcdir = r\"${{ github.workspace }}\".replace(\"\\\\\", \"/\")\n        builddir = f\"{ srcdir }/build\"\n        os.makedirs(builddir)\n        with open(f\"{ builddir }/CMakeLists.txt\", \"w\") as f:\n          f.write(textwrap.dedent(f\"\"\"\\\n            cmake_minimum_required(VERSION 3.0)\n            project(sdl_user)\n            add_subdirectory(\"{ srcdir }\" SDL)\n          \"\"\"))\n    - name: Configure (CMake)\n      run: cmake -S build -B build `\n        -DSDL_WERROR=${{ !matrix.platform.nowerror }} `\n        -DSDL_TESTS=ON `\n        -DSDL_INSTALL_TESTS=ON `\n        -DSDL_VENDOR_INFO=\"Github Workflow\" `\n        -DSDL2_DISABLE_INSTALL=OFF `\n        ${{ matrix.platform.flags }} `\n        -DCMAKE_INSTALL_PREFIX=prefix\n    - name: Build (CMake)\n      run: cmake --build build/ --config Release --parallel\n    - name: Run build-time tests\n      if: \"! contains(matrix.platform.name, 'ARM')\"\n      run: |\n        $env:SDL_TESTS_QUICK=1\n        ctest -VV --test-dir build/ -C Release\n    - name: Install (CMake)\n      run: |\n        echo \"SDL2_DIR=$Env:GITHUB_WORKSPACE/prefix\" >> $Env:GITHUB_ENV\n        cmake --install build/\n    - name: Verify CMake configuration files\n      if: ${{ !contains(matrix.platform.name, 'UWP') }}  # FIXME: cmake/test/CMakeLists.txt should support UWP\n      run: |\n        cmake -S cmake/test -B cmake_config_build `\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }} `\n          ${{ matrix.platform.flags }}\n        cmake --build cmake_config_build --config Release\n\n    - name: Add msbuild to PATH\n      if: ${{ matrix.platform.project != '' }}\n      uses: microsoft/setup-msbuild@v1.1.3\n    - name: Build msbuild\n      if: ${{ matrix.platform.project != '' }}\n      run: msbuild ${{ matrix.platform.project }} /m /p:BuildInParallel=true /p:Configuration=Release ${{ matrix.platform.projectflags }}\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/msvc.yml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/msvc.yml",
    "retrieved_at": "2025-08-19T01:45:33.281326Z"
  },
  {
    "question": "What triggers this workflow to upload the Python package to PyPI?",
    "answer": "name: Upload Python Package\non:\n  release:\n    types: [published]\n  workflow_dispatch: # run on request (no need for PR)\n\njobs:\n  deploy:\n    runs-on: ubuntu-20.04\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel twine\n      - name: Build and publish\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_PASSWORD }}\n        run: |\n          python setup.py sdist bdist_wheel\n          twine upload dist/*\n",
    "source": "versusic/anomalib",
    "path": ".github/workflows/publish.yml",
    "url": "https://github.com/versusic/anomalib/blob/9287d3bb07f8569f3c1cb2e9212da12508929468/.github/workflows/publish.yml",
    "retrieved_at": "2025-08-19T01:45:34.144427Z"
  },
  {
    "question": "What specific events trigger the execution of this workflow?",
    "answer": "name: any-pr\n\non: [pull_request]\n\njobs:\n  build_test:\n    name: Build test\n    runs-on: ${{matrix.os}}\n    strategy:\n      matrix:\n        node-version: [11.x]\n        os: [ubuntu-latest]\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: yarn install --frozen-lockfile\n      - run: yarn build:release\n      - run: yarn test\n  build_test_wasm_env_interop:\n    name: Build and Test WASM running cross OS and NodeJS versions\n    runs-on: ${{matrix.os}}\n    strategy:\n      matrix:\n        node-version: [11.x, 12.x, 13.x, 14.x, 15.x, 16.x]\n        os: [ubuntu-latest, macos-latest]\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: yarn install --frozen-lockfile\n      - run: yarn build:release\n      - run: yarn test:wasm\n  build_test_native_node_env_interop:\n    name: Build and Test native module running cross OS and NodeJS versions\n    runs-on: ${{matrix.os}}\n    strategy:\n      matrix:\n        node-version: [11.x, 12.x, 13.x, 14.x, 15.x, 16.x]\n        os: [ubuntu-latest, macos-latest]\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: yarn install --frozen-lockfile\n      - run: yarn build:release\n      - run: yarn test:node\n",
    "source": "rodrigodg1/broker-bbs",
    "path": ".github/workflows/any-pr.yaml",
    "url": "https://github.com/rodrigodg1/broker-bbs/blob/58e27be9391bc0c6e585cd6824e150f8f897fae4/.github/workflows/any-pr.yaml",
    "retrieved_at": "2025-08-19T01:45:34.997701Z"
  },
  {
    "question": "What triggers this workflow to run?",
    "answer": "name: Upload Website\non:\n  push:\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [12.x]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Use Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: npm install\n      - run: npm run build --if-present\n      - run: npm test\n        env:\n          CI: true\n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@master\n      - uses: jakejarvis/s3-sync-action@master\n        with:\n          args: --acl public-read --follow-symlinks --delete\n        env:\n          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          AWS_REGION: \"us-east-1\" # optional: defaults to us-east-1\n",
    "source": "tjmcode/MITxPRO-GitHubActions",
    "path": ".github/workflows/hello_actions.yml",
    "url": "https://github.com/tjmcode/MITxPRO-GitHubActions/blob/ac052dedade2149c7056d83ca113494f496e2f72/.github/workflows/hello_actions.yml",
    "retrieved_at": "2025-08-19T01:45:35.768459Z"
  },
  {
    "question": "What actions are performed on a pull request to this repository by this workflow?",
    "answer": "name: Checks\n\non:\n    - pull_request\n\njobs:\n    Build:\n        env:\n            BALANCER_SUBGRAPH: https://api.thegraph.com/subgraphs/name/beethovenxfi/beethovenx\n            MASTERCHEF_SUBGRAPH: https://api.thegraph.com/subgraphs/name/beethovenxfi/masterchefv2\n            BLOCKS_SUBGRAPH: https://api.thegraph.com/subgraphs/name/danielmkm/optimism-blocks\n            BEETS_BAR_SUBGRAPH: https://api.thegraph.com/subgraphs/name/beethovenxfi/beets-bar\n            USER_SNAPSHOT_SUBGRAPH: https://api.thegraph.com/subgraphs/name/danielmkm/user-balances-fantom\n            GAUGE_SUBGRAPH: https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-gauges-optimism\n            VEBALLOCKS_SUBGRAPH: https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-gauges\n            RELIQUARY_SUBGRAPH: https://api.thegraph.com/subgraphs/name/beethovenxfi/reliquary\n            SFTMX_SUBGRAPH: 'https://api.thegraph.com/subgraphs/name/beethovenxfi/sftmx'\n        runs-on: ubuntu-latest\n        steps:\n            - uses: actions/checkout@v3\n            - name: Use Node.js\n              uses: actions/setup-node@v3\n              with:\n                  node-version: '18.x'\n            - name: Install deps\n              run: yarn\n            - name: Generate Schema\n              run: yarn generate\n            - name: Prisma Generate\n              run: yarn prisma generate\n            - name: Run build\n              run: yarn build\n",
    "source": "beethovenxfi/beethovenx-backend",
    "path": ".github/workflows/checks.yml",
    "url": "https://github.com/beethovenxfi/beethovenx-backend/blob/4ef0c321ede84bca93c752a7dd1732bbb5addb14/.github/workflows/checks.yml",
    "retrieved_at": "2025-08-19T01:45:36.643664Z"
  },
  {
    "question": "Under what conditions will the workflow build and push the `cometbft/e2e-node` Docker image to Docker Hub?",
    "answer": "name: Docker E2E Node\n# Build & Push rebuilds the e2e Testapp docker image on every push to main and creation of tags\n# and pushes the image to https://hub.docker.com/r/cometbft/e2e-node\non:\n  push:\n    branches:\n      - v0.37.x\n    tags:\n      - \"v[0-9]+.[0-9]+.[0-9]+\"               # Push events to matching v*, i.e. v1.0, v20.15.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-alpha.[0-9]+\"  # e.g. v0.37.0-alpha.1, v0.38.0-alpha.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-beta.[0-9]+\"   # e.g. v0.37.0-beta.1, v0.38.0-beta.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-rc[0-9]+\"      # e.g. v0.37.0-rc1, v0.38.0-rc10\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Prepare\n        id: prep\n        run: |\n          DOCKER_IMAGE=cometbft/e2e-node\n          VERSION=noop\n          if [[ $GITHUB_REF == refs/tags/* ]]; then\n            VERSION=${GITHUB_REF#refs/tags/}\n          elif [[ $GITHUB_REF == refs/heads/* ]]; then\n            VERSION=$(echo ${GITHUB_REF#refs/heads/} | sed -r 's#/+#-#g')\n            if [ \"${{ github.event.repository.default_branch }}\" = \"$VERSION\" ]; then\n              VERSION=latest\n            fi\n          fi\n          TAGS=\"${DOCKER_IMAGE}:${VERSION}\"\n          if [[ $VERSION =~ ^v[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n            TAGS=\"$TAGS,${DOCKER_IMAGE}:${VERSION}\"\n          fi\n          echo \"tags=${TAGS}\" >> $GITHUB_OUTPUT\n\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@master\n        with:\n          platforms: all\n\n      - name: Set up Docker Build\n        uses: docker/setup-buildx-action@v2.7.0\n\n      - name: Login to DockerHub\n        if: ${{ github.event_name != 'pull_request' }}\n        uses: docker/login-action@v2.2.0\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Publish to Docker Hub\n        uses: docker/build-push-action@v4.1.1\n        with:\n          context: .\n          file: ./test/e2e/docker/Dockerfile\n          platforms: linux/amd64,linux/arm64\n          push: ${{ github.event_name != 'beep_boop' }}\n          tags: ${{ steps.prep.outputs.tags }}\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/testapp-docker.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/testapp-docker.yml",
    "retrieved_at": "2025-08-19T01:45:37.357877Z"
  },
  {
    "question": "Under what conditions will the workflow build and push the Docker image tagged with `latest`?",
    "answer": "name: Build & Publish VWA Docker image\non:\n  push:\n    branches:\n      - master\n      - v*-branch\n    paths:\n      - components/crud-web-apps/volumes/**\n      - components/crud-web-apps/common/**\n      - releasing/version/VERSION\n\nenv:\n  DOCKER_USER: kubeflownotebookswg\n  IMG: kubeflownotebookswg/volumes-web-app\n  ARCH: linux/ppc64le,linux/amd64\n\njobs:\n  push_to_registry:\n    name: Build & Push Docker image to Docker Hub\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n\n    - uses: dorny/paths-filter@v2\n      id: filter\n      with:\n        filters: |\n          version:\n            - 'releasing/version/VERSION'\n\n    - name: Login to DockerHub\n      uses: docker/login-action@v2\n      with:\n        username: ${{ env.DOCKER_USER }}\n        password: ${{ secrets.KUBEFLOWNOTEBOOKSWG_DOCKER_TOKEN }}\n\n    - name: Setup QEMU\n      uses: docker/setup-qemu-action@v2\n\n    - name: Setup Docker Buildx\n      uses: docker/setup-buildx-action@v2\n\n    - name: Build and push multi-arch docker image\n      run: |\n        cd components/crud-web-apps/volumes\n        make docker-build-push-multi-arch\n\n    - name: Build and push latest multi-arch docker image\n      if: github.ref == 'refs/heads/master'\n      run: |\n        export TAG=latest\n        cd components/crud-web-apps/volumes\n        make docker-build-push-multi-arch\n\n    - name: Build and push multi-arch docker image on Version change\n      id: version\n      if: steps.filter.outputs.version == 'true'\n      run: |\n        export TAG=$(cat releasing/version/VERSION)\n        cd components/crud-web-apps/volumes\n        make docker-build-push-multi-arch\n",
    "source": "Zveroloff/kubeflow",
    "path": ".github/workflows/vwa_docker_publish.yaml",
    "url": "https://github.com/Zveroloff/kubeflow/blob/1552c90ccaee7acdea3495175592fcca46ae3a77/.github/workflows/vwa_docker_publish.yaml",
    "retrieved_at": "2025-08-19T01:45:38.110821Z"
  },
  {
    "question": "Under what conditions will the `Run reviewdog` step execute within this workflow?",
    "answer": "# textlintでの文法チェックのワークフロー\nname: Lint\n\n# Reviewdogでの指摘を有効に使うため、pull requestでのみ有効\non:\n  pull_request:\n    # Pull Requestかつ\n    # 以下のファイルに変更があった場合、このWorkflowを実行\n    paths:\n      - \"prh.yml\"\n      - \".textlintrc\"\n      - \"package.json\"\n      - \"package-lock.json\"\n      - \".github/workflows/lint.yaml\"\n      - \"**.tex\"\n\njobs:\n  lint:\n    permissions:\n      checks: write\n      contents: read\n      pull-requests: write\n    runs-on: ubuntu-20.04\n    timeout-minutes: 10\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Setup node environment\n        uses: actions/setup-node@v3\n        with:\n          cache: npm\n          node-version: \"18.x\"\n\n      - uses: reviewdog/action-setup@v1\n        with:\n          reviewdog_version: latest\n\n      # 依存関係の構築とtextlintの実行\n      - name: Run lint\n        run: |\n          npm install\n          npm run lint\n\n      # ↑のステップでtextlintが違反を検知した場合のみ実行\n      - name: Run reviewdog\n        if: failure()\n        env:\n          REVIEWDOG_GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          npm run --silent lint-style | reviewdog -f=checkstyle -name=\"textlint\" -diff=\"Git diff HEAD^\" -reporter=github-pr-review\n",
    "source": "pddg/latex-template-ja",
    "path": ".github/workflows/lint.yaml",
    "url": "https://github.com/pddg/latex-template-ja/blob/b70233e3e83958d22d7e42ad121d9eafa77c3721/.github/workflows/lint.yaml",
    "retrieved_at": "2025-08-19T01:45:39.022048Z"
  },
  {
    "question": "What triggers this workflow, and how often does it run automatically?",
    "answer": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nname: Publish nightly artifacts\n\non:\n  schedule:\n    - cron: \"0 0 * * *\"\n  workflow_dispatch:\n\npermissions:\n  contents: read\n\njobs:\n  publish-nightly:\n    name: Publish nightly\n    runs-on: ubuntu-latest\n    if: github.repository == 'apache/pekko-persistence-dynamodb'\n    steps:\n      - name: Checkout\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n        with:\n          fetch-depth: 0\n          fetch-tags: true\n\n      - name: Setup Java 8\n        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00 # v4.7.1\n        with:\n          distribution: temurin\n          java-version: 8\n\n      - name: Install sbt\n        uses: sbt/setup-sbt@6c68d2fe8dfbc0a0534d70101baa2e0420e1a506 # v1.1.9\n\n      - name: Publish to Apache Maven repo\n        env:\n          NEXUS_USER: ${{ secrets.NEXUS_USER }}\n          NEXUS_PW: ${{ secrets.NEXUS_PW }}\n        run: sbt +publish\n\n      - name: Cache Coursier cache\n        uses: coursier/cache-action@4e2615869d13561d626ed48655e1a39e5b192b3c # v6.4.7\n",
    "source": "apache/pekko-persistence-dynamodb",
    "path": ".github/workflows/publish-nightly.yml",
    "url": "https://github.com/apache/pekko-persistence-dynamodb/blob/f88b6b9b20b4d5cec7285a5f049da220998f2dcb/.github/workflows/publish-nightly.yml",
    "retrieved_at": "2025-08-20T01:44:10.404579Z"
  },
  {
    "question": "Under what conditions will the workflow execute a dry run of the npm publish?",
    "answer": "name: Publish npm package\n\non:\n  workflow_dispatch:\n    inputs:\n      dry-run:\n        description: 'Dry run'\n        required: true\n        type: boolean\n        default: true\n  schedule:\n    - cron: '48 3 * * 1' # 3:48 AM UTC every Monday\n\njobs:\n  preflight:\n    name: Preflight\n    runs-on: ubuntu-latest\n    outputs:\n      dry-run: ${{ steps.get-dry-run.outputs.dry-run }}\n\n    steps:\n      - name: Get dry run\n        id: get-dry-run\n        shell: pwsh\n        run: |\n          $IsDryRun = '${{ github.event.inputs.dry-run }}' -Eq 'true' -Or '${{ github.event_name }}' -Eq 'schedule'\n\n          if ($IsDryRun) {\n            echo \"dry-run=true\" >> $Env:GITHUB_OUTPUT\n          } else {\n            echo \"dry-run=false\" >> $Env:GITHUB_OUTPUT\n          }\n\n  build:\n    name: Build package\n    runs-on: ubuntu-latest\n    needs:\n      - preflight\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup wasm-pack\n        shell: bash\n        run: |\n          curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh      \n\n      - name: Install dependencies\n        shell: pwsh\n        run: |\n          Set-Location -Path ./web-client/iron-remote-gui/\n          npm install\n\n      - name: Build package\n        shell: pwsh\n        run: |\n          Set-PSDebug -Trace 1\n\n          Set-Location -Path ./web-client/iron-remote-gui/\n          npm run build\n          Set-Location -Path ./dist\n          npm pack\n\n      - name: Harvest package\n        shell: pwsh\n        run: |\n          Set-PSDebug -Trace 1\n\n          New-Item -ItemType \"directory\" -Path . -Name \"npm-packages\"\n          Get-ChildItem -Path ./web-client/ -Recurse *.tgz | ForEach { Copy-Item $_ \"./npm-packages\" }\n\n      - name: Upload package artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: npm\n          path: npm-packages/*.tgz\n\n  publish:\n    name: Publish package\n    runs-on: ubuntu-latest\n    if: github.event_name == 'workflow_dispatch'\n    environment: publish\n    needs:\n      - preflight\n      - build\n\n    steps:\n      - name: Download NPM packages artifact\n        uses: actions/download-artifact@v4\n        with:\n          name: npm\n          path: npm-packages\n\n      - name: Prepare npm\n        shell: pwsh\n        run: npm config set \"//registry.npmjs.org/:_authToken=${{ secrets.NPM_TOKEN }}\"\n\n      - name: Publish\n        shell: pwsh\n        run: |\n          Set-PSDebug -Trace 1\n\n          $isDryRun = '${{ needs.preflight.outputs.dry-run }}' -Eq 'true'\n\n          $files = Get-ChildItem -Recurse npm-packages/*.tgz\n\n          foreach ($file in $files) {\n            Write-Host \"Publishing $($File.Name)...\"\n\n            $publishCmd = @(\n              'npm', \n              'publish', \n              \"$File\",\n              '--access=public'\n            )\n\n            if ($isDryRun) {\n              $publishCmd += '--dry-run'\n            }\n\n            $publishCmd = $publishCmd -Join ' '\n            Invoke-Expression $publishCmd\n          }\n\n  notify:\n    name: Notify failure\n    runs-on: ubuntu-latest\n    if: ${{ always() && contains(needs.*.result, 'failure') && github.event_name == 'schedule' }}\n    needs:\n      - preflight\n      - build\n    env:\n      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_ARCHITECTURE }}\n      SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n    steps:\n      - name: Send slack notification\n        id: slack\n        uses: slackapi/slack-github-action@v1.26.0\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \"*${{ github.repository }}* :fire::fire::fire::fire::fire: \\n The scheduled build for *${{ github.repository }}* is <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|broken>\"\n                  }\n                }\n              ]\n            }\n",
    "source": "lidarbtc/ironrdp",
    "path": ".github/workflows/npm-publish.yml",
    "url": "https://github.com/lidarbtc/ironrdp/blob/27650265c811f80b4a23edd448cde487d37f0664/.github/workflows/npm-publish.yml",
    "retrieved_at": "2025-08-20T01:44:11.203949Z"
  },
  {
    "question": "What is the purpose of the `GH_PAT` secret used in the deploy step?",
    "answer": "name: Deploy on GitHub Pages\n\non:\n  push:\n    branches:\n    - master\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n    - name: Yarn install\n      uses: Borales/actions-yarn@v2.3.0\n      with:\n        cmd: install\n    - name: Yarn build\n      uses: Borales/actions-yarn@v2.3.0\n      with:\n        cmd: build:prod\n    - name: Deploy\n      uses: maxheld83/ghpages@v0.3.0\n      env:\n        BUILD_DIR: dist/\n        GH_PAT: ${{ secrets.GH_PAT }}\n",
    "source": "Cardoso-topdev/vue_template",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/Cardoso-topdev/vue_template/blob/6dbd6b9c3c0d9c28e7a88253da87158cca6d72f7/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-20T01:44:11.900755Z"
  },
  {
    "question": "What event triggers this workflow to run?",
    "answer": "name: e2e\n\non: push\n\njobs:\n  e2e:\n    name: E2E tests\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - uses: pnpm/action-setup@v2\n        with:\n          version: 8\n      - name: Install cypress\n        uses: cypress-io/github-action@v5\n        with:\n          runTests: false\n      - run: pnpm build\n      - name: Cypress run\n        uses: cypress-io/github-action@v6\n        with:\n          install: false\n          working-directory: apps/web\n          start: pnpm start\n          wait-on: \"http://localhost:3000\"\n        env:\n          NEXT_PUBLIC_SITE_URL: ${{secrets.NEXT_PUBLIC_SITE_URL}}\n          DATABASE_URL: ${{secrets.DATABASE_URL}}\n          STRIPE_SECRET_KEY: ${{secrets.STRIPE_SECRET_KEY}}\n",
    "source": "284247028/supastarter-nextjs",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/284247028/supastarter-nextjs/blob/efd0e6fec2a94bfdefa9e99ca2725ce6ee3b292b/.github/workflows/main.yml",
    "retrieved_at": "2025-08-20T01:44:12.588345Z"
  },
  {
    "question": "What is the purpose of the `docs:build` script executed during the build step?",
    "answer": "name: Qiankun Github Pages Deploy\non:\n  push:\n    branches:\n      - master\njobs:\n  deploy-gh-pages:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: build\n        run: |\n          yarn\n          yarn docs:build\n\n      - name: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./dist\n",
    "source": "baron65/qianKun",
    "path": ".github/workflows/github-pages.yml",
    "url": "https://github.com/baron65/qianKun/blob/513293336bb536e4ab329cd3db9653b4b5c4df49/.github/workflows/github-pages.yml",
    "retrieved_at": "2025-08-20T01:44:13.384986Z"
  },
  {
    "question": "Under what conditions does this workflow deploy the Hugo site to GitHub Pages?",
    "answer": "name: GitHub Pages\n\non:\n  push:\n    branches:\n      - main  # Set a branch to deploy\n  pull_request:\n\njobs:\n  deploy:\n    runs-on: ubuntu-20.04\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.ref }}\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          submodules: true  # Fetch Hugo themes (true OR recursive)\n          fetch-depth: 0    # Fetch all history for .GitInfo and .Lastmod\n\n      - name: Setup Hugo\n        uses: peaceiris/actions-hugo@v2\n        with:\n          hugo-version: '0.85.0'\n          # extended: true\n\n      - name: Build\n        run: hugo --minify\n\n      - name: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        if: ${{ github.ref == 'refs/heads/main' }}\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./public",
    "source": "sharadcodes/hugo-theme-serial-programmer",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/sharadcodes/hugo-theme-serial-programmer/blob/df0f7ab4cb4a48d5a9fabdae21ed8d5d83e02b99/.github/workflows/main.yml",
    "retrieved_at": "2025-08-20T01:44:14.050529Z"
  },
  {
    "question": "What actions are performed to build the Hugo site in this workflow?",
    "answer": "name: Build\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  workflow_dispatch:\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]\n        hugo_version: [\"latest\"]\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Hugo setup\n        uses: peaceiris/actions-hugo@v2\n        with:\n          hugo-version: ${{ matrix.hugo_version }}\n          extended: true\n\n      - uses: actions/setup-node@v3\n        with:\n          node-version: '16'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Build\n        run: hugo -v --source=exampleSite --themesDir ../.. --gc\n",
    "source": "minseong0609/hugo_blog",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/minseong0609/hugo_blog/blob/f925223e9480ed0556ad8a8d1b5efd6559aa8dfc/.github/workflows/build.yml",
    "retrieved_at": "2025-08-20T01:44:14.795432Z"
  },
  {
    "question": "Does this workflow run molecule tests on every push and pull request to the master branch?",
    "answer": "---\nname: ipr-cnrs.nftables.molecule\n\non:\n  push:\n    branches: [master]\n  pull_request:\n    branches: [master]\n\n  workflow_dispatch:\n\njobs:\n  test:\n    runs-on:  ubuntu-latest\n    steps:\n\n      - name: checkout\n        uses: actions/checkout@v2\n        with:\n          path: \"${{ github.repository }}\"\n\n      - name: Ansible Molecule\n        uses: robertdebock/molecule-action@4.0.9\n        with:\n          molecule_working_dir: \"${{ github.repository }}\"\n",
    "source": "chrisvanmeer/ansible-role-nftables",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/chrisvanmeer/ansible-role-nftables/blob/c4da513d08a24df4ae042b85a10d9d3f7a27ca8a/.github/workflows/main.yml",
    "retrieved_at": "2025-08-20T01:44:15.432056Z"
  },
  {
    "question": "Under what conditions does the workflow build and push the \"latest\" tagged Docker image?",
    "answer": "name: Build & Publish PodDefaults Docker image\non:\n  push:\n    branches:\n      - master\n      - v*-branch\n    paths:\n      - components/admission-webhook/**\n      - releasing/version/VERSION\n\nenv:\n  DOCKER_USER: kubeflownotebookswg\n  IMG: kubeflownotebookswg/poddefaults-webhook\n  ARCH: linux/ppc64le,linux/amd64\n\njobs:\n  push_to_registry:\n    name: Build & Push Docker image to Docker Hub\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n\n    - uses: dorny/paths-filter@v2\n      id: filter\n      with:\n        filters: |\n          version:\n            - 'releasing/version/VERSION'\n\n    - name: Login to DockerHub\n      uses: docker/login-action@v2\n      with:\n        username: ${{ env.DOCKER_USER }}\n        password: ${{ secrets.KUBEFLOWNOTEBOOKSWG_DOCKER_TOKEN }}\n\n    - name: Setup QEMU\n      uses: docker/setup-qemu-action@v2\n\n    - name: Setup Docker Buildx\n      uses: docker/setup-buildx-action@v2\n\n    - name: Build and push multi-arch docker image\n      run: |\n        cd components/admission-webhook\n        make docker-build-push-multi-arch\n\n    - name: Build and push latest multi-arch docker image\n      if: github.ref == 'refs/heads/master'\n      run: |\n        export TAG=latest\n        cd components/admission-webhook\n        make docker-build-push-multi-arch\n\n    - name: Build and push multi-arch docker image on Version change\n      id: version\n      if: steps.filter.outputs.version == 'true'\n      run: |\n        export TAG=$(cat releasing/version/VERSION)\n        cd components/admission-webhook\n        make docker-build-push-multi-arch\n",
    "source": "Zveroloff/kubeflow",
    "path": ".github/workflows/poddefaults_docker_publish.yaml",
    "url": "https://github.com/Zveroloff/kubeflow/blob/1552c90ccaee7acdea3495175592fcca46ae3a77/.github/workflows/poddefaults_docker_publish.yaml",
    "retrieved_at": "2025-08-20T01:44:16.317719Z"
  },
  {
    "question": "What specific contribution graph is generated and pushed to the repository by this workflow?",
    "answer": "name: GitHub-Profile-3D-Contrib\n\non:\n  schedule: # 03:00 JST == 18:00 UTC\n    - cron: \"0 18 * * *\"\n  workflow_dispatch:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    name: generate-github-profile-3d-contrib\n    steps:\n      - uses: actions/checkout@v2\n      - uses: yoshi389111/github-profile-3d-contrib@0.7.0\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          USERNAME: ${{ github.repository_owner }}\n      - name: Commit & Push\n        run: |\n          git config user.name github-actions\n          git config user.email github-actions@github.com\n          git add -A .\n          git commit -m \"generated\"\n          git push",
    "source": "Maximilian-Stefan-Ernst/Maximilian-Stefan-Ernst",
    "path": ".github/workflows/contributions.yml",
    "url": "https://github.com/Maximilian-Stefan-Ernst/Maximilian-Stefan-Ernst/blob/89c52643a1b5bc4a3eb2e4143f6e020acfe79ac0/.github/workflows/contributions.yml",
    "retrieved_at": "2025-08-20T01:44:17.095344Z"
  },
  {
    "question": "What is the purpose of the `send_results` job, and what kind of information does it send to Slack?",
    "answer": "name: Self-hosted runner (scheduled)\n\non:\n  push:\n    branches:\n      - multi_ci_*\n  repository_dispatch:\n  schedule:\n    - cron: \"0 0 * * *\"\n\nenv:\n  HF_HOME: /mnt/cache\n  TRANSFORMERS_IS_CI: yes\n  RUN_SLOW: yes\n  OMP_NUM_THREADS: 16\n  MKL_NUM_THREADS: 16\n  PYTEST_TIMEOUT: 600\n  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n\njobs:\n  run_all_tests_torch_gpu:\n    runs-on: [self-hosted, docker-gpu, single-gpu]\n    container:\n      image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev git espeak-ng\n          pip install --upgrade pip\n          pip install .[integrations,sklearn,testing,onnxruntime,sentencepiece,torch-speech,vision,timm]\n          pip install https://github.com/kpu/kenlm/archive/master.zip\n          python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n          wandb login ${{ secrets.WANDB_API_KEY }}\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          utils/print_env_pt.py\n\n      - name: Run all tests on GPU\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_gpu_durations.txt\n\n      - name: Run examples tests on GPU\n        if: ${{ always() }}\n        env:\n          OMP_NUM_THREADS: 16\n          MKL_NUM_THREADS: 16\n          RUN_SLOW: yes\n          HF_HOME: /mnt/cache\n          TRANSFORMERS_IS_CI: yes\n        run: |\n          pip install -r examples/pytorch/_tests_requirements.txt\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=examples_torch_gpu examples\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/examples_torch_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/examples_torch_gpu_durations.txt\n\n      - name: Run all pipeline tests on GPU\n        if: ${{ always() }}\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=tests_torch_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_pipeline_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_pipeline_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_tests_torch_gpu_test_reports\n          path: reports\n\n#  run_all_tests_flax_gpu:\n#    runs-on: [self-hosted, docker-gpu-test, single-gpu]\n#    container:\n#      image: tensorflow/tensorflow:2.4.1-gpu\n#      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n#    steps:\n#      - name: Launcher docker\n#        uses: actions/checkout@v2\n#\n#      - name: NVIDIA-SMI\n#        continue-on-error: true\n#        run: |\n#          nvidia-smi\n#\n#      - name: Install dependencies\n#        run: |\n#          pip install --upgrade pip\n#          pip install --upgrade \"jax[cuda111]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n#          pip install .[flax,integrations,sklearn,testing,sentencepiece,flax-speech,vision]\n#          pip install https://github.com/kpu/kenlm/archive/master.zip\n#\n#      - name: Are GPUs recognized by our DL frameworks\n#        run: |\n#          python -c \"from jax.lib import xla_bridge; print('GPU available:', xla_bridge.get_backend().platform)\"\n#          python -c \"import jax; print('Number of GPUs available:', len(jax.local_devices()))\"\n#\n#      - name: Run all tests on GPU\n#        run: |\n#          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_flax_gpu tests\n#\n#      - name: Failure short reports\n#        if: ${{ always() }}\n#        run: cat reports/tests_flax_gpu_failures_short.txt\n#\n#      - name: Test durations\n#        if: ${{ always() }}\n#        run: cat reports/tests_flax_gpu_durations.txt\n#\n#      - name: Test suite reports artifacts\n#        if: ${{ always() }}\n#        uses: actions/upload-artifact@v2\n#        with:\n#          name: run_all_tests_flax_gpu_test_reports\n#          path: reports\n\n  run_all_tests_tf_gpu:\n    runs-on: [self-hosted, docker-gpu, single-gpu]\n    container:\n      image: tensorflow/tensorflow:2.4.1-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev git espeak-ng\n          pip install --upgrade pip\n          pip install .[sklearn,testing,onnx,sentencepiece,tf-speech,vision]\n          pip install https://github.com/kpu/kenlm/archive/master.zip\n\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          TF_CPP_MIN_LOG_LEVEL=3 python -c \"import tensorflow as tf; print('TF GPUs available:', bool(tf.config.list_physical_devices('GPU')))\"\n          TF_CPP_MIN_LOG_LEVEL=3 python -c \"import tensorflow as tf; print('Number of TF GPUs available:', len(tf.config.list_physical_devices('GPU')))\"\n\n      - name: Run all tests on GPU\n        env:\n          TF_NUM_INTEROP_THREADS: 1\n          TF_NUM_INTRAOP_THREADS: 16\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_tf_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_tf_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_tf_gpu_durations.txt\n\n      - name: Run all pipeline tests on GPU\n        if: ${{ always() }}\n        env:\n          RUN_PIPELINE_TESTS: yes\n          TF_NUM_INTEROP_THREADS: 1\n          TF_NUM_INTRAOP_THREADS: 16\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=tests_tf_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_tf_pipeline_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_tf_pipeline_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_tests_tf_gpu_test_reports\n          path: reports\n\n  run_all_examples_torch_xla_tpu:\n    runs-on: [self-hosted, docker-tpu-test, tpu-v3-8]\n    container:\n      image: gcr.io/tpu-pytorch/xla:nightly_3.8_tpuvm\n      options: --privileged -v \"/lib/libtpu.so:/lib/libtpu.so\" -v /mnt/cache/.cache/huggingface:/mnt/cache/ --shm-size 16G\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: Install dependencies\n        run: |\n          pip install --upgrade pip\n          pip install .[testing]\n\n      - name: Are TPUs recognized by our DL frameworks\n        env:\n          XRT_TPU_CONFIG: localservice;0;localhost:51011\n        run: |\n          python -c \"import torch_xla.core.xla_model as xm; print(xm.xla_device())\"\n\n      - name: Run example tests on TPU\n        env:\n          XRT_TPU_CONFIG: \"localservice;0;localhost:51011\"\n          MKL_SERVICE_FORCE_INTEL: \"1\"  # See: https://github.com/pytorch/pytorch/issues/37377\n\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_xla_tpu examples/pytorch/test_xla_examples.py\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_xla_tpu_failures_short.txt\n\n      - name: Tests durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_xla_tpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_examples_torch_xla_tpu\n          path: reports\n\n  run_all_tests_torch_multi_gpu:\n    runs-on: [self-hosted, docker-gpu, multi-gpu]\n    container:\n      image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime\n      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        continue-on-error: true\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev git espeak-ng\n          pip install --upgrade pip\n          pip install .[integrations,sklearn,testing,onnxruntime,sentencepiece,torch-speech,vision,timm]\n          pip install https://github.com/kpu/kenlm/archive/master.zip\n          python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n          wandb login ${{ secrets.WANDB_API_KEY }}\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          utils/print_env_pt.py\n\n      - name: Run all tests on GPU\n        env:\n          MKL_SERVICE_FORCE_INTEL: 1\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_multi_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_multi_gpu_durations.txt\n\n      - name: Run all pipeline tests on GPU\n        if: ${{ always() }}\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=tests_torch_pipeline_multi_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_pipeline_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_pipeline_multi_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_tests_torch_multi_gpu_test_reports\n          path: reports\n\n  run_all_tests_tf_multi_gpu:\n    runs-on: [self-hosted, docker-gpu, multi-gpu]\n    container:\n      image: tensorflow/tensorflow:2.4.1-gpu\n      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        continue-on-error: true\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev git espeak-ng\n          pip install --upgrade pip\n          pip install .[sklearn,testing,onnx,sentencepiece,tf-speech,vision]\n          pip install https://github.com/kpu/kenlm/archive/master.zip\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          TF_CPP_MIN_LOG_LEVEL=3 python -c \"import tensorflow as tf; print('TF GPUs available:', bool(tf.config.list_physical_devices('GPU')))\"\n          TF_CPP_MIN_LOG_LEVEL=3 python -c \"import tensorflow as tf; print('Number of TF GPUs available:', len(tf.config.list_physical_devices('GPU')))\"\n\n      - name: Run all tests on GPU\n        env:\n          TF_NUM_INTEROP_THREADS: 1\n          TF_NUM_INTRAOP_THREADS: 16\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_tf_multi_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_tf_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_tf_multi_gpu_durations.txt\n\n      - name: Run all pipeline tests on GPU\n        if: ${{ always() }}\n        env:\n          RUN_PIPELINE_TESTS: yes\n          TF_NUM_INTEROP_THREADS: 1\n          TF_NUM_INTRAOP_THREADS: 16\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=tests_tf_pipeline_multi_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_tf_pipeline_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_tf_pipeline_multi_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_tests_tf_multi_gpu_test_reports\n          path: reports\n\n#  run_all_tests_flax_multi_gpu:\n#    runs-on: [self-hosted, docker-gpu, multi-gpu]\n#    container:\n#      image: tensorflow/tensorflow:2.4.1-gpu\n#      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n#    steps:\n#      - name: Launcher docker\n#        uses: actions/checkout@v2\n#\n#      - name: NVIDIA-SMI\n#        run: |\n#          nvidia-smi\n#\n#      - name: Install dependencies\n#        run: |\n#          pip install --upgrade pip\n#          pip install --upgrade \"jax[cuda111]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n#          pip install .[flax,integrations,sklearn,testing,sentencepiece,flax-speech,vision]\n#\n#      - name: Are GPUs recognized by our DL frameworks\n#        run: |\n#          python -c \"from jax.lib import xla_bridge; print('GPU available:', xla_bridge.get_backend().platform)\"\n#          python -c \"import jax; print('Number of GPUs available:', len(jax.local_devices()))\"\n#\n#      - name: Run all tests on GPU\n#        run: |\n#          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_flax_gpu tests\n#\n#      - name: Failure short reports\n#        if: ${{ always() }}\n#        run: cat reports/tests_flax_gpu_failures_short.txt\n#\n#      - name: Test suite reports artifacts\n#        if: ${{ always() }}\n#        uses: actions/upload-artifact@v2\n#        with:\n#          name: run_all_tests_flax_gpu_test_reports\n#          path: reports\n\n  run_all_tests_torch_cuda_extensions_gpu:\n    runs-on: [self-hosted, docker-gpu, single-gpu]\n    container:\n      image: nvcr.io/nvidia/pytorch:21.03-py3\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libaio-dev\n          pip install --upgrade pip\n          pip install .[testing,deepspeed]\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          utils/print_env_pt.py\n\n      - name: Run all tests on GPU\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_cuda_extensions_gpu tests/deepspeed tests/extended\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_cuda_extensions_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_cuda_extensions_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_tests_torch_cuda_extensions_gpu_test_reports\n          path: reports\n\n  run_all_tests_torch_cuda_extensions_multi_gpu:\n    runs-on: [self-hosted, docker-gpu, multi-gpu]\n    container:\n      image: nvcr.io/nvidia/pytorch:21.03-py3\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        continue-on-error: true\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libaio-dev\n          pip install --upgrade pip\n          rm -rf ~/.cache/torch_extensions/ # shared between conflicting builds\n          pip install .[testing,deepspeed,fairscale]\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          utils/print_env_pt.py\n\n      - name: Run all tests on GPU\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_cuda_extensions_multi_gpu tests/deepspeed tests/extended\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_cuda_extensions_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_cuda_extensions_multi_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_tests_torch_cuda_extensions_multi_gpu_test_reports\n          path: reports\n\n  send_results:\n    name: Send results to webhook\n    runs-on: ubuntu-latest\n    if: always()\n    needs: [\n        run_all_tests_torch_gpu,\n        run_all_tests_tf_gpu,\n        run_all_tests_torch_multi_gpu,\n        run_all_tests_tf_multi_gpu,\n        run_all_tests_torch_cuda_extensions_gpu,\n        run_all_tests_torch_cuda_extensions_multi_gpu\n    ]\n    steps:\n      - uses: actions/checkout@v2\n\n      - uses: actions/download-artifact@v2\n\n      - name: Send message to Slack\n        env:\n          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n\n\n        run: |\n          pip install slack_sdk\n          python utils/notification_service.py scheduled\n",
    "source": "reynolds9808/transformers_backdoor_attack",
    "path": ".github/workflows/self-scheduled.yml",
    "url": "https://github.com/reynolds9808/transformers_backdoor_attack/blob/e9536ea0046703f854fa0c05cfdcf179000bb1a2/.github/workflows/self-scheduled.yml",
    "retrieved_at": "2025-08-21T01:42:57.083030Z"
  },
  {
    "question": "Under what conditions does the `setup` job execute based on the release name?",
    "answer": "name: rAPId Release\n\non:\n  release:\n    types: [released]\n\njobs:\n  setup:\n    if: \"${{ startsWith(github.event.release.name, 'SDK: ') }}\"\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Log commit SHA\n        run: echo $GITHUB_SHA\n\n  sdk-release:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          echo TWINE_USERNAME=${{ secrets.TWINE_USERNAME }} >> .env\n          echo TWINE_PASSWORD=${{ secrets.TWINE_PASSWORD }} >> .env\n          echo TWINE_NON_INTERACTIVE=true >> .env\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - name: Setup Python Environment\n        run: |\n          make sdk/setup\n\n      - name: SDK Release\n        run: make sdk/release\n",
    "source": "no10ds/rapid",
    "path": ".github/workflows/release_sdk.yml",
    "url": "https://github.com/no10ds/rapid/blob/c763da3d92b4371effa90f6df460c4d027b899c6/.github/workflows/release_sdk.yml",
    "retrieved_at": "2025-08-21T01:42:57.720317Z"
  },
  {
    "question": "What tests are performed to verify the SDL2 build for the Sony Playstation Portable?",
    "answer": "name: Build (Sony Playstation Portable)\n\non: [push, pull_request]\n\njobs:\n  psp:\n    runs-on: ubuntu-latest\n    container: pspdev/pspdev:latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Setup dependencies\n      run: |\n        apk update \n        apk add cmake gmp mpc1 mpfr4 make pkgconf\n    - name: Configure CMake\n      run: |\n        cmake -S . -B build \\\n          -DCMAKE_TOOLCHAIN_FILE=$PSPDEV/psp/share/pspdev.cmake \\\n          -DSDL_WERROR=ON \\\n          -DSDL_TESTS=ON \\\n          -DSDL_INSTALL_TESTS=ON \\\n          -DCMAKE_BUILD_TYPE=Release \\\n          -DCMAKE_INSTALL_PREFIX=prefix\n    - name: Build\n      run: cmake --build build --config Release\n    - name: Install\n      run: |\n        echo \"SDL2_DIR=$(pwd)/prefix\" >> $GITHUB_ENV\n        cmake --install build --config Release\n        ( cd prefix; find ) | LC_ALL=C sort -u\n    - name: Verify CMake configuration files\n      run: |\n        cmake -S cmake/test -B cmake_config_build \\\n          -DCMAKE_TOOLCHAIN_FILE=$PSPDEV/psp/share/pspdev.cmake \\\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }} \\\n          -DTEST_SHARED=FALSE \\\n          -DCMAKE_BUILD_TYPE=Release\n        cmake --build cmake_config_build --verbose\n    - name: Verify sdl2-config\n      run: |\n        export CC=psp-gcc\n        export PATH=${{ env.SDL2_DIR }}/bin:$PATH\n        export EXTRA_LDFLAGS=\"-L$PSPDEV/lib -L$PSPDEV/psp/lib -L$PSPDEV/psp/sdk/lib\"\n        cmake/test/test_sdlconfig.sh\n    - name: Verify sdl2.pc\n      run: |\n        export CC=psp-gcc\n        export PKG_CONFIG_PATH=${{ env.SDL2_DIR }}/lib/pkgconfig\n        export EXTRA_LDFLAGS=\"-L$PSPDEV/lib -L$PSPDEV/psp/lib -L$PSPDEV/psp/sdk/lib\"\n        cmake/test/test_pkgconfig.sh\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/psp.yaml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/psp.yaml",
    "retrieved_at": "2025-08-21T01:42:58.589925Z"
  },
  {
    "question": "What triggers this workflow, and what schedule does it follow?",
    "answer": "name: Doctests\n\non:\n  push:\n    branches:\n      - doctest*\n  repository_dispatch:\n  schedule:\n    - cron: \"0 0 * * *\"\n\n\nenv:\n  HF_HOME: /mnt/cache\n  TRANSFORMERS_IS_CI: yes\n  RUN_SLOW: yes\n  OMP_NUM_THREADS: 16\n  MKL_NUM_THREADS: 16\n  PYTEST_TIMEOUT: 600\n\njobs:\n  run_doctests:\n    runs-on: [self-hosted, docker-gpu-test, single-gpu]\n    container:\n      image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev\n          pip install --upgrade pip\n          pip install .[testing,torch-speech]\n\n      - name: Prepare files for doctests\n        run: |\n          python utils/prepare_for_doc_test.py src docs\n\n      - name: Run doctests\n        run: |\n          pytest --doctest-modules $(cat utils/documentation_tests.txt) -sv --doctest-continue-on-failure --doctest-glob=\"*.mdx\"\n\n      - name: Clean files after doctests\n        run: |\n          python utils/prepare_for_doc_test.py src docs --remove_new_line\n",
    "source": "datakloud/transformers-play",
    "path": ".github/workflows/doctests.yml",
    "url": "https://github.com/datakloud/transformers-play/blob/f380bf2b612e6030ef8bc8904b287d274f035e29/.github/workflows/doctests.yml",
    "retrieved_at": "2025-08-21T01:42:59.348340Z"
  },
  {
    "question": "Under what conditions will this workflow be triggered, specifically regarding the `renovate/**` branches?",
    "answer": "name: CI\n\non:\n  workflow_dispatch:\n  pull_request:\n  push:\n    branches:\n      - dev\n      - 'renovate/**'\n      - '!renovate/lock-file-maintenance'\n\njobs:\n  CI:\n    strategy:\n      fail-fast: false\n\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: install pnpm\n        uses: pnpm/action-setup@v2\n        with:\n          version: 7.x.x\n\n      - name: Setup Node\n        uses: actions/setup-node@v3\n        with:\n          node-version: 16\n          cache: pnpm\n\n      - name: install dependencies\n        run: pnpm install\n\n      - name: check formatting\n        run: pnpm format\n\n      - name: run linter\n        run: pnpm lint\n\n      #- name: run tests\n      #  run: pnpm test\n",
    "source": "Josem1801/tauri",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/Josem1801/tauri/blob/4b5b72d6abe9770f5482d75dea7befa786be36bb/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-21T01:42:59.974147Z"
  },
  {
    "question": "What actions are triggered when a pull request is opened, synchronized, or reopened?",
    "answer": "name: Pull Request\n\non:\n  pull_request:\n    types: [ opened, synchronize, reopened ]\n\npermissions:\n  contents: write\n  actions: read\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Validate Gradle Wrapper\n        uses: gradle/wrapper-validation-action@v2\n\n      - name: Setup JDK\n        uses: actions/setup-java@v4\n        with:\n          distribution: 'zulu'\n          java-version: 17\n\n      - name: Setup Gradle\n        uses: gradle/gradle-build-action@v3\n\n      - name: Check spotless\n        run: ./gradlew spotlessCheck\n\n      - name: Build App\n        run: ./gradlew :app:assembleDebug\n",
    "source": "igoracad/socialiteWidget",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/igoracad/socialiteWidget/blob/8069627a302bca2a52fff2bc6c5135a2558fc002/.github/workflows/build.yml",
    "retrieved_at": "2025-08-21T01:43:00.766236Z"
  },
  {
    "question": "What kind of evaluations (ESLint, Jest, store) are performed on the pull request, and in what order?",
    "answer": "on:\n  pull_request:\n    types: [opened, synchronize]\n    \njobs:\n  evaluator:\n    runs-on: self-hosted\n    name: Evaluator\n    services:\n      mysql:\n        image: mysql:8.0.34\n        env:\n          MYSQL_ROOT_PASSWORD: 'password'\n        ports:\n          - 3306:3306\n        options: --health-cmd=\"mysqladmin ping\" --health-interval=10s --health-timeout=5s --health-retries=3\n    steps:    \n      - name: Fetch project repository\n        uses: actions/checkout@v2\n\n      - name: Fetch Blocked Files Checkout action\n        uses: actions/checkout@v2\n        with:\n          repository: betrybe/blocked-files-checkout-action\n          ref: v2\n          token: ${{ secrets.GIT_HUB_PAT }}\n          path: .github/actions/blocked-files-checkout\n\n      - name: Fetch ESLint evaluator\n        uses: actions/checkout@v2\n        with:\n          repository: betrybe/eslint-linter-action\n          ref: v3.4\n          token: ${{ secrets.GIT_HUB_PAT }}\n          path: .github/actions/eslint-evaluator\n\n      - name: Fetch Jest evaluator\n        uses: actions/checkout@v2\n        with:\n          repository: betrybe/jest-evaluator-action\n          ref: v9.5\n          token: ${{ secrets.GIT_HUB_PAT }}\n          path: .github/actions/jest-evaluator\n\n      - name: Fetch Store evaluation\n        uses: actions/checkout@v2\n        with:\n          repository: betrybe/store-evaluation-action\n          ref: v8.0\n          token: ${{ secrets.GIT_HUB_PAT }}\n          path: .github/actions/store-evaluation\n\n      - name: Setup NodeJS\n        uses: actions/setup-node@v1.4.4\n        with:\n          node-version: '16'\n\n      - name: Restore protected files\n        uses: ./.github/actions/blocked-files-checkout\n        with:\n          restore_branch: 'main'\n\n      - name: Run ESLint evaluator\n        uses: ./.github/actions/eslint-evaluator\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          pr_number: ${{ github.event.pull_request.number }}\n\n      - name: Run Jest evaluation\n        id: evaluator\n        uses: ./.github/actions/jest-evaluator\n        with:\n          pr_author_username: ${{ github.event.pull_request.user.login }}\n        env:\n          MYSQL_USER: 'root'\n          MYSQL_PASSWORD: 'password'\n          MYSQL_HOSTNAME: 'mysql'\n          MYSQL_DATABASE: 'trybe_eval'\n          MYSQL_PORT: 3306\n\n      - name: Run Store evaluation\n        uses: ./.github/actions/store-evaluation\n        with:\n          evaluation-data: ${{ steps.evaluator.outputs.result }}\n          environment: production\n          token: ${{ secrets.GITHUB_TOKEN }}",
    "source": "eduzissimo/Project-Trybefy",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/eduzissimo/Project-Trybefy/blob/e05bf9721ab18303b4e3f78055474fcf6e7840be/.github/workflows/main.yml",
    "retrieved_at": "2025-08-21T01:43:01.521273Z"
  },
  {
    "question": "What pull request event triggers this workflow to add a reminder comment?",
    "answer": "name: Pull Request Opened\npermissions:\n  pull-requests: write\n\n# only trigger on pull request closed events\non:\n  pull_request_target:\n    types: [ opened ]\n\njobs:\n  pr_open_job:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/github-script@d7906e4ad0b1822421a7e6a35d5ca353c962f410 # v6.4.1\n        with:\n          script: |\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: \"Reminder for the PR assignee: If this is a user-visible change, please update the changelog as part of the PR.\"\n            })\n",
    "source": "augur-ai/mantis",
    "path": ".github/workflows/pr-opened.yml",
    "url": "https://github.com/augur-ai/mantis/blob/dce7be5b39e2498d5d4262b5bed15769e10df324/.github/workflows/pr-opened.yml",
    "retrieved_at": "2025-08-21T01:43:02.342533Z"
  },
  {
    "question": "What is the purpose of setting `fetch-depth: 0` in the checkout step?",
    "answer": "# Sample workflow for building and deploying a VitePress site to GitHub Pages\n#\nname: Deploy VitePress site to Pages\n\non:\n  # Runs on pushes targeting the `main` branch. Change this to `master` if you're\n  # using the `master` branch as the default branch.\n  push:\n    branches: [master]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\n# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\nconcurrency:\n  group: pages\n  cancel-in-progress: false\n\njobs:\n  # Build job\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0 # Not needed if lastUpdated is not enabled\n      # - uses: pnpm/action-setup@v3 # Uncomment this if you're using pnpm\n      # - uses: oven-sh/setup-bun@v1 # Uncomment this if you're using Bun\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20\n          cache: npm # or pnpm / yarn\n      - name: Setup Pages\n        uses: actions/configure-pages@v4\n      - name: Install dependencies\n        run: npm ci # or pnpm install / yarn install / bun install\n      - name: Build with VitePress\n        run: npm run docs:build # or pnpm docs:build / yarn docs:build / bun run docs:build\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: .vitepress/dist\n\n  # Deployment job\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    needs: build\n    runs-on: ubuntu-latest\n    name: Deploy\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\n",
    "source": "net-success/vitepress",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/net-success/vitepress/blob/4f1e6d3be5c9b20f40c481dad337b6d71342fe54/.github/workflows/main.yml",
    "retrieved_at": "2025-08-21T01:43:02.896962Z"
  },
  {
    "question": "Does this workflow build the project only when a pull request targets the `master` branch?",
    "answer": "name: Build on PR\n\non:\n  pull_request:\n    branches:\n      - master\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Use Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '20'\n      \n      - name: Install Dependencies\n        run: npm install\n        \n      - name: Run Build\n        run: npm run build\n",
    "source": "deepak-choure/turboWallet",
    "path": ".github/workflows/build.yaml",
    "url": "https://github.com/deepak-choure/turboWallet/blob/6b1ed27c2ceebaeb70261158d201335e8ef8d17e/.github/workflows/build.yaml",
    "retrieved_at": "2025-08-21T01:43:03.601111Z"
  },
  {
    "question": "What benchmarks are run and where are the results sent by this workflow?",
    "answer": "name: Run Benchmarks and upload results\n\non:\n  push:\n    branches:\n    - benchmark  # TODO: change it back to master once we really track the results. We commented this as speed.wasmer.io is failing\n\njobs:\n  run_benchmark:\n    name: Benchmark on ${{ matrix.build }}\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        build: [linux]\n        include:\n          - build: linux\n            os: ubuntu-latest\n    env:\n      SCCACHE_AZURE_BLOB_CONTAINER: wasmerstoragesccacheblob\n      SCCACHE_AZURE_CONNECTION_STRING: ${{ secrets.SCCACHE_AZURE_CONNECTION_STRING }}\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          profile: minimal\n          override: true\n      - name: Configure cargo data directory\n        # After this point, all cargo registry and crate data is stored in\n        # $GITHUB_WORKSPACE/.cargo_home. This allows us to cache only the files\n        # that are needed during the build process. Additionally, this works\n        # around a bug in the 'cache' action that causes directories outside of\n        # the workspace dir to be saved/restored incorrectly.\n        run: echo \"CARGO_HOME=$(pwd)/.cargo_home\" >> $GITHUB_ENV\n      - name: Cache\n        uses: actions/cache@master\n        with:\n          # Note: crates from the git repo always get rebuilt\n          # so we cache only those subdirectories of target/{debug|release} that\n          # contain the build output for crates that come from the registry.\n          path: |-\n            .cargo_home\n            target/*/.*\n            target/*/build\n            target/*/deps\n          key: ${{ matrix.os }}-${{ hashFiles('Cargo.lock') }}\n          restore-keys: |\n            ${{ matrix.os }}\n      - name: Install LLVM (Linux)\n        if: matrix.os == 'ubuntu-latest'\n        run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz -L -o llvm.tar.xz\n          mkdir -p /opt/llvm-10\n          tar xf llvm.tar.xz --strip-components=1 -C /opt/llvm-10\n          echo '/opt/llvm-10/bin' >> $GITHUB_PATH\n          echo 'name=LLVM_SYS_100_PREFIX=/opt/llvm-10' >> $GITHUB_ENV\n      - name: Install Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.8\n      - name: Install Python dependencies\n        run: |\n          pip install codespeed-client\n          pip install toml\n      - name: Run Benchmark\n        run: |\n          make bench\n          git clone https://github.com/wasmerio/wasmer-bench\n\n          python3 wasmer-bench/send_metrics.py\n\n",
    "source": "GeorgKreuzmayr/wasmer",
    "path": ".github/workflows/benchmark.yaml",
    "url": "https://github.com/GeorgKreuzmayr/wasmer/blob/14d8084c29f6c47e76a16fdee2083aa997d34482/.github/workflows/benchmark.yaml",
    "retrieved_at": "2025-08-22T01:43:27.681380Z"
  },
  {
    "question": "What types of tags trigger this workflow?",
    "answer": "name: \"Pre-release\"\n\non:\n  push:\n    tags:\n      - \"v[0-9]+.[0-9]+.[0-9]+-alpha.[0-9]+\"  # e.g. v0.37.0-alpha.1, v0.38.0-alpha.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-beta.[0-9]+\"   # e.g. v0.37.0-beta.1, v0.38.0-beta.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-rc[0-9]+\"      # e.g. v0.37.0-rc1, v0.38.0-rc10\n\njobs:\n  prerelease:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      # Similar check to ./release-version.yml, but enforces this when pushing\n      # tags. The ./release-version.yml check can be bypassed and is mainly\n      # present for informational purposes.\n      - name: Check release version\n        run: |\n          # We strip the refs/tags/v prefix of the tag name.\n          TAG_VERSION=${GITHUB_REF#refs/tags/v}\n          # Get the version of the code, which has no \"v\" prefix.\n          CODE_VERSION=`go run ./cmd/cometbft/ version`\n          if [ \"$TAG_VERSION\" != \"$CODE_VERSION\" ]; then\n            echo \"\"\n            echo \"Tag version ${TAG_VERSION} does not match code version ${CODE_VERSION}\"\n            echo \"\"\n            echo \"Please either fix the release tag or the version of the software in version/version.go.\"\n            exit 1\n          fi\n\n      - name: Generate release notes\n        run: |\n          VERSION=\"${GITHUB_REF#refs/tags/}\"\n          CHANGELOG_URL=\"https://github.com/cometbft/cometbft/blob/${VERSION}/CHANGELOG.md\"\n          echo \"See the [CHANGELOG](${CHANGELOG_URL}) for changes available in this pre-release, but not yet officially released.\" > ../release_notes.md\n\n      - name: Release\n        uses: goreleaser/goreleaser-action@v4\n        with:\n          version: latest\n          args: release --clean --release-notes ../release_notes.md\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n  prerelease-success:\n    needs: prerelease\n    if: ${{ success() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack upon pre-release\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          RELEASE_URL: \"${{ github.server_url }}/${{ github.repository }}/releases/tag/${{ github.ref_name }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":sparkles: New CometBFT pre-release: <${{ env.RELEASE_URL }}|${{ github.ref_name }}>\"\n                  }\n                }\n              ]\n            }\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/pre-release.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/pre-release.yml",
    "retrieved_at": "2025-08-22T01:43:28.348782Z"
  },
  {
    "question": "What action is triggered when a push event occurs with a tag matching the pattern \"v[0-9]+.[0-9]+.[0-9]+\"?",
    "answer": "name: \"Release\"\n\non:\n  push:\n    tags:\n      - \"v[0-9]+.[0-9]+.[0-9]+\"  # Push events to matching v*, i.e. v1.0, v20.15.10\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      # Similar check to ./release-version.yml, but enforces this when pushing\n      # tags. The ./release-version.yml check can be bypassed and is mainly\n      # present for informational purposes.\n      - name: Check release version\n        run: |\n          # We strip the refs/tags/v prefix of the tag name.\n          TAG_VERSION=${GITHUB_REF#refs/tags/v}\n          # Get the version of the code, which has no \"v\" prefix.\n          CODE_VERSION=`go run ./cmd/cometbft/ version`\n          if [ \"$TAG_VERSION\" != \"$CODE_VERSION\" ]; then\n            echo \"\"\n            echo \"Tag version ${TAG_VERSION} does not match code version ${CODE_VERSION}\"\n            echo \"\"\n            echo \"Please either fix the release tag or the version of the software in version/version.go.\"\n            exit 1\n          fi\n\n      - name: Generate release notes\n        run: |\n          VERSION=\"${GITHUB_REF#refs/tags/}\"\n          VERSION_REF=\"${VERSION//[\\.]/}\"\n          CHANGELOG_URL=\"https://github.com/cometbft/cometbft/blob/${VERSION}/CHANGELOG.md#${VERSION_REF}\"\n          echo \"See the [CHANGELOG](${CHANGELOG_URL}) for this release.\" > ../release_notes.md\n\n      - name: Release\n        uses: goreleaser/goreleaser-action@v4\n        with:\n          version: latest\n          args: release --clean --release-notes ../release_notes.md\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n  release-success:\n    needs: release\n    if: ${{ success() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack upon release\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          RELEASE_URL: \"${{ github.server_url }}/${{ github.repository }}/releases/tag/${{ github.ref_name }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":rocket: New CometBFT release: <${{ env.RELEASE_URL }}|${{ github.ref_name }}>\"\n                  }\n                }\n              ]\n            }\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/release.yml",
    "retrieved_at": "2025-08-22T01:43:29.205070Z"
  },
  {
    "question": "Under what conditions are the SDK released to NPM and the `dev-frontend` image released to Docker Hub?",
    "answer": "name: Release SDK & UI\n\nenv:\n  CI: true\n  FORCE_COLOR: true\n\non:\n  push:\n    branches: [master, main]\n  pull_request:\n    branches: [master, main]\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v2\n\n      - uses: actions/setup-node@v2\n        with:\n          node-version: 12.x\n\n      - id: yarn-cache-dir-path\n        run: echo \"::set-output name=dir::$(yarn cache dir)\"\n\n      - uses: actions/cache@v1\n        with:\n          path: ${{ steps.yarn-cache-dir-path.outputs.dir }}\n          key: ${{ runner.os }}-yarn-${{ hashFiles('**/yarn.lock') }}\n          restore-keys: |\n            ${{ runner.os }}-yarn-\n\n      - run: yarn install --frozen-lockfile\n      - run: yarn build\n\n      - name: Test SDK & UI\n        run: yarn test\n\n      - name: Test SDK integration against live contracts\n        if: ${{ github.ref == 'refs/heads/master' }}\n        run: yarn test-live\n\n      - name: Release SDK on NPM\n        if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/master' }}\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: yarn release\n\n      - name: Login to Docker Hub\n        uses: azure/docker-login@v1\n        if: ${{ github.event_name == 'push' }}\n        with:\n          username: ${{ secrets.DOCKER_USERNAME }}\n          password: ${{ secrets.DOCKER_ACCESS_TOKEN }}\n\n      - name: Release dev-frontend on Docker Hub\n        if: ${{ github.event_name == 'push' }}\n        run: |\n          docker-compose build\n          docker-compose push\n        working-directory: ./packages/dev-frontend\n        env:\n          TAG: ${{ fromJSON('{ \"refs/heads/master\":\"latest\", \"refs/heads/main\":\"next\" }')[github.ref] }}\n",
    "source": "hcheng826/crangon",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/hcheng826/crangon/blob/2021a3fe84ac675d5a71e9238e1f1053237715a0/.github/workflows/release.yml",
    "retrieved_at": "2025-08-22T01:43:29.994671Z"
  },
  {
    "question": "What branch is checked out by the `actions/checkout@v3` action in the `e2e-long-test` job?",
    "answer": "# Weekly run of the E2E testnet using the long-running manifest on main\n\n# !! Relevant changes to this file should be propagated to the e2e-nightly-<V>x\n# files for the supported backport branches, when appropriate, modulo version\n# markers.\n\nname: e2e-long-main\non:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 3 * * 3'\n\njobs:\n  e2e-long-test:\n    runs-on: ubuntu-latest\n    timeout-minutes: 120\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker runner\n\n      - name: Run testnet\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/long.toml\n\n  e2e-long-fail:\n    needs: e2e-long-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ github.ref_name }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ github.ref_name }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Weekly long-run E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/e2e-long-37x.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/e2e-long-37x.yml",
    "retrieved_at": "2025-08-22T01:43:30.769673Z"
  },
  {
    "question": "Does this workflow lint protobuf files within the `proto` directory on both pull requests and pushes to the `v0.37.x` branch?",
    "answer": "name: Protobuf Lint\non:\n  pull_request:\n    paths:\n      - 'proto/**'\n  push:\n    branches:\n      - v0.37.x\n    paths:\n      - 'proto/**'\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    timeout-minutes: 5\n    steps:\n      - uses: actions/checkout@v3\n      - uses: bufbuild/buf-setup-action@v1.21.0\n      - uses: bufbuild/buf-lint-action@v1\n        with:\n          input: 'proto'\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/proto-lint.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/proto-lint.yml",
    "retrieved_at": "2025-08-22T01:43:31.571347Z"
  },
  {
    "question": "Does this workflow update and push the index on every push/pull request to the master branch?",
    "answer": "name: CI\n\non:\n  # Trigger the workflow on push or pull request,\n  # but only for the master branch\n  push:\n    branches:\n      - master\n  pull_request:\n    branches:\n      - master\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n      with:\n        ref: ${{ github.head_ref }}\n\n    - name: Run gen index\n      run: \"./index_gen.sh\"\n      shell: bash\n\n    - name: Add & Commit\n      uses: github-actions-x/commit@v2.4\n      with:\n        commit-message: 'Index updated'\n        name: '${{ secrets.GH_USER }}'\n        email: '${{ secrets.GH_EMAIL }}'\n        push-branch: ${{ github.head_ref }}\n        github-token: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Push\n      uses: ad-m/github-push-action@v0.5.0\n      with:\n        github_token: ${{ secrets.GH_TOKEN }}\n        branch: ${{ github.head_ref }}\n",
    "source": "kilitary/yara-malware-rules",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/kilitary/yara-malware-rules/blob/9bb7053697c09b3c24ad6a3e04a70c6000cdafb4/.github/workflows/main.yml",
    "retrieved_at": "2025-08-22T01:43:33.771040Z"
  },
  {
    "question": "What specific end-to-end tests are executed by this workflow?",
    "answer": "# Runs randomly generated E2E testnets nightly on the v0.37.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-37x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01', '02', '03', \"04\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"branch=`git branch --show-current`\" >> $GITHUB_OUTPUT\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner tests\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 5 -d networks/nightly/ -p\n\n      - name: Run ${{ matrix.p2p }} p2p testnets\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ needs.e2e-nightly-test.outputs.git-branch }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "zkMeLabs/moca-cometbft",
    "path": ".github/workflows/e2e-nightly-37x.yml",
    "url": "https://github.com/zkMeLabs/moca-cometbft/blob/04ec82d07cab97bbe98fe1feaf9fc5dbeaae36fb/.github/workflows/e2e-nightly-37x.yml",
    "retrieved_at": "2025-08-22T01:43:34.405995Z"
  },
  {
    "question": "What specific tests are executed by the `make tests` command in the \"Build\" step?",
    "answer": "# Runs randomly generated E2E testnets nightly on the v0.37.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-37x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01', '02', '03', \"04\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"branch=`git branch --show-current`\" >> $GITHUB_OUTPUT\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner tests\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 5 -d networks/nightly/ -p\n\n      - name: Run ${{ matrix.p2p }} p2p testnets\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ needs.e2e-nightly-test.outputs.git-branch }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "lightmos/lightmosbft",
    "path": ".github/workflows/e2e-nightly-37x.yml",
    "url": "https://github.com/lightmos/lightmosbft/blob/6f48b680f6d1bc32abf7f2ad91e84d3d53e7f96d/.github/workflows/e2e-nightly-37x.yml",
    "retrieved_at": "2025-08-22T01:43:35.214572Z"
  },
  {
    "question": "What specific tests are executed by the `e2e-nightly-test` job, and how are they defined?",
    "answer": "# Runs randomly generated E2E testnets nightly on the v0.37.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-37x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01', '02', '03', \"04\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"branch=`git branch --show-current`\" >> $GITHUB_OUTPUT\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner tests\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 5 -d networks/nightly/ -p\n\n      - name: Run ${{ matrix.p2p }} p2p testnets\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ needs.e2e-nightly-test.outputs.git-branch }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "SigmaGmbH/cometbft",
    "path": ".github/workflows/e2e-nightly-37x.yml",
    "url": "https://github.com/SigmaGmbH/cometbft/blob/b23ef56f8e6d8a7015a7f816a61f2e53b0b07b0d/.github/workflows/e2e-nightly-37x.yml",
    "retrieved_at": "2025-08-22T01:43:36.019417Z"
  },
  {
    "question": "What triggers this workflow, and what sequence of jobs will it execute upon triggering?",
    "answer": "name: Deploy Project\non: [push, workflow_dispatch]\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Get repo code\n        uses: actions/checkout@v3\n      - name: Install NodeJS\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: cd ./section-2-exercise && npm ci\n      - name: Run Lint\n        run: cd ./section-2-exercise && npm run lint\n  test:\n    needs: lint\n    runs-on: ubuntu-latest\n    steps:\n      - name: Get repo code\n        uses: actions/checkout@v3\n      - name: Install NodeJS\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: cd ./section-2-exercise && npm ci\n      - name: Run tests\n        run: cd ./section-2-exercise && npm run test\n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Get repo code\n        uses: actions/checkout@v3\n      - name: Install NodeJS\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: cd ./section-2-exercise && npm ci\n      - name: Build project\n        run: cd ./section-2-exercise && npm run build\n      - name: Deploy\n        run: \"echo 'Deploying...'\"\n\n\n\n",
    "source": "OrElharar/github-actions-workshop",
    "path": ".github/workflows/section-2-exercise-deployment.yml",
    "url": "https://github.com/OrElharar/github-actions-workshop/blob/bb9e6f94413bcafd9748806ea9c5caa2b861f11f/.github/workflows/section-2-exercise-deployment.yml",
    "retrieved_at": "2025-08-23T01:40:06.025472Z"
  },
  {
    "question": "What specific versioning scheme is implemented by the \"Bump version\" step in this workflow?",
    "answer": "on:\n  push:\n    branches:\n      - develop\n\nname: Publish to NPM\n\njobs:\n  publish:\n    name: Publish\n    runs-on: ubuntu-latest\n    environment: dev\n    steps:\n      - uses: actions/checkout@v3\n      - name: Use Node.js 18\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18.x'\n          always-auth: true\n      - name: Install Yarn\n        run: npm install -g yarn\n      - name: Bump version\n        id: version\n        run: |\n          CURRENT_VERSION=$(jq -r '.version' package.json)\n\n          BASE_VERSION=$(echo $CURRENT_VERSION | cut -d '-' -f 1)\n\n          NEW_BASE=$(echo $BASE_VERSION | awk -v OFS='.' -F. '{\n            $2 = $2 + 1;\n            $3 = 0;\n            print $0\n          }')\n\n          TIMESTAMP=$(date -u +\"%Y%m%d%H%M%S\")\n\n          COMMIT_SHA=$(git rev-parse --short HEAD)\n\n          NEW_VERSION=\"${NEW_BASE}-dev.${TIMESTAMP}.${COMMIT_SHA}\"\n          echo \"new_version=$NEW_VERSION\" >> $GITHUB_OUTPUT\n      - name: Update package.json version\n        run: |\n          jq \".version = \\\"${{ steps.version.outputs.new_version }}\\\"\" package.json > package.tmp\n          mv package.tmp package.json\n      - name: Install dependencies\n        run: yarn\n      - name: Run tests\n        run: yarn test\n      - name: Build\n        run: yarn build\n      - name: Setup .yarnrc.yml\n        run: |\n          yarn config set npmAuthToken $NPM_AUTH_TOKEN\n          yarn config set npmAlwaysAuth true\n        env:\n          NPM_AUTH_TOKEN: ${{ secrets.NPM_AUTH_TOKEN }}\n      - name: Publish\n        run: yarn npm publish --access public --tag dev\n",
    "source": "ton-org/sandbox",
    "path": ".github/workflows/publish-dev.yml",
    "url": "https://github.com/ton-org/sandbox/blob/24ad00977d3abb99ee027149acee1ab11a162bab/.github/workflows/publish-dev.yml",
    "retrieved_at": "2025-08-23T01:40:06.647322Z"
  },
  {
    "question": "What specific environment variables are set when running tests for the \"Latest klee-uclibc\" configuration in the Linux job?",
    "answer": "name: CI\n\non:\n  pull_request:\n    branches: master\n  push:\n    branches: master\n\n# Defaults for building KLEE\nenv:\n  BASE_IMAGE: ubuntu:bionic-20200807\n  REPOSITORY: klee\n  COVERAGE: 0\n  DISABLE_ASSERTIONS: 0\n  ENABLE_DOXYGEN: 0\n  ENABLE_OPTIMIZED: 1\n  ENABLE_DEBUG: 1\n  GTEST_VERSION: 1.7.0\n  KLEE_RUNTIME_BUILD: \"Debug+Asserts\"\n  LLVM_VERSION: 9\n  METASMT_VERSION: qf_abv\n  MINISAT_VERSION: \"master\"\n  REQUIRES_RTTI: 0\n  SANITIZER_BUILD:\n  SOLVERS: STP:Z3\n  STP_VERSION: 2.3.3\n  TCMALLOC_VERSION: 2.7\n  UCLIBC_VERSION: klee_uclibc_v1.2\n  USE_TCMALLOC: 1\n  USE_LIBCXX: 1\n  Z3_VERSION: 4.8.4\n\njobs:\n  Linux:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        name: [\n                \"LLVM 11\",\n                \"LLVM 10\",\n                \"LLVM 9\",\n                \"LLVM 8\",\n                \"LLVM 7\",\n                \"LLVM 6\",\n                \"LLVM 5\",\n                \"LLVM 4\",\n                \"LLVM 3.9\",\n                \"LLVM 3.8\",\n                \"ASan\",\n                \"UBSan\",\n                \"MSan\",\n                \"Z3 only\",\n                \"metaSMT STP\",\n                \"metaSMT Boolector\",\n                \"STP master\",\n                \"Latest klee-uclibc\",\n                \"Asserts enabled\",\n                \"No TCMalloc, optimised runtime\",\n            ]\n        include:\n          - name: \"LLVM 11\"\n            env:\n              LLVM_VERSION: 11\n          - name: \"LLVM 10\"\n            env:\n              LLVM_VERSION: 10\n          - name: \"LLVM 9\"\n            env:\n              LLVM_VERSION: 9\n          - name: \"LLVM 8\"\n            env:\n              LLVM_VERSION: 8\n          - name: \"LLVM 7\"\n            env:\n              LLVM_VERSION: 7\n          - name: \"LLVM 6\"\n            env:\n              LLVM_VERSION: 6\n          - name: \"LLVM 5\"\n            env:\n              LLVM_VERSION: 5\n          - name: \"LLVM 4\"\n            env:\n              LLVM_VERSION: 4\n          - name: \"LLVM 3.9\"\n            env:\n              LLVM_VERSION: 3.9\n          - name: \"LLVM 3.8\"\n            env:\n              LLVM_VERSION: 3.8\n              USE_LIBCXX: 0\n          # Sanitizer builds. Do unoptimized build otherwise the optimizer might remove problematic code\n          - name: \"ASan\"\n            env:\n              SANITIZER_BUILD: address\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n          - name: \"UBSan\"\n            env:\n              SANITIZER_BUILD: undefined\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n          - name: \"MSan\"\n            env:\n              SANITIZER_BUILD: memory\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n              SOLVERS: STP\n            # Test just using Z3 only\n          - name: \"Z3 only\"\n            env:\n              SOLVERS: Z3\n          # Test just using metaSMT\n          - name: \"metaSMT STP\"\n            env:\n              SOLVERS: metaSMT\n              METASMT_DEFAULT: STP\n              REQUIRES_RTTI: 1\n          - name: \"metaSMT Boolector\"\n            env:\n              SOLVERS: metaSMT\n              METASMT_DEFAULT: BTOR\n              REQUIRES_RTTI: 1\n          # Test we can build against STP master\n          - name: \"STP master\"\n            env:\n              SOLVERS: STP\n              STP_VERSION: master\n          # Check we can build latest klee-uclibc branch\n          - name: \"Latest klee-uclibc\"\n            env:\n              UCLIBC_VERSION: klee_0_9_29\n          # Check at least one build with Asserts disabled.\n          - name: \"Asserts enabled\"\n            env:\n              SOLVERS: STP\n              DISABLE_ASSERTIONS: 1\n          # Check without TCMALLOC and with an optimised runtime library\n          - name: \"No TCMalloc, optimised runtime\"\n            env:\n              USE_TCMALLOC: 0\n              KLEE_RUNTIME_BUILD: \"Release+Debug+Asserts\"\n    steps:\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        env: ${{ matrix.env }}\n        run: scripts/build/build.sh klee --docker --create-final-image\n      - name: Run tests\n        run: scripts/build/run-tests.sh --run-docker --debug\n\n  macOS:\n    runs-on: macos-latest\n    env:\n      BASE: /tmp\n      SOLVERS: STP\n      UCLIBC_VERSION: 0\n      USE_TCMALLOC: 0\n      USE_LIBCXX: 0\n    steps:\n      - name: Install newer version of Bash\n        run: brew install bash\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        run: scripts/build/build.sh klee --debug --install-system-deps\n      - name: Run tests\n        run: scripts/build/run-tests.sh /tmp/klee_build* --debug\n\n  Docker:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout KLEE Code\n        uses: actions/checkout@v2\n      - name: Build Docker image\n        run: docker build .\n\n  Coverage:\n    runs-on: ubuntu-latest\n    env:\n      ENABLE_OPTIMIZED: 0\n      COVERAGE: 1\n    steps:\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        run: scripts/build/build.sh klee --docker --create-final-image\n      - name: Run tests\n        run: scripts/build/run-tests.sh --coverage --upload-coverage --run-docker --debug\n",
    "source": "liuzikai/klc3",
    "path": ".github/workflows/build.yaml",
    "url": "https://github.com/liuzikai/klc3/blob/2d2773dfaf288bee6055c3b0693cb105afda2796/.github/workflows/build.yaml",
    "retrieved_at": "2025-08-23T01:40:08.892245Z"
  },
  {
    "question": "Under what conditions are the \"Build and push (for Push CI) in a daily basis\" steps executed, and for which jobs?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "Beomi/transformers-lmhead-logits",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/Beomi/transformers-lmhead-logits/blob/7a5ac2c84abfadd875eb1d2219bdeba621e28dbe/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-23T01:40:09.729238Z"
  },
  {
    "question": "Under what conditions are the Docker images rebuilt and pushed with the `-push-ci` tag?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "charon52hz/chz_transformers",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/charon52hz/chz_transformers/blob/779b26303dfb6e5218b1346f3a3c1ec87e8a82aa/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-23T01:40:10.538596Z"
  },
  {
    "question": "Under what conditions are the jobs triggered, and how does the `image_postfix` input affect the specific Docker images built and pushed?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "nelionel/T5_small_ADED",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/nelionel/T5_small_ADED/blob/fcf23af80ddd1023ce4be0dcb3615fff85c25108/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-23T01:40:11.451246Z"
  },
  {
    "question": "Under what conditions does the workflow build and push the `-push-ci` tagged Docker images?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "tekiny/rag_ray_finetune",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/tekiny/rag_ray_finetune/blob/a502b0d427e6ea217bb4d28b352297823385860a/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-23T01:40:12.305513Z"
  },
  {
    "question": "How does the `metadata2gha` command generate the test matrices for unit and acceptance tests?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "scibian/puppet-module-arioch-keepalived",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/scibian/puppet-module-arioch-keepalived/blob/b74c85726bf9330f1243bd1f070d34c841aa953b/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-23T01:40:12.895486Z"
  },
  {
    "question": "How does `metadata2gha` determine the `puppet_unit_test_matrix`, `beaker_setfiles`, and `puppet_major_versions` outputs?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "SoftwareHeritage/puppet-puppet-archive",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/SoftwareHeritage/puppet-puppet-archive/blob/17315544ed355179d962caa6ff4ee83e8439b0ef/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-23T01:40:13.788312Z"
  },
  {
    "question": "How does the `metadata2gha` command in the `setup_matrix` job determine the test matrix configurations?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "jonsax/puppet-nginx",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/jonsax/puppet-nginx/blob/b704d632d50105256d46233b89f528a1a9bb9e7f/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-23T01:40:14.449146Z"
  },
  {
    "question": "How does the `metadata2gha` command generate the test matrix for unit and acceptance tests?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "Daemon-Solutions/puppet-rundeck",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/Daemon-Solutions/puppet-rundeck/blob/2feaa75b16f614bc14bfb10042321f9d232a2c7a/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-24T01:53:12.711273Z"
  },
  {
    "question": "What specific files or directories are excluded from triggering the workflow when changes are pushed or pulled?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "LokiXun/HumanPoseEstimation_TongjiSurvey",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/LokiXun/HumanPoseEstimation_TongjiSurvey/blob/838bef00caa26e8aba553fa7b2588ae69a0dc690/.github/workflows/build.yml",
    "retrieved_at": "2025-08-24T01:53:13.457299Z"
  },
  {
    "question": "Under what conditions will the workflow run, and which files trigger or prevent it?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "jiwei-dot/mmpose",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/jiwei-dot/mmpose/blob/94304d3038898a13198b7b5008352d22294a5b5e/.github/workflows/build.yml",
    "retrieved_at": "2025-08-24T01:53:14.270615Z"
  },
  {
    "question": "What specific conditions trigger the workflow to run, and which file changes are explicitly ignored?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "ReggieVW/mmpose",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/ReggieVW/mmpose/blob/566fac22b9ab2dc8231dd58a0b025ca08ebe2dee/.github/workflows/build.yml",
    "retrieved_at": "2025-08-24T01:53:15.149082Z"
  },
  {
    "question": "What specific files or directories are excluded from triggering the workflow on push and pull request events?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "ritaank/pose-estimation-ui",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/ritaank/pose-estimation-ui/blob/d9faea69a3db7802d5813826c328ab87964fd20c/.github/workflows/build.yml",
    "retrieved_at": "2025-08-24T01:53:15.970846Z"
  },
  {
    "question": "Under what conditions will the `prepare` job be skipped?",
    "answer": "name: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  prepare:\n    runs-on: ubuntu-latest\n    if: \"! contains(github.event.head_commit.message, '[skip ci]')\"\n    steps:\n      - run: echo \"${{ github.event.head_commit.message }}\"\n  release:\n    needs: prepare\n    name: Release\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          persist-credentials: false\n      - name: Setup Node.js\n        uses: actions/setup-node@v1\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: npm ci\n      - name: Release\n        env:\n          GIT_AUTHOR_EMAIL: ${{ secrets.GIT_AUTHOR_EMAIL }}\n          GIT_AUTHOR_NAME: ${{ secrets.GIT_AUTHOR_NAME }}\n          GIT_COMMITTER_EMAIL: ${{ secrets.GIT_COMMITTER_EMAIL }}\n          GIT_COMMITTER_NAME: ${{ secrets.GIT_COMMITTER_NAME }}\n          GITHUB_TOKEN: ${{ secrets.PA_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n",
    "source": "americanexpress/lumberjack",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/americanexpress/lumberjack/blob/55a7e4915248382cbf9739cd79fcf904930c40cb/.github/workflows/release.yml",
    "retrieved_at": "2025-08-24T01:53:16.514595Z"
  },
  {
    "question": "Under what conditions will the `prepare` job be skipped based on the commit message?",
    "answer": "name: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  prepare:\n    runs-on: ubuntu-latest\n    if: \"! contains(github.event.head_commit.message, '[skip ci]')\"\n    steps:\n      - run: echo \"${{ github.event.head_commit.message }}\"\n  release:\n    needs: prepare\n    name: Release\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          persist-credentials: false\n      - name: Setup Node.js\n        uses: actions/setup-node@v1\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: npm ci\n      - name: Release\n        env:\n          GIT_AUTHOR_EMAIL: ${{ secrets.GIT_AUTHOR_EMAIL }}\n          GIT_AUTHOR_NAME: ${{ secrets.GIT_AUTHOR_NAME }}\n          GIT_COMMITTER_EMAIL: ${{ secrets.GIT_COMMITTER_EMAIL }}\n          GIT_COMMITTER_NAME: ${{ secrets.GIT_COMMITTER_NAME }}\n          GITHUB_TOKEN: ${{ secrets.PA_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n",
    "source": "americanexpress/amex-jest-preset-react",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/americanexpress/amex-jest-preset-react/blob/59b72225131d35209a3ea8184fd10dc47863d880/.github/workflows/release.yml",
    "retrieved_at": "2025-08-24T01:53:17.059263Z"
  },
  {
    "question": "Under what conditions will the `prepare` job be skipped?",
    "answer": "name: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  prepare:\n    runs-on: ubuntu-latest\n    if: \"! contains(github.event.head_commit.message, '[skip ci]')\"\n    steps:\n      - run: echo \"${{ github.event.head_commit.message }}\"\n  release:\n    needs: prepare\n    name: Release\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          persist-credentials: false\n      - name: Setup Node.js\n        uses: actions/setup-node@v1\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: npm ci\n      - name: Release\n        env:\n          GIT_AUTHOR_EMAIL: ${{ secrets.GIT_AUTHOR_EMAIL }}\n          GIT_AUTHOR_NAME: ${{ secrets.GIT_AUTHOR_NAME }}\n          GIT_COMMITTER_EMAIL: ${{ secrets.GIT_COMMITTER_EMAIL }}\n          GIT_COMMITTER_NAME: ${{ secrets.GIT_COMMITTER_NAME }}\n          GITHUB_TOKEN: ${{ secrets.PA_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n",
    "source": "americanexpress/json-parse-context",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/americanexpress/json-parse-context/blob/ed84703711ad4ded75d00261a3f518c47d84daa6/.github/workflows/release.yml",
    "retrieved_at": "2025-08-24T01:53:17.678675Z"
  },
  {
    "question": "What is the purpose of building and deploying the application to CDN, FTP, and GitHub Pages upon the publishing of a release?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "HappyPot/democratic-management-",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/HappyPot/democratic-management-/blob/07aec203a3551c763908f8aecc819315150b5a6a/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-24T01:53:18.407889Z"
  },
  {
    "question": "What is the purpose of deploying the built application using three different methods (CDN, FTP, and GitHub Pages) when a release is published?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "singpay/d2dash",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/singpay/d2dash/blob/c2c6a8940e923421d24a0126b9dbd5c4993084fa/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-24T01:53:19.118407Z"
  },
  {
    "question": "What is the purpose of the different `VUE_APP_PUBLIC_PATH` values set in the `cdn`, `ftp`, and `gh-pages` jobs?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "swjgithub/d2-admin",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/swjgithub/d2-admin/blob/533616f64ea8dded70d6e92dbd96313f9efd2de1/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-25T01:48:06.999950Z"
  },
  {
    "question": "What is the purpose of deploying the built application to CDN, FTP, and GitHub Pages simultaneously upon a release?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "sftfjugg/d2-admin",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/sftfjugg/d2-admin/blob/eb0b002c6d9b431b5df5e9072c28693476287848/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-25T01:48:07.940286Z"
  },
  {
    "question": "What compliance checks are performed on the pull request by the `check_compliance` job?",
    "answer": "name: Compliance Checks\n\non: pull_request\n\njobs:\n  maintainer_check:\n    runs-on: ubuntu-latest\n    name: Check MAINTAINERS file\n    steps:\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n    - name: Run Maintainers Script\n      id: maintainer\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        python3 ./scripts/get_maintainer.py path CMakeLists.txt\n\n  check_compliance:\n    runs-on: ubuntu-latest\n    name: Run compliance checks on patch series (PR)\n    steps:\n    - name: Update PATH for west\n      run: |\n        echo \"$HOME/.local/bin\" >> $GITHUB_PATH\n\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n\n    - name: cache-pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-doc-pip\n\n    - name: Install python dependencies\n      run: |\n        pip3 install setuptools\n        pip3 install wheel\n        pip3 install python-magic junitparser==1.6.3 gitlint pylint pykwalify\n        pip3 install west\n\n    - name: west setup\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        git config --global user.email \"you@example.com\"\n        git config --global user.name \"Your Name\"\n        git remote -v\n        git rebase origin/${BASE_REF}\n        # debug\n        git log  --pretty=oneline | head -n 10\n        west init -l . || true\n        west update 2>&1 1> west.update.log || west update 2>&1 1> west.update2.log\n\n    - name: Run Compliance Tests\n      continue-on-error: true\n      id: compliance\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        export ZEPHYR_BASE=$PWD\n        # debug\n        ls -la\n        git log  --pretty=oneline | head -n 10\n        ./scripts/ci/check_compliance.py -m Devicetree -m Gitlint -m Identity -m Nits -m pylint -m checkpatch -m Kconfig -c origin/${BASE_REF}..\n\n    - name: upload-results\n      uses: actions/upload-artifact@master\n      continue-on-error: True\n      with:\n        name: compliance.xml\n        path: compliance.xml\n\n    - name: check-warns\n      run: |\n        if [[ ! -s \"compliance.xml\" ]]; then\n          exit 1;\n        fi\n\n        for file in Nits.txt checkpatch.txt Identity.txt Gitlint.txt pylint.txt Devicetree.txt Kconfig.txt; do\n          if [[ -s $file ]]; then\n            errors=$(cat $file)\n            errors=\"${errors//'%'/'%25'}\"\n            errors=\"${errors//$'\\n'/'%0A'}\"\n            errors=\"${errors//$'\\r'/'%0D'}\"\n            echo \"::error file=${file}::$errors\"\n            exit=1\n          fi\n        done\n\n        if [ \"${exit}\" == \"1\" ]; then\n          exit 1;\n        fi\n",
    "source": "jevinskie/js2232-zephyr",
    "path": ".github/workflows/compliance.yml",
    "url": "https://github.com/jevinskie/js2232-zephyr/blob/db40504c272734a519aa814a4f936455b527f5d1/.github/workflows/compliance.yml",
    "retrieved_at": "2025-08-25T01:48:08.679617Z"
  },
  {
    "question": "What compliance checks are performed on the pull request in the `check_compliance` job?",
    "answer": "name: Compliance Checks\n\non: pull_request\n\njobs:\n  maintainer_check:\n    runs-on: ubuntu-latest\n    name: Check MAINTAINERS file\n    steps:\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n    - name: Run Maintainers Script\n      id: maintainer\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        python3 ./scripts/get_maintainer.py path CMakeLists.txt\n\n  check_compliance:\n    runs-on: ubuntu-latest\n    name: Run compliance checks on patch series (PR)\n    steps:\n    - name: Update PATH for west\n      run: |\n        echo \"$HOME/.local/bin\" >> $GITHUB_PATH\n\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n\n    - name: cache-pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-doc-pip\n\n    - name: Install python dependencies\n      run: |\n        pip3 install setuptools\n        pip3 install wheel\n        pip3 install python-magic junitparser==1.6.3 gitlint pylint pykwalify\n        pip3 install west\n\n    - name: west setup\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        git config --global user.email \"you@example.com\"\n        git config --global user.name \"Your Name\"\n        git remote -v\n        git rebase origin/${BASE_REF}\n        # debug\n        git log  --pretty=oneline | head -n 10\n        west init -l . || true\n        west update 2>&1 1> west.update.log || west update 2>&1 1> west.update2.log\n\n    - name: Run Compliance Tests\n      continue-on-error: true\n      id: compliance\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        export ZEPHYR_BASE=$PWD\n        # debug\n        ls -la\n        git log  --pretty=oneline | head -n 10\n        ./scripts/ci/check_compliance.py -m Devicetree -m Gitlint -m Identity -m Nits -m pylint -m checkpatch -m Kconfig -c origin/${BASE_REF}..\n\n    - name: upload-results\n      uses: actions/upload-artifact@master\n      continue-on-error: True\n      with:\n        name: compliance.xml\n        path: compliance.xml\n\n    - name: check-warns\n      run: |\n        if [[ ! -s \"compliance.xml\" ]]; then\n          exit 1;\n        fi\n\n        for file in Nits.txt checkpatch.txt Identity.txt Gitlint.txt pylint.txt Devicetree.txt Kconfig.txt; do\n          if [[ -s $file ]]; then\n            errors=$(cat $file)\n            errors=\"${errors//'%'/'%25'}\"\n            errors=\"${errors//$'\\n'/'%0A'}\"\n            errors=\"${errors//$'\\r'/'%0D'}\"\n            echo \"::error file=${file}::$errors\"\n            exit=1\n          fi\n        done\n\n        if [ \"${exit}\" == \"1\" ]; then\n          exit 1;\n        fi\n",
    "source": "sstabellini/zephyr",
    "path": ".github/workflows/compliance.yml",
    "url": "https://github.com/sstabellini/zephyr/blob/7df99a12ab870b59e0e4af7b730d7b5d3fb43945/.github/workflows/compliance.yml",
    "retrieved_at": "2025-08-25T01:48:09.459096Z"
  },
  {
    "question": "How does the workflow handle compliance test failures and report errors within the pull request?",
    "answer": "name: Compliance Checks\n\non: pull_request\n\njobs:\n  maintainer_check:\n    runs-on: ubuntu-latest\n    name: Check MAINTAINERS file\n    steps:\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n    - name: Run Maintainers Script\n      id: maintainer\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        python3 ./scripts/get_maintainer.py path CMakeLists.txt\n\n  check_compliance:\n    runs-on: ubuntu-latest\n    name: Run compliance checks on patch series (PR)\n    steps:\n    - name: Update PATH for west\n      run: |\n        echo \"$HOME/.local/bin\" >> $GITHUB_PATH\n\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n\n    - name: cache-pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-doc-pip\n\n    - name: Install python dependencies\n      run: |\n        pip3 install setuptools\n        pip3 install wheel\n        pip3 install python-magic junitparser==1.6.3 gitlint pylint pykwalify\n        pip3 install west\n\n    - name: west setup\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        git config --global user.email \"you@example.com\"\n        git config --global user.name \"Your Name\"\n        git remote -v\n        git rebase origin/${BASE_REF}\n        # debug\n        git log  --pretty=oneline | head -n 10\n        west init -l . || true\n        west update 2>&1 1> west.update.log || west update 2>&1 1> west.update2.log\n\n    - name: Run Compliance Tests\n      continue-on-error: true\n      id: compliance\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        export ZEPHYR_BASE=$PWD\n        # debug\n        ls -la\n        git log  --pretty=oneline | head -n 10\n        ./scripts/ci/check_compliance.py -m Devicetree -m Gitlint -m Identity -m Nits -m pylint -m checkpatch -m Kconfig -c origin/${BASE_REF}..\n\n    - name: upload-results\n      uses: actions/upload-artifact@master\n      continue-on-error: True\n      with:\n        name: compliance.xml\n        path: compliance.xml\n\n    - name: check-warns\n      run: |\n        if [[ ! -s \"compliance.xml\" ]]; then\n          exit 1;\n        fi\n\n        for file in Nits.txt checkpatch.txt Identity.txt Gitlint.txt pylint.txt Devicetree.txt Kconfig.txt; do\n          if [[ -s $file ]]; then\n            errors=$(cat $file)\n            errors=\"${errors//'%'/'%25'}\"\n            errors=\"${errors//$'\\n'/'%0A'}\"\n            errors=\"${errors//$'\\r'/'%0D'}\"\n            echo \"::error file=${file}::$errors\"\n            exit=1\n          fi\n        done\n\n        if [ \"${exit}\" == \"1\" ]; then\n          exit 1;\n        fi\n",
    "source": "lorenzomanica/sma-zephyr-threads",
    "path": ".github/workflows/compliance.yml",
    "url": "https://github.com/lorenzomanica/sma-zephyr-threads/blob/a76c81a00aa6936743f2d6f73be3e19e32856d62/.github/workflows/compliance.yml",
    "retrieved_at": "2025-08-25T01:48:10.400826Z"
  },
  {
    "question": "Under what conditions will this workflow execute given the defined branch exclusions?",
    "answer": "on:\n  push:\n    branches:\n      - '!trying'\n      - '!trying.tmp'\n      - '!staging'\n      - '!staging.tmp'\n\nname: Coverage\n\nenv:\n  RUST_BACKTRACE: 1\n  RUSTFLAGS: \"-Ccodegen-units=1 -Clink-dead-code -Coverflow-checks=off\"\n\njobs:\n  coverage:\n    name: Coverage\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          profile: minimal\n          override: true\n      - name: Install LLVM (Linux)\n        run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz -L -o llvm.tar.xz\n          mkdir -p /opt/llvm-10\n          tar xf llvm.tar.xz --strip-components=1 -C /opt/llvm-10\n          echo '/opt/llvm-10/bin' >> $GITHUB_PATH\n          echo 'LLVM_SYS_100_PREFIX=/opt/llvm-10' >> $GITHUB_ENV\n      - name: Generate Coverage Report\n        run: |\n          cargo install cargo-tarpaulin\n          cargo tarpaulin --forward --release -t120 --out Xml --ignore-tests --workspace --exclude wasmer-wasi-experimental-io-devices --exclude wasmer-c-api -- --skip traps:: --skip spec::linking --test-threads=1\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v1\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          file: ./cobertura.xml\n",
    "source": "jamesblacklock/wasmer-singlepass-v2",
    "path": ".github/workflows/coverage.yaml",
    "url": "https://github.com/jamesblacklock/wasmer-singlepass-v2/blob/b6e4ab8a6f9e4192425938cc9e6e070fc485a899/.github/workflows/coverage.yaml",
    "retrieved_at": "2025-08-25T01:48:11.114741Z"
  },
  {
    "question": "Under what conditions will this workflow run based on branch push events?",
    "answer": "on:\n  push:\n    branches:\n      - '!trying'\n      - '!trying.tmp'\n      - '!staging'\n      - '!staging.tmp'\n\nname: Coverage\n\nenv:\n  RUST_BACKTRACE: 1\n  RUSTFLAGS: \"-Ccodegen-units=1 -Clink-dead-code -Coverflow-checks=off\"\n\njobs:\n  coverage:\n    name: Coverage\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          profile: minimal\n          override: true\n      - name: Install LLVM (Linux)\n        run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz -L -o llvm.tar.xz\n          mkdir -p /opt/llvm-10\n          tar xf llvm.tar.xz --strip-components=1 -C /opt/llvm-10\n          echo '/opt/llvm-10/bin' >> $GITHUB_PATH\n          echo 'LLVM_SYS_100_PREFIX=/opt/llvm-10' >> $GITHUB_ENV\n      - name: Generate Coverage Report\n        run: |\n          cargo install cargo-tarpaulin\n          cargo tarpaulin --forward --release -t120 --out Xml --ignore-tests --workspace --exclude wasmer-wasi-experimental-io-devices --exclude wasmer-c-api -- --skip traps:: --skip spec::linking --test-threads=1\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v1\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          file: ./cobertura.xml\n",
    "source": "GeorgKreuzmayr/wasmer",
    "path": ".github/workflows/coverage.yaml",
    "url": "https://github.com/GeorgKreuzmayr/wasmer/blob/14d8084c29f6c47e76a16fdee2083aa997d34482/.github/workflows/coverage.yaml",
    "retrieved_at": "2025-08-25T01:48:11.800715Z"
  },
  {
    "question": "Under what conditions will this workflow automatically merge Dependabot pull requests?",
    "answer": "name: Dependabot Auto Merge\n\non:\n  pull_request_target:\n\njobs:\n  auto-merge:\n    timeout-minutes: 5\n    runs-on: ubuntu-latest\n    if: github.actor == 'dependabot[bot]'\n    steps:\n      - uses: actions/checkout@v2\n      - uses: ahmadnassri/action-dependabot-auto-merge@v2\n        with:\n          target: minor\n          github-token: ${{ secrets.DEP_AUTOMERGE }}\n",
    "source": "hash3liZer/khatta2",
    "path": ".github/workflows/auto-merge.yml",
    "url": "https://github.com/hash3liZer/khatta2/blob/e813a28b30b484d6fa75566837209a44c96d3f11/.github/workflows/auto-merge.yml",
    "retrieved_at": "2025-08-25T01:48:12.496227Z"
  },
  {
    "question": "What type of benchmarks are run on the Haystack framework by this workflow?",
    "answer": "name: Haystack 1.x Benchmarks\n\non:\n  workflow_dispatch:\n\npermissions:\n  id-token: write\n  contents: read\n\nenv:\n  AWS_REGION: eu-central-1\n\njobs:\n  deploy-runner:\n    runs-on: ubuntu-latest\n    outputs:\n      cml_runner_id: ${{ steps.deploy.outputs.cml_runner_id }}\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: iterative/setup-cml@v3\n\n      - name: AWS authentication\n        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}\n\n      - name: Launch EC2 instance and deploy runner\n        id: deploy\n        env:\n          repo_token: ${{ secrets.HAYSTACK_BOT_TOKEN }}\n        run: |\n          OUTPUT=$(cml runner launch \\\n          --cloud aws \\\n          --cloud-region ${{ env.AWS_REGION }} \\\n          --cloud-type=p3.2xlarge \\\n          --cloud-hdd-size=64 \\\n          --labels=cml 2>&1 | tee /dev/fd/2)\n          # Extract 'id' from the log and set it as an environment variable\n          ID_VALUE=$(echo \"$OUTPUT\" | jq -r '.message? | fromjson? | select(.id != null) | .id // empty')\n          echo \"cml_runner_id=$ID_VALUE\" >> \"$GITHUB_OUTPUT\"\n\n  run-reader-benchmarks:\n    needs: deploy-runner\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/reader/*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-reader\n          path: test/benchmarks/out/\n\n  run-elasticsearch-benchmarks:\n    needs:\n      - deploy-runner\n      - run-reader-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      elasticsearch:\n        image: elasticsearch:7.17.6\n        env:\n          discovery.type: \"single-node\"\n        ports:\n          - 9201:9200\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,elasticsearch,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-elasticsearch-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-elasticsearch\n          path: test/benchmarks/out/\n\n  run-weaviate-benchmarks:\n    needs:\n      - deploy-runner\n      - run-elasticsearch-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      weaviate:\n        image: semitechnologies/weaviate:1.17.2\n        env:\n          AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: \"true\"\n          PERSISTENCE_DATA_PATH: \"/var/lib/weaviate\"\n        ports:\n          - 8080:8080\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,weaviate,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-weaviate-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-weaviate\n          path: test/benchmarks/out/\n\n  run-opensearch-benchmarks:\n    needs:\n      - deploy-runner\n      - run-weaviate-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      opensearch:\n        image: opensearchproject/opensearch:1.3.5\n        env:\n          discovery.type: \"single-node\"\n          OPENSEARCH_JAVA_OPTS: \"-Xms4096m -Xmx4096m\"\n        ports:\n          - 9200:9200\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,opensearch,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-opensearch-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-opensearch\n          path: test/benchmarks/out/\n\n  terminate-runner:\n    if: always()\n    needs:\n      - deploy-runner\n      - run-opensearch-benchmarks\n    runs-on: ubuntu-latest\n    steps:\n      - name: AWS authentication\n        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}\n\n      - name: Terminate EC2 instance\n        env:\n          CML_RUNNER_ID: ${{needs.deploy-runner.outputs.cml_runner_id}}\n        run: |\n          # Get the instance ID using its Name tag and terminate the instance\n          INSTANCE_ID=$(aws ec2 describe-instances --filters \"Name=tag:Name,Values=${{ env.CML_RUNNER_ID }}\" --query \"Reservations[*].Instances[*].[InstanceId]\" --output text)\n          aws ec2 terminate-instances --instance-ids \"$INSTANCE_ID\"\n",
    "source": "jkinda/haystack",
    "path": ".github/workflows/benchmarks.yml",
    "url": "https://github.com/jkinda/haystack/blob/25d333bed327cc40d536051ec3c4db26bbb1b147/.github/workflows/benchmarks.yml",
    "retrieved_at": "2025-08-25T01:48:13.279502Z"
  },
  {
    "question": "What triggers this workflow to run, and how frequently does it run by default?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "iGrubesic/expert-disco",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/iGrubesic/expert-disco/blob/d25f48756b2e76d29a08ac4e264c6c57bdac8e66/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-25T01:48:14.052351Z"
  },
  {
    "question": "Under what conditions (branches, events) does this workflow trigger?",
    "answer": "name: grpc Tests\n\n# START OF COMMON SECTION\non:\n  push:\n    branches: [ 'master', 'main', 'release/**' ]\n  pull_request:\n    branches: [ '*' ]\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n# END OF COMMON SECTION\n\njobs:\n  build_wolfssl:\n    name: Build wolfSSL\n    if: github.repository_owner == 'wolfssl'\n    # Just to keep it the same as the testing target\n    runs-on: ubuntu-22.04\n    # This should be a safe limit for the tests to run.\n    timeout-minutes: 10\n    steps:\n      - name: Build wolfSSL\n        uses: wolfSSL/actions-build-autotools-project@v1\n        with:\n          path: wolfssl\n          configure: --enable-all 'CPPFLAGS=-DWOLFSSL_RSA_KEY_CHECK -DHAVE_EX_DATA_CLEANUP_HOOKS'\n          install: true\n\n      - name: tar build-dir\n        run: tar -zcf build-dir.tgz build-dir\n\n      - name: Upload built lib\n        uses: actions/upload-artifact@v4\n        with:\n          name: wolf-install-grpc\n          path: build-dir.tgz\n          retention-days: 5\n\n  grpc_check:\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - ref: v1.60.0\n            tests: >-\n              bad_ssl_alpn_test bad_ssl_cert_test client_ssl_test\n              crl_ssl_transport_security_test server_ssl_test\n              ssl_transport_security_test ssl_transport_security_utils_test\n              test_core_security_ssl_credentials_test test_cpp_end2end_ssl_credentials_test\n              h2_ssl_cert_test h2_ssl_session_reuse_test\n    name: ${{ matrix.ref }}\n    if: github.repository_owner == 'wolfssl'\n    runs-on: ubuntu-22.04\n    # This should be a safe limit for the tests to run.\n    timeout-minutes: 30\n    needs: build_wolfssl\n    steps:\n      - name: Confirm IPv4 and IPv6 support\n        run: |\n          ip addr list lo | grep 'inet '\n          ip addr list lo | grep 'inet6 '\n\n      - name: Install prereqs\n        run:\n          sudo apt-get install build-essential autoconf libtool pkg-config cmake clang libc++-dev\n\n      - name: Download lib\n        uses: actions/download-artifact@v4\n        with:\n          name: wolf-install-grpc\n\n      - name: untar build-dir\n        run: tar -xf build-dir.tgz\n\n      - name: Checkout OSP\n        uses: actions/checkout@v4\n        with:\n          repository: wolfssl/osp\n          path: osp\n\n      - name: Checkout grpc\n        uses: actions/checkout@v4\n        with:\n          repository: grpc/grpc\n          path: grpc\n          ref: ${{ matrix.ref }}\n\n      - name: Build grpc\n        working-directory: ./grpc\n        run: |\n          patch -p1 < ../osp/grpc/grpc-${{ matrix.ref }}.patch\n          git submodule update --init\n          mkdir cmake/build\n          cd cmake/build\n          cmake -DgRPC_BUILD_TESTS=ON -DgRPC_SSL_PROVIDER=wolfssl \\\n            -DWOLFSSL_INSTALL_DIR=$GITHUB_WORKSPACE/build-dir ../..\n          make -j $(nproc) ${{ matrix.tests }}\n\n      - name: Run grpc tests\n        working-directory: ./grpc\n        run: |\n          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/build-dir/lib:$LD_LIBRARY_PATH\n          ./tools/run_tests/start_port_server.py\n          for t in ${{ matrix.tests }} ; do\n            ./cmake/build/$t\n          done\n",
    "source": "deepaksirone/wolfssl_bellerophon",
    "path": ".github/workflows/grpc.yml",
    "url": "https://github.com/deepaksirone/wolfssl_bellerophon/blob/50121495276c3c39dc5e525f07f6ebb5ae36cbc0/.github/workflows/grpc.yml",
    "retrieved_at": "2025-08-26T01:44:26.124139Z"
  },
  {
    "question": "What code quality and security checks are performed by this workflow on pull requests?",
    "answer": "name: Linters\n\non:\n  pull_request: { }\n\njobs:\n\n  linters:\n    name: linters\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.10'\n\n      - name: Install and Run Pre-commit\n        uses: pre-commit/action@v2.0.3\n\n      - name: Download Semgrep rules\n        run: git clone --depth 1 https://github.com/frappe/semgrep-rules.git frappe-semgrep-rules\n\n      - name: Download semgrep\n        run: pip install semgrep==0.97.0\n\n      - name: Run Semgrep rules\n        run: semgrep ci --config ./frappe-semgrep-rules/rules --config r/python.lang.correctness\n",
    "source": "CloudPlinthOne/cloudidp",
    "path": ".github/workflows/linters.yml",
    "url": "https://github.com/CloudPlinthOne/cloudidp/blob/dc4e40a2bf0dbb9fd3991528f6f96ad5676ced31/.github/workflows/linters.yml",
    "retrieved_at": "2025-08-26T01:44:26.935741Z"
  },
  {
    "question": "For every push or pull request to any branch, which Node.js versions are used to run the `npm test` command?",
    "answer": "name: 'CI'\n\non:\n  push:\n    branches: '**'\n  pull_request:\n    branches: '**'\n\npermissions:\n  contents: read\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        node-version: [12.x, 14.x, 16.x, 18.x]\n\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          persist-credentials: false\n      - name: Setup node\n        uses: actions/setup-node@v3\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: npm\n      - run: npm install\n      - run: npm test\n",
    "source": "X-oss-byte/Axios",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/X-oss-byte/Axios/blob/383089c74466b204992650a885b310e96872bfaf/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-26T01:44:27.749891Z"
  },
  {
    "question": "Under what conditions will the \"Post-processing\" job be executed?",
    "answer": "# Copyright (c) 2022, NVIDIA CORPORATION.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nname: Blossom-CI\non:\n  issue_comment:\n    types: [created]\n  workflow_dispatch:\n      inputs:\n          platform:\n            description: 'runs-on argument'\n            required: false\n          args:\n            description: 'argument'\n            required: false\n\npermissions:\n  actions: write\n  checks: write\n  contents: write\n  issues: write\n  pull-requests: write\n  repository-projects: write\n  statuses: write\n\njobs:\n  Authorization:\n    name: Authorization\n    runs-on: blossom\n    outputs:\n      args: ${{ env.args }}\n\n    # This job only runs for pull request comments\n    if: |\n      github.event.comment.body == '/build' &&\n      (\n        github.actor == 'wendell-hom' ||\n        github.actor == 'wyli' ||\n        github.actor == 'Nic-Ma' ||\n        github.actor == 'yiheng-wang-nv' ||\n        github.actor == 'KumoLiu'\n      )\n    steps:\n      - name: Check if comment is issued by authorized person\n        run: blossom-ci\n        env:\n          OPERATION: 'AUTH'\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          REPO_KEY_DATA: ${{ secrets.BLOSSOM_KEY }}\n\n  Vulnerability-scan:\n    name: Vulnerability scan\n    needs: [Authorization]\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n        with:\n          repository: ${{ fromJson(needs.Authorization.outputs.args).repo }}\n          ref: ${{ fromJson(needs.Authorization.outputs.args).ref }}\n          lfs: 'true'\n\n      # add blackduck properties https://synopsys.atlassian.net/wiki/spaces/INTDOCS/pages/631308372/Methods+for+Configuring+Analysis#Using-a-configuration-file\n      - name: Setup blackduck properties\n        run: |\n             echo detect.excluded.detector.types=PIP >> application.properties\n\n      - name: Run blossom action\n        uses: NVIDIA/blossom-action@main\n        env:\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          REPO_KEY_DATA: ${{ secrets.BLOSSOM_KEY }}\n        with:\n          args1: ${{ fromJson(needs.Authorization.outputs.args).args1 }}\n          args2: ${{ fromJson(needs.Authorization.outputs.args).args2 }}\n          args3: ${{ fromJson(needs.Authorization.outputs.args).args3 }}\n\n  Job-trigger:\n    name: Start ci job\n    needs: [Vulnerability-scan]\n    runs-on: blossom\n    steps:\n      - name: Start ci job\n        run: blossom-ci\n        env:\n          OPERATION: 'START-CI-JOB'\n          CI_SERVER: ${{ secrets.CI_SERVER }}\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n  Post-processing:\n    name: Post processing\n    runs-on: blossom\n    if : github.event_name == 'workflow_dispatch'\n    steps:\n      - name: Start post processing\n        run: blossom-ci\n        env:\n          OPERATION: 'POST-PROCESSING'\n          CI_SERVER: ${{ secrets.CI_SERVER }}\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "source": "Project-MONAI/model-zoo",
    "path": ".github/workflows/blossom-ci.yml",
    "url": "https://github.com/Project-MONAI/model-zoo/blob/e5c6da35788c54fcbadcef00884b87d2b10b14bc/.github/workflows/blossom-ci.yml",
    "retrieved_at": "2025-08-26T01:44:28.658131Z"
  },
  {
    "question": "What repository data is captured and stored by this workflow's `github-repo-stats` job?",
    "answer": "name: \"Repo Stats\"\n\non:\n  schedule:\n    # Run this once per day, towards the end of the day for keeping the most\n    # recent data point most meaningful (hours are interpreted in UTC).\n    - cron: \"0 23 * * *\"\n  workflow_dispatch:\n    # Allow for running this manually.\n\n    # Declare default permissions as read only.\npermissions: read-all\n\njobs:\n  snapshot:\n    name: github-repo-stats\n    runs-on: ubuntu-latest\n    steps:\n      - name: run-ghrs\n        # Use latest release.\n        uses: jgehrcke/github-repo-stats@306db38ad131cab2aa5f2cd3062bf6f8aa78c1aa # v1.4.2\n        with:\n          databranch: github-repo-stats\n          ghtoken: ${{ secrets.KEPTN_BOT_TOKEN }}\n",
    "source": "keptn/lifecycle-toolkit",
    "path": ".github/workflows/github-repo-stats.yml",
    "url": "https://github.com/keptn/lifecycle-toolkit/blob/40e195d78dd1886166e17d2e9c69b75a249fc384/.github/workflows/github-repo-stats.yml",
    "retrieved_at": "2025-08-26T01:44:29.562305Z"
  },
  {
    "question": "What i18n-related checks are performed by this workflow when triggered by a push or pull request to the main branch?",
    "answer": "name: Check i18n\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  RAILS_ENV: test\n\njobs:\n  check-i18n:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libicu-dev libidn11-dev libprotobuf-dev protobuf-compiler\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: '2.7'\n        bundler-cache: true\n    - name: Check locale file normalization\n      run: bundle exec i18n-tasks check-normalized\n    - name: Check for unused strings\n      run: bundle exec i18n-tasks unused -l en\n    - name: Check for wrong string interpolations\n      run: bundle exec i18n-tasks check-consistent-interpolations\n    - name: Check that all required locale files exist\n      run: bundle exec rake repo:check_locales_files\n",
    "source": "justjosias/truth-social",
    "path": ".github/workflows/check-i18n.yml",
    "url": "https://github.com/justjosias/truth-social/blob/70a5176b97247151340f83359142936dedb99e5b/.github/workflows/check-i18n.yml",
    "retrieved_at": "2025-08-26T01:44:31.879606Z"
  },
  {
    "question": "What i18n checks are performed by this workflow on push and pull requests to the main branch?",
    "answer": "name: Check i18n\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  RAILS_ENV: test\n\njobs:\n  check-i18n:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libicu-dev libidn11-dev libprotobuf-dev protobuf-compiler\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: '2.7'\n        bundler-cache: true\n    - name: Check locale file normalization\n      run: bundle exec i18n-tasks check-normalized\n    - name: Check for unused strings\n      run: bundle exec i18n-tasks unused -l en\n    - name: Check for wrong string interpolations\n      run: bundle exec i18n-tasks check-consistent-interpolations\n    - name: Check that all required locale files exist\n      run: bundle exec rake repo:check_locales_files\n",
    "source": "tugtug12121/e",
    "path": ".github/workflows/check-i18n.yml",
    "url": "https://github.com/tugtug12121/e/blob/70a5176b97247151340f83359142936dedb99e5b/.github/workflows/check-i18n.yml",
    "retrieved_at": "2025-08-26T01:44:32.670357Z"
  },
  {
    "question": "What specific i18n-related checks are performed by this workflow?",
    "answer": "name: Check i18n\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  RAILS_ENV: test\n\njobs:\n  check-i18n:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libicu-dev libidn11-dev libprotobuf-dev protobuf-compiler\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: '2.7'\n        bundler-cache: true\n    - name: Check locale file normalization\n      run: bundle exec i18n-tasks check-normalized\n    - name: Check for unused strings\n      run: bundle exec i18n-tasks unused -l en\n    - name: Check for wrong string interpolations\n      run: bundle exec i18n-tasks check-consistent-interpolations\n    - name: Check that all required locale files exist\n      run: bundle exec rake repo:check_locales_files\n",
    "source": "ftrbndd/truth",
    "path": ".github/workflows/check-i18n.yml",
    "url": "https://github.com/ftrbndd/truth/blob/6a6d9e5843e1da243f7b2bf901951b9f569931e3/.github/workflows/check-i18n.yml",
    "retrieved_at": "2025-08-26T01:44:33.440729Z"
  },
  {
    "question": "What triggers this workflow to run, and how frequently does it run on a schedule?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "wowdesarrollo/status-pages",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/wowdesarrollo/status-pages/blob/e2845769a7ed7ff7abcc43bcdfaaea23a6b79acf/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-26T01:44:34.446225Z"
  },
  {
    "question": "What triggers this workflow to run and check the status of the configured endpoints?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "bishrant/uptime-monitor",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/bishrant/uptime-monitor/blob/e86e3db03ccc7f5528957c7fc6ad3907765d075f/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-26T01:44:35.397866Z"
  }
]