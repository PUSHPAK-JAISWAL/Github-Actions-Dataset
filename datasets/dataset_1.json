[
  {
    "question": "How does the workflow use and access variables, context objects, outputs from previous jobs, and secrets?",
    "answer": "# Workflow to demonstrate variables and context objects\n\nname: Variables and Context\n\n# Controls when the action will run. Workflow runs when manually triggered using the UI or API\non:\n  workflow_dispatch:\n    # Inputs the workflow accepts.\n    inputs:\n      name:\n        # Friendly description to be shown in the UI instead of 'name'\n        description: 'Person to greet'\n        # Default value if no value is explicitly provided\n        default: 'World'\n        # Input has to be provided for the workflow to run\n        required: true\n\nenv:\n  VAR1: myworkflowvar1\n  VAR2: myworkflowvar2\n  VAR3: myworkflowvar3\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n\n  job1:\n    runs-on: ubuntu-latest \n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJSON(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      \n  #step/job output variables\n  job2:\n    runs-on: ubuntu-latest\n    \n    outputs:\n      output1: ${{ steps.step1.outputs.step1value }}\n      output2: ${{ steps.step2.outputs.step2value }}\n    \n    steps:\n      - name: Step 1\n        id: step1\n        # run: echo \"::set-output name=step1value::hello\"\n        run: echo \"step1value=hello\" >> $GITHUB_OUTPUT\n\n      - name: Step 2\n        id: step2\n        # run: echo \"::set-output name=step2value::world\"\n        run: echo \"step2value=world\" >> $GITHUB_OUTPUT\n  \n  job3:\n    runs-on: ubuntu-latest\n    needs: job2\n    steps:\n      - run: echo ${{needs.job2.outputs.output1}} ${{needs.job2.outputs.output2}}\n\n  # access/set env and secrets \n  job4:\n    runs-on: ubuntu-latest\n    env:\n      VAR2: myjobvar2\n      VAR3: myjobvar3\n      SECRET: ${{ secrets.mySecret }}\n    steps:\n\n      - run: |\n          echo $VAR1\n          echo ${{env.VAR1}}\n\n          echo \"\"\n\n          echo $VAR2\n\n          echo $VAR3\n\n          echo $SECRET\n        env: \n          VAR3: mystepvar3\n",
    "source": "nampereira/desofs-tp04",
    "path": ".github/workflows/2-context.yaml",
    "url": "https://github.com/nampereira/desofs-tp04/blob/a901b8f0160c924cc14bc6013b432e8bed448453/.github/workflows/2-context.yaml",
    "retrieved_at": "2025-08-14T12:24:45.141505Z"
  },
  {
    "question": "How does the workflow ensure cache consistency across different Dart SDK versions and packages?",
    "answer": "# Created with package:mono_repo v6.6.3\nname: Dart CI\non:\n  push:\n    branches:\n      - main\n      - master\n  pull_request:\n  schedule:\n    - cron: \"0 0 * * 0\"\ndefaults:\n  run:\n    shell: bash\nenv:\n  PUB_ENVIRONMENT: bot.github\npermissions: read-all\n\njobs:\n  job_001:\n    name: mono_repo self validate\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:stable\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: stable\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - name: mono_repo self validate\n        run: dart pub global activate mono_repo 6.6.3\n      - name: mono_repo self validate\n        run: dart pub global run mono_repo generate --validate\n  job_002:\n    name: \"smoke_test; PKGS: app, pkg/_pub_shared, pkg/api_builder, pkg/fake_gcloud, pkg/pub_package_reader, pkg/web_app, pkg/web_css; `dart format --output=none --set-exit-if-changed .`, `dart analyze --fatal-infos  .`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app-pkg/_pub_shared-pkg/api_builder-pkg/fake_gcloud-pkg/pub_package_reader-pkg/web_app-pkg/web_css;commands:format-analyze_0\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app-pkg/_pub_shared-pkg/api_builder-pkg/fake_gcloud-pkg/pub_package_reader-pkg/web_app-pkg/web_css\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - id: pkg__pub_shared_pub_get\n        name: pkg/_pub_shared; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n      - name: \"pkg/_pub_shared; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg__pub_shared_pub_get.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n      - name: \"pkg/_pub_shared; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg__pub_shared_pub_get.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n      - id: pkg_api_builder_pub_get\n        name: pkg/api_builder; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/api_builder\n      - name: \"pkg/api_builder; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_api_builder_pub_get.conclusion == 'success'\"\n        working-directory: pkg/api_builder\n      - name: \"pkg/api_builder; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_api_builder_pub_get.conclusion == 'success'\"\n        working-directory: pkg/api_builder\n      - id: pkg_fake_gcloud_pub_get\n        name: pkg/fake_gcloud; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n      - name: \"pkg/fake_gcloud; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_fake_gcloud_pub_get.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n      - name: \"pkg/fake_gcloud; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_fake_gcloud_pub_get.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n      - id: pkg_pub_package_reader_pub_get\n        name: pkg/pub_package_reader; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n      - name: \"pkg/pub_package_reader; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_pub_package_reader_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n      - name: \"pkg/pub_package_reader; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_pub_package_reader_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n      - id: pkg_web_app_pub_get\n        name: pkg/web_app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - name: \"pkg/web_app; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_web_app_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - name: \"pkg/web_app; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_web_app_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - id: pkg_web_css_pub_get\n        name: pkg/web_css; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_css\n      - name: \"pkg/web_css; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_web_css_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_css\n      - name: \"pkg/web_css; dart analyze --fatal-infos  .\"\n        run: dart analyze --fatal-infos  .\n        if: \"always() && steps.pkg_web_css_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_css\n  job_003:\n    name: \"smoke_test; PKG: pkg/indexed_blob; `dart format --output=none --set-exit-if-changed .`, `dart analyze --fatal-infos .`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/indexed_blob;commands:format-analyze_1\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/indexed_blob\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_indexed_blob_pub_get\n        name: pkg/indexed_blob; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n      - name: \"pkg/indexed_blob; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_indexed_blob_pub_get.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n      - name: \"pkg/indexed_blob; dart analyze --fatal-infos .\"\n        run: dart analyze --fatal-infos .\n        if: \"always() && steps.pkg_indexed_blob_pub_get.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n  job_004:\n    name: \"smoke_test; PKG: pkg/pub_integration; `dart format --output=none --set-exit-if-changed .`, `dart analyze --fatal-infos lib/`, `dart analyze --fatal-infos test/`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:format-analyze_2-analyze_3\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart analyze --fatal-infos lib/\"\n        run: dart analyze --fatal-infos lib/\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart analyze --fatal-infos test/\"\n        run: dart analyze --fatal-infos test/\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n  job_005:\n    name: \"smoke_test; PKG: pkg/pub_worker; `dart format --output=none --set-exit-if-changed .`, `dart analyze --fatal-infos --fatal-warnings bin/ lib/`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker;commands:format-analyze_4\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_worker_pub_get\n        name: pkg/pub_worker; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart format --output=none --set-exit-if-changed .\"\n        run: \"dart format --output=none --set-exit-if-changed .\"\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart analyze --fatal-infos --fatal-warnings bin/ lib/\"\n        run: dart analyze --fatal-infos --fatal-warnings bin/ lib/\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n  job_006:\n    name: \"build; PKG: pkg/web_app; `./build.sh`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_app;commands:command_1\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_web_app_pub_get\n        name: pkg/web_app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - name: pkg/web_app; ./build.sh\n        run: ./build.sh\n        if: \"always() && steps.pkg_web_app_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n  job_007:\n    name: \"build; PKG: pkg/web_css; `./build.sh`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_css;commands:command_1\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_css\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_web_css_pub_get\n        name: pkg/web_css; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_css\n      - name: pkg/web_css; ./build.sh\n        run: ./build.sh\n        if: \"always() && steps.pkg_web_css_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_css\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n  job_008:\n    name: \"build_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P build-only -j 1`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_00\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P build-only -j 1\"\n        run: dart test -P build-only -j 1\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n  job_009:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 0`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_01\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 0\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 0\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_010:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 1`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_02\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 1\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 1\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_011:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 2`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_03\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 2\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 2\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_012:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 3`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_04\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 3\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 3\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_013:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 4`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_05\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 4\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 4\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_014:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 5`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_06\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 5\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 5\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_015:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 6`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_07\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 6\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 6\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_016:\n    name: \"unit_test; PKG: app; `sudo apt-get update -yq && sudo apt-get install webp`, `dart test -P presubmit --total-shards 8 --shard-index 7`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app;commands:command_0-test_08\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: app_pub_get\n        name: app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; sudo apt-get update -yq && sudo apt-get install webp\"\n        run: \"sudo apt-get update -yq && sudo apt-get install webp\"\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n      - name: \"app; dart test -P presubmit --total-shards 8 --shard-index 7\"\n        run: dart test -P presubmit --total-shards 8 --shard-index 7\n        if: \"always() && steps.app_pub_get.conclusion == 'success'\"\n        working-directory: app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_017:\n    name: \"unit_test; PKG: pkg/_pub_shared; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/_pub_shared;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/_pub_shared\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg__pub_shared_pub_get\n        name: pkg/_pub_shared; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n      - name: \"pkg/_pub_shared; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg__pub_shared_pub_get.conclusion == 'success'\"\n        working-directory: pkg/_pub_shared\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_018:\n    name: \"unit_test; PKG: pkg/fake_gcloud; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/fake_gcloud;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/fake_gcloud\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_fake_gcloud_pub_get\n        name: pkg/fake_gcloud; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n      - name: \"pkg/fake_gcloud; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_fake_gcloud_pub_get.conclusion == 'success'\"\n        working-directory: pkg/fake_gcloud\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_019:\n    name: \"unit_test; PKG: pkg/indexed_blob; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/indexed_blob;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/indexed_blob\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_indexed_blob_pub_get\n        name: pkg/indexed_blob; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n      - name: \"pkg/indexed_blob; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_indexed_blob_pub_get.conclusion == 'success'\"\n        working-directory: pkg/indexed_blob\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_020:\n    name: \"unit_test; PKG: pkg/pub_package_reader; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_package_reader;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_package_reader\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_package_reader_pub_get\n        name: pkg/pub_package_reader; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n      - name: \"pkg/pub_package_reader; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_pub_package_reader_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_package_reader\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_021:\n    name: \"unit_test; PKG: pkg/web_app; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_app;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_app\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_web_app_pub_get\n        name: pkg/web_app; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_app\n      - name: \"pkg/web_app; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_web_app_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_app\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_022:\n    name: \"unit_test; PKG: pkg/web_css; `dart test --run-skipped`\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_css;commands:test_09\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/web_css\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_web_css_pub_get\n        name: pkg/web_css; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/web_css\n      - name: \"pkg/web_css; dart test --run-skipped\"\n        run: dart test --run-skipped\n        if: \"always() && steps.pkg_web_css_pub_get.conclusion == 'success'\"\n        working-directory: pkg/web_css\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_023:\n    name: \"unit_test; PKG: pkg/pub_integration; `dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~4p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:test_10\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~4p'`\"\n        run: \"dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~4p'`\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_024:\n    name: \"unit_test; PKG: pkg/pub_integration; `dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~4p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:test_11\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~4p'`\"\n        run: \"dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~4p'`\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_025:\n    name: \"unit_test; PKG: pkg/pub_integration; `dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~4p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:test_12\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~4p'`\"\n        run: \"dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~4p'`\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_026:\n    name: \"unit_test; PKG: pkg/pub_integration; `dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '3~4p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration;commands:test_13\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_integration\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_integration_pub_get\n        name: pkg/pub_integration; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n      - name: \"pkg/pub_integration; dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '3~4p'`\"\n        run: \"dart test -j1 --run-skipped `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '3~4p'`\"\n        if: \"always() && steps.pkg_pub_integration_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_integration\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_027:\n    name: \"unit_test; PKG: pkg/pub_worker; `dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~3p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker;commands:test_14\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_worker_pub_get\n        name: pkg/pub_worker; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~3p'`\"\n        run: \"dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '0~3p'`\"\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_028:\n    name: \"unit_test; PKG: pkg/pub_worker; `dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~3p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker;commands:test_15\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_worker_pub_get\n        name: pkg/pub_worker; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~3p'`\"\n        run: \"dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '1~3p'`\"\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n  job_029:\n    name: \"unit_test; PKG: pkg/pub_worker; `dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~3p'``\"\n    runs-on: ubuntu-latest\n    steps:\n      - name: Cache Pub hosted dependencies\n        uses: actions/cache@d4323d4df104b026a6aa633fdb11d772146be0bf\n        with:\n          path: \"~/.pub-cache/hosted\"\n          key: \"os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker;commands:test_16\"\n          restore-keys: |\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0;packages:pkg/pub_worker\n            os:ubuntu-latest;pub-cache-hosted;sdk:3.8.0\n            os:ubuntu-latest;pub-cache-hosted\n            os:ubuntu-latest\n      - name: Setup Dart SDK\n        uses: dart-lang/setup-dart@e51d8e571e22473a2ddebf0ef8a2123f0ab2c02c\n        with:\n          sdk: \"3.8.0\"\n      - id: checkout\n        name: Checkout repository\n        uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c\n      - id: pkg_pub_worker_pub_get\n        name: pkg/pub_worker; dart pub get\n        run: dart pub get\n        if: \"always() && steps.checkout.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n      - name: \"pkg/pub_worker; dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~3p'`\"\n        run: \"dart test --run-skipped --concurrency=1 `find test -name \\\"*_test\\\\\\\\.dart\\\" | sort | sed -n '2~3p'`\"\n        if: \"always() && steps.pkg_pub_worker_pub_get.conclusion == 'success'\"\n        working-directory: pkg/pub_worker\n    needs:\n      - job_001\n      - job_002\n      - job_003\n      - job_004\n      - job_005\n      - job_006\n      - job_007\n      - job_008\n",
    "source": "dart-lang/pub-dev",
    "path": ".github/workflows/dart.yml",
    "url": "https://github.com/dart-lang/pub-dev/blob/59af9a05fb70e699e7caa325d43f60649a3f431a/.github/workflows/dart.yml",
    "retrieved_at": "2025-08-14T12:24:50.273229Z"
  },
  {
    "question": "What i18n-related checks are performed by this workflow on push and pull requests to the main branch?",
    "answer": "name: Check i18n\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  RAILS_ENV: test\n\njobs:\n  check-i18n:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libicu-dev libidn11-dev libprotobuf-dev protobuf-compiler\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: '2.7'\n        bundler-cache: true\n    - name: Check locale file normalization\n      run: bundle exec i18n-tasks check-normalized\n    - name: Check for unused strings\n      run: bundle exec i18n-tasks unused -l en\n    - name: Check for wrong string interpolations\n      run: bundle exec i18n-tasks check-consistent-interpolations\n    - name: Check that all required locale files exist\n      run: bundle exec rake repo:check_locales_files\n",
    "source": "magicstone-dev/ecko",
    "path": ".github/workflows/check-i18n.yml",
    "url": "https://github.com/magicstone-dev/ecko/blob/a11982ed5341fb447ac3f65543488a014aa319d5/.github/workflows/check-i18n.yml",
    "retrieved_at": "2025-08-14T12:24:51.039771Z"
  },
  {
    "question": "What specific end-to-end tests are executed by this workflow?",
    "answer": "# Runs randomly generated E2E testnets nightly on the v0.37.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-37x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01', '02', '03', \"04\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"branch=`git branch --show-current`\" >> $GITHUB_OUTPUT\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner tests\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 5 -d networks/nightly/ -p\n\n      - name: Run ${{ matrix.p2p }} p2p testnets\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ needs.e2e-nightly-test.outputs.git-branch }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/e2e-nightly-37x.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/e2e-nightly-37x.yml",
    "retrieved_at": "2025-08-14T12:24:51.947805Z"
  },
  {
    "question": "Under what condition is the distribution artifact uploaded in this workflow?",
    "answer": "# This workflow will build a Java project with Gradle\n# For more information see: https://help.github.com/actions/language-and-framework-guides/building-and-testing-java-with-gradle\n\nname: Java CI\n\non: [ push, pull_request ]\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        env:\n          # see https://www.jetbrains.com/idea/download/previous.html\n          # and https://www.jetbrains.com/intellij-repository/snapshots/\n          - IDEA_VERSION: IC-2022.2.5\n          - IDEA_VERSION: IC-2022.3.1\n          - IDEA_VERSION: IC-2023.2.2\n          - IDEA_VERSION: IU-LATEST-EAP-SNAPSHOT\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up JDK 17\n        uses: actions/setup-java@v1\n        with:\n          java-version: 17\n      - name: Build with Gradle\n        run: |\n          ./gradlew -PideaVersion=${IDEA_VERSION} buildPlugin\n          echo \"DIST_FILE=$(find build/distributions/ -name 'jetbrains-plugin-st4-*.zip' -exec basename {} .zip \\;)\" >> $GITHUB_ENV\n        env: ${{ matrix.env }}\n      - name: Archive distribution artifact\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{env.DIST_FILE}}\n          path: build/distributions/jetbrains-plugin-st4-*.zip\n        if: matrix.env.IDEA_VERSION == 'IC-2022.2.5'\n",
    "source": "antlr/jetbrains-plugin-st4",
    "path": ".github/workflows/gradle.yml",
    "url": "https://github.com/antlr/jetbrains-plugin-st4/blob/1f8246e9d47f13c1e052ddd27232e8d4afaf67d9/.github/workflows/gradle.yml",
    "retrieved_at": "2025-08-14T12:24:53.834344Z"
  },
  {
    "question": "Does this workflow check if the code in a pull request branch is correctly formatted according to scalafmt, comparing it to the `origin/main` branch?",
    "answer": "name: Scalafmt\n\npermissions: read-all\n\non:\n  pull_request:\n    branches: ['**']\n\njobs:\n  build:\n    name: Code is formatted\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout current branch (full)\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n        with:\n          fetch-depth: 0\n          fetch-tags: true\n          persist-credentials: false\n\n      - name: Check project is formatted\n        uses: jrouly/scalafmt-native-action@v4\n        with:\n          arguments: '--list --mode diff-ref=origin/main'\n",
    "source": "apache/pekko-persistence-dynamodb",
    "path": ".github/workflows/format.yml",
    "url": "https://github.com/apache/pekko-persistence-dynamodb/blob/f88b6b9b20b4d5cec7285a5f049da220998f2dcb/.github/workflows/format.yml",
    "retrieved_at": "2025-08-14T12:24:55.489823Z"
  },
  {
    "question": "What different configurations are tested for KLEE on Linux, based on the matrix strategy?",
    "answer": "name: CI\n\non:\n  pull_request:\n    branches: master\n  push:\n    branches: master\n\n# Defaults for building KLEE\nenv:\n  BASE_IMAGE: ubuntu:bionic-20200807\n  REPOSITORY: klee\n  COVERAGE: 0\n  DISABLE_ASSERTIONS: 0\n  ENABLE_DOXYGEN: 0\n  ENABLE_OPTIMIZED: 1\n  ENABLE_DEBUG: 1\n  GTEST_VERSION: 1.7.0\n  KLEE_RUNTIME_BUILD: \"Debug+Asserts\"\n  LLVM_VERSION: 9\n  METASMT_VERSION: qf_abv\n  MINISAT_VERSION: \"master\"\n  REQUIRES_RTTI: 0\n  SANITIZER_BUILD:\n  SOLVERS: STP:Z3\n  STP_VERSION: 2.3.3\n  TCMALLOC_VERSION: 2.7\n  UCLIBC_VERSION: klee_uclibc_v1.2\n  USE_TCMALLOC: 1\n  USE_LIBCXX: 1\n  Z3_VERSION: 4.8.4\n\njobs:\n  Linux:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        name: [\n                \"LLVM 11\",\n                \"LLVM 10\",\n                \"LLVM 9\",\n                \"LLVM 8\",\n                \"LLVM 7\",\n                \"LLVM 6\",\n                \"LLVM 5\",\n                \"LLVM 4\",\n                \"LLVM 3.9\",\n                \"LLVM 3.8\",\n                \"ASan\",\n                \"UBSan\",\n                \"MSan\",\n                \"Z3 only\",\n                \"metaSMT STP\",\n                \"metaSMT Boolector\",\n                \"STP master\",\n                \"Latest klee-uclibc\",\n                \"Asserts enabled\",\n                \"No TCMalloc, optimised runtime\",\n            ]\n        include:\n          - name: \"LLVM 11\"\n            env:\n              LLVM_VERSION: 11\n          - name: \"LLVM 10\"\n            env:\n              LLVM_VERSION: 10\n          - name: \"LLVM 9\"\n            env:\n              LLVM_VERSION: 9\n          - name: \"LLVM 8\"\n            env:\n              LLVM_VERSION: 8\n          - name: \"LLVM 7\"\n            env:\n              LLVM_VERSION: 7\n          - name: \"LLVM 6\"\n            env:\n              LLVM_VERSION: 6\n          - name: \"LLVM 5\"\n            env:\n              LLVM_VERSION: 5\n          - name: \"LLVM 4\"\n            env:\n              LLVM_VERSION: 4\n          - name: \"LLVM 3.9\"\n            env:\n              LLVM_VERSION: 3.9\n          - name: \"LLVM 3.8\"\n            env:\n              LLVM_VERSION: 3.8\n              USE_LIBCXX: 0\n          # Sanitizer builds. Do unoptimized build otherwise the optimizer might remove problematic code\n          - name: \"ASan\"\n            env:\n              SANITIZER_BUILD: address\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n          - name: \"UBSan\"\n            env:\n              SANITIZER_BUILD: undefined\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n          - name: \"MSan\"\n            env:\n              SANITIZER_BUILD: memory\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n              SOLVERS: STP\n            # Test just using Z3 only\n          - name: \"Z3 only\"\n            env:\n              SOLVERS: Z3\n          # Test just using metaSMT\n          - name: \"metaSMT STP\"\n            env:\n              SOLVERS: metaSMT\n              METASMT_DEFAULT: STP\n              REQUIRES_RTTI: 1\n          - name: \"metaSMT Boolector\"\n            env:\n              SOLVERS: metaSMT\n              METASMT_DEFAULT: BTOR\n              REQUIRES_RTTI: 1\n          # Test we can build against STP master\n          - name: \"STP master\"\n            env:\n              SOLVERS: STP\n              STP_VERSION: master\n          # Check we can build latest klee-uclibc branch\n          - name: \"Latest klee-uclibc\"\n            env:\n              UCLIBC_VERSION: klee_0_9_29\n          # Check at least one build with Asserts disabled.\n          - name: \"Asserts enabled\"\n            env:\n              SOLVERS: STP\n              DISABLE_ASSERTIONS: 1\n          # Check without TCMALLOC and with an optimised runtime library\n          - name: \"No TCMalloc, optimised runtime\"\n            env:\n              USE_TCMALLOC: 0\n              KLEE_RUNTIME_BUILD: \"Release+Debug+Asserts\"\n    steps:\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        env: ${{ matrix.env }}\n        run: scripts/build/build.sh klee --docker --create-final-image\n      - name: Run tests\n        run: scripts/build/run-tests.sh --run-docker --debug\n\n  macOS:\n    runs-on: macos-latest\n    env:\n      BASE: /tmp\n      SOLVERS: STP\n      UCLIBC_VERSION: 0\n      USE_TCMALLOC: 0\n      USE_LIBCXX: 0\n    steps:\n      - name: Install newer version of Bash\n        run: brew install bash\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        run: scripts/build/build.sh klee --debug --install-system-deps\n      - name: Run tests\n        run: scripts/build/run-tests.sh /tmp/klee_build* --debug\n\n  Docker:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout KLEE Code\n        uses: actions/checkout@v2\n      - name: Build Docker image\n        run: docker build .\n\n  Coverage:\n    runs-on: ubuntu-latest\n    env:\n      ENABLE_OPTIMIZED: 0\n      COVERAGE: 1\n    steps:\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        run: scripts/build/build.sh klee --docker --create-final-image\n      - name: Run tests\n        run: scripts/build/run-tests.sh --coverage --upload-coverage --run-docker --debug\n",
    "source": "COMSYS/SymbolicLivenessAnalysis",
    "path": ".github/workflows/build.yaml",
    "url": "https://github.com/COMSYS/SymbolicLivenessAnalysis/blob/69af518cdb6cc4323f2a639e0a15a265552e9df5/.github/workflows/build.yaml",
    "retrieved_at": "2025-08-14T12:24:56.586759Z"
  },
  {
    "question": "Under what conditions does the workflow publish the created NuGet package to NuGet.org?",
    "answer": "name: .NET Core\non:\n  push:\n  workflow_dispatch:\n  release:\n    types: [published]\n\nenv:\n  DEFAULT_VERSION: \"3.1.0.4-alpha\"\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup .NET Core 2.2\n        uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: 2.2.x\n      - name: Setup .NET Core 3.1\n        uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: 3.1.x\n      - name: Setup .NET 6\n        uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: 6.0.x\n      - name: Setup .NET 8\n        uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: 8.0.x\n      - name: Build with dotnet\n        run: dotnet build --configuration Release /p:ContinuousIntegrationBuild=true\n      - name: Test\n        run: dotnet test src/PostalCodes.UnitTests/PostalCodes.UnitTests.csproj\n      - name: Extract Version from Release Tag\n        id: get_version\n        run: echo \"VERSION=$(echo '${{ github.event.release.tag_name || env.DEFAULT_VERSION }}' | sed -e 's/^v//')\" >> $GITHUB_ENV\n      - name: Create nuget package\n        run: dotnet pack . -p:PackageVersion=${{ env.VERSION }} -o out --no-build\n      - name: Install dotnet-validate\n        run: dotnet tool install --global dotnet-validate --version 0.0.1-preview.304\n\n      - name: Validate NuGet package\n        run: dotnet-validate package local out/*.nupkg\n\n      - name: Publish\n        if: github.event_name == 'release'\n        run: |\n          dotnet nuget push out/PostalCodes.${{ env.VERSION }}.nupkg -k ${{ secrets.NUGET_API_KEY }} -s https://api.nuget.org/v3/index.json\n",
    "source": "Cimpress-MCP/PostalCodes.Net",
    "path": ".github/workflows/dotnetcore.yml",
    "url": "https://github.com/Cimpress-MCP/PostalCodes.Net/blob/1dea354096b5858e6584b8d233db1a88456a02f3/.github/workflows/dotnetcore.yml",
    "retrieved_at": "2025-08-14T12:24:57.771476Z"
  },
  {
    "question": "What specific tests are executed by the `npm test` command in this workflow?",
    "answer": "name: Tests\n\non: [push, pull_request]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@master\n      - name: Use Node.js 12.x\n        uses: actions/setup-node@v1\n        with:\n          node-version: 12.x\n      - name: npm install and test\n        run: |\n          npm install\n          npm test\n",
    "source": "nervetattoo/banner-card",
    "path": ".github/workflows/nodejs.yml",
    "url": "https://github.com/nervetattoo/banner-card/blob/554c77c4fca5019896d336492b81b1fc56f9a281/.github/workflows/nodejs.yml",
    "retrieved_at": "2025-08-14T12:24:58.621683Z"
  },
  {
    "question": "How does the `metadata2gha` command in the `setup_matrix` job determine the test matrix configurations?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "scibian/puppet-module-puppet-archive",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/scibian/puppet-module-puppet-archive/blob/56ac175624cd57ebd4245348165ea97895f5fd1c/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-14T12:24:59.444331Z"
  },
  {
    "question": "What specific files or directories are excluded from triggering the workflow on push and pull requests?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "DeepLink-org/ParrotsDL-mmpose",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/DeepLink-org/ParrotsDL-mmpose/blob/5734804f05fdcc8849bfee34d79f8bd47b9a9fdb/.github/workflows/build.yml",
    "retrieved_at": "2025-08-14T12:50:06.774488Z"
  },
  {
    "question": "Under what conditions does the \"prepare\" job not run, and how does that impact the \"release\" job?",
    "answer": "name: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  prepare:\n    runs-on: ubuntu-latest\n    if: \"! contains(github.event.head_commit.message, '[skip ci]')\"\n    steps:\n      - run: echo \"${{ github.event.head_commit.message }}\"\n  release:\n    needs: prepare\n    name: Release\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          persist-credentials: false\n      - name: Setup Node.js\n        uses: actions/setup-node@v1\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: npm ci\n      - name: Release\n        env:\n          GIT_AUTHOR_EMAIL: ${{ secrets.GIT_AUTHOR_EMAIL }}\n          GIT_AUTHOR_NAME: ${{ secrets.GIT_AUTHOR_NAME }}\n          GIT_COMMITTER_EMAIL: ${{ secrets.GIT_COMMITTER_EMAIL }}\n          GIT_COMMITTER_NAME: ${{ secrets.GIT_COMMITTER_NAME }}\n          GITHUB_TOKEN: ${{ secrets.PA_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n",
    "source": "americanexpress/jest-json-schema",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/americanexpress/jest-json-schema/blob/5bc73a66ea7e1b6164923c69d12103a36dfdd21c/.github/workflows/release.yml",
    "retrieved_at": "2025-08-14T12:50:07.442808Z"
  },
  {
    "question": "What actions are performed to lint the code and report the results when a pull request targets the main branch?",
    "answer": "name: Lint\non:\n  pull_request_target:\n    branches:\n      - main\njobs:\n  lint:\n    strategy:\n      matrix:\n        node: [\"14.x\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v2\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n          fetch-depth: 2\n\n      - name: Use Node.js 14.x\n        uses: actions/setup-node@v2\n        with:\n          node-version: ${{ matrix.node }}\n          # cache: \"yarn\"\n          # cache-dependency-path: yarn.lock\n\n      - name: Install deps\n        if: steps.yarn-cache.outputs.cache-hit != 'true'\n        run: yarn\n\n      - name: Lint\n        run: yarn lint:report\n        continue-on-error: true\n\n      - name: Merge lint reports\n        run: jq -s '[.[]]|flatten' lint-results/*.json &> lint-results/eslint_report.json\n\n      - name: Annotate Code Linting Results\n        uses: ataylorme/eslint-annotate-action@1.2.0\n        with:\n          repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          report-json: \"lint-results/eslint_report.json\"\n\n      - name: Upload ESLint report\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: lint-results\n          path: lint-results\n",
    "source": "meruhealth/calendso",
    "path": ".github/workflows/lint.yml",
    "url": "https://github.com/meruhealth/calendso/blob/d88af28e441320b9d1e32babb2bd9d11bdb0834d/.github/workflows/lint.yml",
    "retrieved_at": "2025-08-14T12:50:08.214848Z"
  },
  {
    "question": "What triggers this workflow, and what distinguishes those triggers?",
    "answer": "name: 'build and deploy Speckle functions'\non:\n  workflow_dispatch:\n  push:\n    tags:\n      - '*'\n\njobs:\n  publish-automate-function-version: # make sure the action works on a clean machine without building\n    env:\n      FUNCTION_SCHEMA_FILE_NAME: functionSchema.json\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.1.1\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install and configure Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: 1.3.2\n          virtualenvs-create: false\n          virtualenvs-in-project: false\n          installer-parallel: true\n      - name: Restore dependencies\n        run: poetry install --no-root\n      - name: Extract functionInputSchema\n        id: extract_schema\n        run: |\n          python main.py generate_schema ${HOME}/${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n      - name: Speckle Automate Function - Build and Publish\n        uses: specklesystems/speckle-automate-github-composite-action@0.8.0\n        with:\n          speckle_automate_url: ${{ env.SPECKLE_AUTOMATE_URL || 'https://automate.speckle.dev' }} \n          speckle_token: ${{ secrets.SPECKLE_FUNCTION_TOKEN }}\n          speckle_function_id: ${{ secrets.SPECKLE_FUNCTION_ID }}\n          speckle_function_input_schema_file_path: ${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n          speckle_function_command: 'python -u main.py run'\n",
    "source": "specklesystems/Mott-mac-demo",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/specklesystems/Mott-mac-demo/blob/d08b92f1948bf28ab8a39f2c3b2c75e56bf297fc/.github/workflows/main.yml",
    "retrieved_at": "2025-08-14T12:50:08.879430Z"
  },
  {
    "question": "What happens when a push event triggers this workflow with a tag?",
    "answer": "---\nname: release\n\non:\n  push:\n    tags: [\"*\"]\n\npermissions:\n  id-token: write\n  contents: write\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out code\n        uses: actions/checkout@v2\n\n      - name: version\n        run: echo \"version=$(/usr/bin/basename ${{ github.ref }})\" >> $GITHUB_OUTPUT\n        id: version\n\n      - name: release\n        uses: actions/create-release@v1\n        id: create_release\n        with:\n          draft: false\n          prerelease: false\n          release_name: ${{ steps.version.outputs.version }}\n          tag_name: ${{ github.ref }}\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n\n      - name: Package\n        uses: a7ul/tar-action@v1.1.2\n        with:\n          files: .\n          command: c\n          outPath: ${{ steps.version.outputs.version }}.tar.gz\n\n      - name: Sign release with Sigstore\n        continue-on-error: true\n        uses: sigstore/gh-action-sigstore-python@v0.0.9\n        with:\n          inputs: ${{ steps.version.outputs.version }}.tar.gz\n          release-signing-artifacts: true\n          upload-signing-artifacts: true\n\n      - name: upload signed asset\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz\n          asset_content_type: application/gzip\n\n      - name: upload sigstore certificate\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz.crt\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz.crt\n          asset_content_type: application/x-x509-ca-cert\n\n      - name: upload sigstore signature\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz.sig\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz.sig\n          asset_content_type: application/octet-stream\n\n      - name: Build and Deploy Collection\n        uses: 0x022b/galaxy-role-import-action@1.0.0\n        with:\n          galaxy_api_key: \"${{ secrets.ANSIBLE_GALAXY_TOKEN }}\"\n",
    "source": "ipr-cnrs/glpi-agent",
    "path": ".github/workflows/release.yaml",
    "url": "https://github.com/ipr-cnrs/glpi-agent/blob/55a7d8b68c8dee8b30b2c2f65fbc9d66ad7eabba/.github/workflows/release.yaml",
    "retrieved_at": "2025-08-14T12:50:09.700604Z"
  },
  {
    "question": "What automated code formatting and fixing tasks are executed by the `autofix` job in this workflow?",
    "answer": "name: autofix.ci # needed to securely identify the workflow\n\non:\n  pull_request:\n  push:\n    branches: [\"main\"]\n\npermissions:\n  contents: read\n\njobs:\n  autofix:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: pnpm/action-setup@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: 20\n          cache: \"pnpm\"\n      - run: pnpm install\n      - run: pnpm automd\n      - run: pnpm lint:fix\n      - uses: autofix-ci/action@635ffb0c9798bd160680f18fd73371e355b85f27\n        with:\n          commit-message: \"chore: apply automated updates\"\n",
    "source": "unjs/runtime-compat",
    "path": ".github/workflows/autofix.yml",
    "url": "https://github.com/unjs/runtime-compat/blob/9a2246b51cb25292975a24619394bc2c1754a445/.github/workflows/autofix.yml",
    "retrieved_at": "2025-08-14T12:50:10.490003Z"
  },
  {
    "question": "What specific compliance checks are performed by the `check_compliance` job, and how are the results reported?",
    "answer": "name: Compliance Checks\n\non: pull_request\n\njobs:\n  maintainer_check:\n    runs-on: ubuntu-latest\n    name: Check MAINTAINERS file\n    steps:\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n    - name: Run Maintainers Script\n      id: maintainer\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        python3 ./scripts/get_maintainer.py path CMakeLists.txt\n\n  check_compliance:\n    runs-on: ubuntu-latest\n    name: Run compliance checks on patch series (PR)\n    steps:\n    - name: Update PATH for west\n      run: |\n        echo \"$HOME/.local/bin\" >> $GITHUB_PATH\n\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n\n    - name: cache-pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-doc-pip\n\n    - name: Install python dependencies\n      run: |\n        pip3 install setuptools\n        pip3 install wheel\n        pip3 install python-magic junitparser==1.6.3 gitlint pylint pykwalify\n        pip3 install west\n\n    - name: west setup\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        git config --global user.email \"you@example.com\"\n        git config --global user.name \"Your Name\"\n        git remote -v\n        git rebase origin/${BASE_REF}\n        # debug\n        git log  --pretty=oneline | head -n 10\n        west init -l . || true\n        west update 2>&1 1> west.update.log || west update 2>&1 1> west.update2.log\n\n    - name: Run Compliance Tests\n      continue-on-error: true\n      id: compliance\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        export ZEPHYR_BASE=$PWD\n        # debug\n        ls -la\n        git log  --pretty=oneline | head -n 10\n        ./scripts/ci/check_compliance.py -m Devicetree -m Gitlint -m Identity -m Nits -m pylint -m checkpatch -m Kconfig -c origin/${BASE_REF}..\n\n    - name: upload-results\n      uses: actions/upload-artifact@master\n      continue-on-error: True\n      with:\n        name: compliance.xml\n        path: compliance.xml\n\n    - name: check-warns\n      run: |\n        if [[ ! -s \"compliance.xml\" ]]; then\n          exit 1;\n        fi\n\n        for file in Nits.txt checkpatch.txt Identity.txt Gitlint.txt pylint.txt Devicetree.txt Kconfig.txt; do\n          if [[ -s $file ]]; then\n            errors=$(cat $file)\n            errors=\"${errors//'%'/'%25'}\"\n            errors=\"${errors//$'\\n'/'%0A'}\"\n            errors=\"${errors//$'\\r'/'%0D'}\"\n            echo \"::error file=${file}::$errors\"\n            exit=1\n          fi\n        done\n\n        if [ \"${exit}\" == \"1\" ]; then\n          exit 1;\n        fi\n",
    "source": "sofarocean/zephyr",
    "path": ".github/workflows/compliance.yml",
    "url": "https://github.com/sofarocean/zephyr/blob/429954f467b388b839c8bfedc51f39a5fb3609d1/.github/workflows/compliance.yml",
    "retrieved_at": "2025-08-14T12:50:11.384065Z"
  },
  {
    "question": "Under what conditions will the \"bulk-import\" job actually run, given the `if` condition?",
    "answer": "name: \"bulk quest import\"\non:\n  schedule:\n    - cron: '0 10 * * *' # UTC time, that's 5:00 am EST, 2:00 am PST.\n  workflow_dispatch:\n    inputs:\n      reason:\n        description: \"The reason for running the bulk import workflow\"\n        required: true\n        default: \"Initial import into Quest (Azure DevOps)\"\n\njobs:\n  bulk-import:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n    if: ${{ github.repository_owner == 'dotnet' }}\n\n    steps:\n      - name: \"Print manual bulk import run reason\"\n        if: ${{ github.event_name == 'workflow_dispatch' }}\n        run: |\n          echo \"Reason: ${{ github.event.inputs.reason }}\"\n\n      - name: bulk-sequester\n        id: bulk-sequester\n        uses: dotnet/docs-tools/actions/sequester@main\n        env:\n          ImportOptions__ApiKeys__GitHubToken: ${{ secrets.GITHUB_TOKEN }}\n          ImportOptions__ApiKeys__OSPOKey: ${{ secrets.OSPO_KEY }}\n          ImportOptions__ApiKeys__QuestKey: ${{ secrets.QUEST_KEY }}\n        with:\n          org: ${{ github.repository_owner }}\n          repo: ${{ github.repository }}\n          issue: '-1'\n",
    "source": "void0620/ASP.NET-Core",
    "path": ".github/workflows/quest-bulk.yml",
    "url": "https://github.com/void0620/ASP.NET-Core/blob/d56a806f9d8c99ba375db558c49a38faf7aa09ec/.github/workflows/quest-bulk.yml",
    "retrieved_at": "2025-08-14T12:50:12.324009Z"
  },
  {
    "question": "For the Windows MSVC builds, what environment variables are being set using the Visual Studio batch script?",
    "answer": "name: range-v3 CI\n\n# Trigger on pushes to all branches and for all pull-requests\non: [push, pull_request]\n\nenv:\n  CMAKE_VERSION: 3.16.2\n  NINJA_VERSION: 1.9.0\n\njobs:\n  build:\n    name: ${{ matrix.config.name }}\n    runs-on: ${{ matrix.config.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        config:\n        # GCC-6\n        - {\n            name: \"Linux GCC 6 Debug (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 6 Release (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 6 Debug (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 6 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 6 Release (C++17, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-6\", cxx: \"g++-6\",\n            cxx_standard: 17,\n          }\n\n        # GCC-7\n        - {\n            name: \"Linux GCC 7 Debug (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 7 Release (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 7 Debug (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 7 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 7 Release (C++17, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-7\", cxx: \"g++-7\",\n            cxx_standard: 17,\n          }\n\n        # GCC-8\n        - {\n            name: \"Linux GCC 8 Debug (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 8 Release (C++14)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 14,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 8 Debug (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 8 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 8 Release (C++17, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-8\", cxx: \"g++-8\",\n            cxx_standard: 17,\n          }\n\n        # GCC-9\n        - {\n            name: \"Linux GCC 9 Debug (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 9 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 17,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 9 Debug (C++20)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 20,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 9 Release (C++20)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 20,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux GCC 9 Release (C++20, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-9\", cxx: \"g++-9\",\n            cxx_standard: 20,\n          }\n\n        # GCC-10\n        - {\n            name: \"Linux GCC 10 Debug (C++20, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"gcc-10\", cxx: \"g++-10\",\n            cxx_standard: 20\n          }\n        - {\n            name: \"Linux GCC 10 Release (C++20, Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"gcc-10\", cxx: \"g++-10\",\n            cxx_standard: 20\n          }\n\n        # Clang-5.0\n        - {\n            name: \"Linux Clang 5.0 Debug (C++14 / libc++ / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"clang-5.0\", cxx: \"clang++-5.0\",\n            cxx_standard: 14,\n            cxx_asan: true,\n            libcxx: true\n          }\n        - {\n            name: \"Linux Clang 5.0 Debug (C++17 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"clang-5.0\", cxx: \"clang++-5.0\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n\n        # Clang-6.0\n        - {\n            name: \"Linux Clang 6.0 Debug (C++14 / libc++ / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"clang-6.0\", cxx: \"clang++-6.0\",\n            cxx_standard: 14,\n            cxx_asan: true,\n            libcxx: true\n          }\n        - {\n            name: \"Linux Clang 6.0 Debug (C++17 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-18.04,\n            build_type: Debug,\n            cc: \"clang-6.0\", cxx: \"clang++-6.0\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n\n        # Clang-8\n        - {\n            name: \"Linux Clang 8 Debug (C++14 / libc++ / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"clang-8\", cxx: \"clang++-8\",\n            cxx_standard: 14,\n            cxx_asan: true,\n            libcxx: true\n          }\n        - {\n            name: \"Linux Clang 8 Debug (C++17 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"clang-8\", cxx: \"clang++-8\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n\n        # Clang-9\n        - {\n            name: \"Linux Clang 9 Debug (C++17 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"clang-9\", cxx: \"clang++-9\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n        - {\n            name: \"Linux Clang 9 Release (C++17)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"clang-9\", cxx: \"clang++-9\",\n            cxx_standard: 17,\n          }\n\n        # Clang-10\n        - {\n            name: \"Linux Clang 10 Debug (C++20 / ASAN)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: Debug,\n            cc: \"clang-10\", cxx: \"clang++-10\",\n            cxx_standard: 20,\n            cxx_asan: true,\n            cxx_concepts: false\n          }\n        - {\n            name: \"Linux Clang 10 Release (C++20 / Concepts)\", artifact: \"Linux.tar.xz\",\n            os: ubuntu-latest,\n            build_type: RelWithDebInfo,\n            cc: \"clang-10\", cxx: \"clang++-10\",\n            cxx_standard: 20,\n          }\n\n        # AppleClang\n        - {\n            name: \"macOS Clang Debug (C++17)\", artifact: \"macOS.tar.xz\",\n            os: macos-latest,\n            build_type: Debug,\n            cc: \"clang\", cxx: \"clang++\",\n            cxx_standard: 17,\n            cxx_asan: true,\n          }\n        - {\n            name: \"macOS Clang Release (C++17)\", artifact: \"macOS.tar.xz\",\n            os: macos-latest,\n            build_type: RelWithDebInfo,\n            cc: \"clang\", cxx: \"clang++\",\n            cxx_standard: 17,\n          }\n        - {\n            name: \"macOS Clang Debug (C++20 / ASAN)\", artifact: \"macOS.tar.xz\",\n            os: macos-latest,\n            build_type: Debug,\n            cc: \"clang\", cxx: \"clang++\",\n            cxx_standard: 20,\n            cxx_asan: true,\n          }\n        - {\n            name: \"macOS Clang Release (C++20)\", artifact: \"macOS.tar.xz\",\n            os: macos-latest,\n            build_type: RelWithDebInfo,\n            cc: \"clang\", cxx: \"clang++\",\n            cxx_standard: 20,\n          }\n\n        # MSVC 2022\n        - {\n            name: \"Windows MSVC 2022 Debug (C++17)\", artifact: \"Windows-MSVC.tar.xz\",\n            os: windows-latest,\n            build_type: Debug,\n            cc: \"cl\", cxx: \"cl\",\n            environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n            cxx_standard: 17,\n          }\n        - {\n            name: \"Windows MSVC 2022 Release (C++17)\", artifact: \"Windows-MSVC.tar.xz\",\n            os: windows-latest,\n            build_type: RelWithDebInfo,\n            cc: \"cl\", cxx: \"cl\",\n            environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n            cxx_standard: 17,\n          }\n        - {\n            name: \"Windows MSVC 2022 Debug (C++20)\", artifact: \"Windows-MSVC.tar.xz\",\n            os: windows-latest,\n            build_type: Debug,\n            cc: \"cl\", cxx: \"cl\",\n            environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n            cxx_standard: 20,\n          }\n        - {\n            name: \"Windows MSVC 2022 Release (C++20)\", artifact: \"Windows-MSVC.tar.xz\",\n            os: windows-latest,\n            build_type: RelWithDebInfo,\n            cc: \"cl\", cxx: \"cl\",\n            environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n            cxx_standard: 20,\n          }\n\n    steps:\n    - uses: actions/checkout@v1\n\n    - name: Download Ninja and CMake\n      id: cmake_and_ninja\n      shell: cmake -P {0}\n      run: |\n        set(cmake_version $ENV{CMAKE_VERSION})\n        set(ninja_version $ENV{NINJA_VERSION})\n\n        message(STATUS \"Using host CMake version: ${CMAKE_VERSION}\")\n\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n          set(ninja_suffix \"win.zip\")\n          set(cmake_suffix \"win64-x64.zip\")\n          set(cmake_dir \"cmake-${cmake_version}-win64-x64/bin\")\n        elseif (\"${{ runner.os }}\" STREQUAL \"Linux\")\n          set(ninja_suffix \"linux.zip\")\n          set(cmake_suffix \"Linux-x86_64.tar.gz\")\n          set(cmake_dir \"cmake-${cmake_version}-Linux-x86_64/bin\")\n        elseif (\"${{ runner.os }}\" STREQUAL \"macOS\")\n          set(ninja_suffix \"mac.zip\")\n          set(cmake_suffix \"Darwin-x86_64.tar.gz\")\n          set(cmake_dir \"cmake-${cmake_version}-Darwin-x86_64/CMake.app/Contents/bin\")\n        endif()\n\n        set(ninja_url \"https://github.com/ninja-build/ninja/releases/download/v${ninja_version}/ninja-${ninja_suffix}\")\n        file(DOWNLOAD \"${ninja_url}\" ./ninja.zip SHOW_PROGRESS)\n        execute_process(COMMAND ${CMAKE_COMMAND} -E tar xvf ./ninja.zip)\n\n        set(cmake_url \"https://github.com/Kitware/CMake/releases/download/v${cmake_version}/cmake-${cmake_version}-${cmake_suffix}\")\n        file(DOWNLOAD \"${cmake_url}\" ./cmake.zip SHOW_PROGRESS)\n        execute_process(COMMAND ${CMAKE_COMMAND} -E tar xvf ./cmake.zip)\n\n        # Save the path for other steps\n        file(TO_CMAKE_PATH \"$ENV{GITHUB_WORKSPACE}/${cmake_dir}\" cmake_dir)\n        message(\"::set-output name=cmake_dir::${cmake_dir}\")\n\n        if (NOT \"${{ runner.os }}\" STREQUAL \"Windows\")\n          execute_process(\n            COMMAND chmod +x ninja\n            COMMAND chmod +x ${cmake_dir}/cmake\n          )\n        endif()\n\n    - name: Install GCC or Clang\n      id: install_gcc_clang\n      if: startsWith(matrix.config.os, 'ubuntu')\n      shell: bash\n      working-directory: ${{ env.HOME }}\n      run: |\n        sudo apt-get install -y ${{matrix.config.cc}} ${{matrix.config.cxx}}\n\n    - name: Install libc++\n      id: install_libcxx\n      if: matrix.config.libcxx\n      shell: bash\n      working-directory: ${{ env.HOME }}\n      env:\n        CC: ${{ matrix.config.cc }}\n        CXX: ${{ matrix.config.cxx }}\n      run: |\n        $GITHUB_WORKSPACE/install_libcxx.sh\n\n    - name: Configure\n      shell: cmake -P {0}\n      run: |\n        set(ENV{CC} ${{ matrix.config.cc }})\n        set(ENV{CXX} ${{ matrix.config.cxx }})\n\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\" AND NOT \"x${{ matrix.config.environment_script }}\" STREQUAL \"x\")\n          execute_process(\n            COMMAND \"${{ matrix.config.environment_script }}\" && set\n            OUTPUT_FILE environment_script_output.txt\n          )\n          set(cxx_flags \"/permissive- /EHsc\")\n          file(STRINGS environment_script_output.txt output_lines)\n          foreach(line IN LISTS output_lines)\n            if (line MATCHES \"^([a-zA-Z0-9_-]+)=(.*)$\")\n              set(ENV{${CMAKE_MATCH_1}} \"${CMAKE_MATCH_2}\")\n            endif()\n          endforeach()\n        endif()\n\n        set(path_separator \":\")\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n          set(path_separator \";\")\n        endif()\n        set(ENV{PATH} \"$ENV{GITHUB_WORKSPACE}${path_separator}$ENV{PATH}\")\n\n        if (\"x${{ matrix.config.libcxx }}\" STREQUAL \"xtrue\")\n          set(cxx_flags \"${cxx_flags} -stdlib=libc++ -nostdinc++ -cxx-isystem $ENV{GITHUB_WORKSPACE}/llvm/include/c++/v1/ -Wno-unused-command-line-argument\")\n          set(link_flags \"${link_flags} -L $ENV{GITHUB_WORKSPACE}/llvm/lib -Wl,-rpath,$ENV{GITHUB_WORKSPACE}/llvm/lib -lc++abi\")\n        endif()\n\n        if (\"${{ matrix.config.cxx }}\" MATCHES \"clang.*\")\n            # clang spurious warning on <=> use\n            set(cxx_flags \"${cxx_flags} -Wno-zero-as-null-pointer-constant\")\n        endif()\n\n        if (\"x${{ matrix.config.cxx_asan }}\" STREQUAL \"xtrue\")\n          set(cxx_flags \"${cxx_flags} -fsanitize=address -fno-omit-frame-pointer\")\n        endif()\n\n        set(cxx_concepts ON)\n        if (\"x${{ matrix.config.cxx_concepts }}\" STREQUAL \"xfalse\")\n          set(cxx_concepts OFF)\n        endif()\n\n        execute_process(\n          COMMAND ${{ steps.cmake_and_ninja.outputs.cmake_dir }}/cmake\n            -S .\n            -B build\n            -G Ninja\n            -D CMAKE_BUILD_TYPE=${{ matrix.config.build_type }}\n            -D CMAKE_MAKE_PROGRAM:STRING=ninja\n            -D CMAKE_CXX_STANDARD:STRING=${{ matrix.config.cxx_standard }}\n            -D \"CMAKE_CXX_FLAGS:STRING=${cxx_flags}\"\n            -D \"CMAKE_EXE_LINKER_FLAGS:STRING=${link_flags}\"\n            -D CMAKE_VERBOSE_MAKEFILE:BOOL=ON\n            -D RANGE_V3_HEADER_CHECKS:BOOL=ON\n            -D RANGES_PREFER_REAL_CONCEPTS:BOOL=${cxx_concepts}\n            ${{ matrix.config.cmake_args }}\n            ${extra_cmake_args}\n          RESULT_VARIABLE result\n        )\n        if (NOT result EQUAL 0)\n          message(FATAL_ERROR \"Bad exit status\")\n        endif()\n\n    - name: Build\n      shell: cmake -P {0}\n      continue-on-error: ${{ matrix.config.experimental || false }}\n      run: |\n        set(ENV{NINJA_STATUS} \"[%f/%t %o/sec] \")\n\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\" AND NOT \"x${{ matrix.config.environment_script }}\" STREQUAL \"x\")\n          file(STRINGS environment_script_output.txt output_lines)\n          foreach(line IN LISTS output_lines)\n            if (line MATCHES \"^([a-zA-Z0-9_-]+)=(.*)$\")\n              set(ENV{${CMAKE_MATCH_1}} \"${CMAKE_MATCH_2}\")\n            endif()\n          endforeach()\n        endif()\n\n        set(path_separator \":\")\n        if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n          set(path_separator \";\")\n        endif()\n        set(ENV{PATH} \"$ENV{GITHUB_WORKSPACE}${path_separator}$ENV{PATH}\")\n\n        execute_process(\n          COMMAND ${{ steps.cmake_and_ninja.outputs.cmake_dir }}/cmake --build build\n          RESULT_VARIABLE result\n        )\n        if (NOT result EQUAL 0)\n          message(FATAL_ERROR \"Bad exit status\")\n        endif()\n\n    - name: Run tests\n      shell: cmake -P {0}\n      continue-on-error: ${{ matrix.config.experimental || false }}\n      run: |\n        include(ProcessorCount)\n        ProcessorCount(N)\n\n        set(ENV{CTEST_OUTPUT_ON_FAILURE} \"ON\")\n\n        execute_process(\n          COMMAND ${{ steps.cmake_and_ninja.outputs.cmake_dir }}/ctest --verbose -j ${N}\n          WORKING_DIRECTORY build\n          RESULT_VARIABLE result\n        )\n        if (NOT result EQUAL 0)\n          message(FATAL_ERROR \"Running tests failed!\")\n        endif()\n",
    "source": "MrE-Fog/range-v3",
    "path": ".github/workflows/range-v3-ci.yml",
    "url": "https://github.com/MrE-Fog/range-v3/blob/689b4f3da769fb21dd7acf62550a038242d832e5/.github/workflows/range-v3-ci.yml",
    "retrieved_at": "2025-08-14T12:50:13.870317Z"
  },
  {
    "question": "How does the `concurrency` group ensure only one workflow runs at a time for the same pull request or branch?",
    "answer": "name: trunk\n\non:\n  push:\n    branches:\n      - main\n      - release/*\n      - landchecks/*\n    tags:\n      - ciflow/trunk/*\n  workflow_dispatch:\n  schedule:\n    - cron: 29 8 * * *  # about 1:29am PDT\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref_name }}-${{ github.ref_type == 'branch' && github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}\n  cancel-in-progress: true\n\npermissions: read-all\n\njobs:\n  target-determination:\n    name: before-test\n    uses: ./.github/workflows/target_determination.yml\n    permissions:\n      id-token: write\n      contents: read\n\n  # Build PyTorch with BUILD_CAFFE2=ON\n  caffe2-linux-jammy-py3_8-gcc11-build:\n    name: caffe2-linux-jammy-py3.8-gcc11\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: caffe2-linux-jammy-py3.8-gcc11\n      docker-image-name: pytorch-linux-jammy-py3.8-gcc11\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1 },\n        ]}\n\n  linux-focal-cuda12_1-py3_10-gcc9-build:\n    name: linux-focal-cuda12.1-py3.10-gcc9\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      test-matrix: |\n        { include: [\n          { config: \"nogpu_AVX512\", shard: 1, num_shards: 1, runner: \"linux.2xlarge\" },\n          { config: \"nogpu_NO_AVX2\", shard: 1, num_shards: 1, runner: \"linux.2xlarge\" },\n          { config: \"jit_legacy\", shard: 1, num_shards: 1, runner: \"linux.4xlarge.nvidia.gpu\" },\n        ]}\n\n  linux-focal-cuda12_1-py3_10-gcc9-test:\n    name: linux-focal-cuda12.1-py3.10-gcc9\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-focal-cuda12_1-py3_10-gcc9-build\n      - target-determination\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9\n      docker-image: ${{ needs.linux-focal-cuda12_1-py3_10-gcc9-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-cuda12_1-py3_10-gcc9-build.outputs.test-matrix }}\n\n  libtorch-linux-focal-cuda12_1-py3_7-gcc9-debug-build:\n    name: libtorch-linux-focal-cuda12.1-py3.7-gcc9-debug\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: libtorch-linux-focal-cuda12.1-py3.7-gcc9\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      build-generates-artifacts: false\n      runner: linux.4xlarge\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1 },\n        ]}\n\n  # no-ops builds test USE_PER_OPERATOR_HEADERS=0 where ATen/ops is not generated\n  linux-focal-cuda12_1-py3_10-gcc9-no-ops-build:\n    name: linux-focal-cuda12.1-py3.10-gcc9-no-ops\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9-no-ops\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1 },\n        ]}\n\n  pytorch-linux-focal-py3-clang9-android-ndk-r21e-build:\n    name: pytorch-linux-focal-py3-clang9-android-ndk-r21e-build\n    uses: ./.github/workflows/_android-full-build-test.yml\n    with:\n      build-environment: pytorch-linux-focal-py3-clang9-android-ndk-r21e-build\n      docker-image-name: pytorch-linux-focal-py3-clang9-android-ndk-r21e\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1, runner: \"linux.2xlarge\" },\n        ]}\n\n  macos-12-py3-arm64-build:\n    name: macos-12-py3-arm64\n    uses: ./.github/workflows/_mac-build.yml\n    with:\n      sync-tag: macos-12-py3-arm64-build\n      build-environment: macos-12-py3-arm64\n      runner-type: macos-m1-stable\n      build-generates-artifacts: true\n      # To match the one pre-installed in the m1 runners\n      python-version: 3.9.12\n      # We need to set the environment file here instead of trying to detect it automatically because\n      # MacOS arm64 is cross-compiled from x86-64. Specifically, it means that arm64 conda environment\n      # is needed when building PyTorch MacOS arm64 from x86-64\n      environment-file: .github/requirements/conda-env-macOS-ARM64\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 3, runner: \"macos-m1-stable\" },\n          { config: \"default\", shard: 2, num_shards: 3, runner: \"macos-m1-stable\" },\n          { config: \"default\", shard: 3, num_shards: 3, runner: \"macos-m1-stable\" },\n        ]}\n\n  macos-12-py3-arm64-mps-test:\n    name: macos-12-py3-arm64-mps\n    uses: ./.github/workflows/_mac-test-mps.yml\n    needs: macos-12-py3-arm64-build\n    if: needs.macos-12-py3-arm64-build.outputs.build-outcome == 'success'\n    with:\n      sync-tag: macos-12-py3-arm64-mps-test\n      build-environment: macos-12-py3-arm64\n      # Same as the build job\n      python-version: 3.9.12\n      test-matrix: |\n        { include: [\n          { config: \"mps\", shard: 1, num_shards: 1, runner: \"macos-m1-stable\" },\n        ]}\n\n  macos-12-py3-arm64-test:\n    name: macos-12-py3-arm64\n    uses: ./.github/workflows/_mac-test.yml\n    needs:\n      - macos-12-py3-arm64-build\n      - target-determination\n    with:\n      build-environment: macos-12-py3-arm64\n      # Same as the build job\n      python-version: 3.9.12\n      test-matrix: ${{ needs.macos-12-py3-arm64-build.outputs.test-matrix }}\n      arch: arm64\n\n  win-vs2019-cpu-py3-build:\n    name: win-vs2019-cpu-py3\n    uses: ./.github/workflows/_win-build.yml\n    with:\n      build-environment: win-vs2019-cpu-py3\n      cuda-version: cpu\n      sync-tag: win-cpu-build\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 3, runner: \"windows.4xlarge.nonephemeral\" },\n          { config: \"default\", shard: 2, num_shards: 3, runner: \"windows.4xlarge.nonephemeral\" },\n          { config: \"default\", shard: 3, num_shards: 3, runner: \"windows.4xlarge.nonephemeral\" },\n        ]}\n\n  win-vs2019-cpu-py3-test:\n    name: win-vs2019-cpu-py3\n    uses: ./.github/workflows/_win-test.yml\n    needs:\n      - win-vs2019-cpu-py3-build\n      - target-determination\n    with:\n      build-environment: win-vs2019-cpu-py3\n      cuda-version: cpu\n      test-matrix: ${{ needs.win-vs2019-cpu-py3-build.outputs.test-matrix }}\n\n  win-vs2019-cuda11_8-py3-build:\n    name: win-vs2019-cuda11.8-py3\n    uses: ./.github/workflows/_win-build.yml\n    with:\n      build-environment: win-vs2019-cuda11.8-py3\n      cuda-version: \"11.8\"\n      sync-tag: win-cuda-build\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 2, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 3, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 4, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 5, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 6, num_shards: 6, runner: \"windows.g5.4xlarge.nvidia.gpu\" },\n          { config: \"force_on_cpu\", shard: 1, num_shards: 1, runner: \"windows.4xlarge.nonephemeral\" },\n        ]}\n\n  linux-focal-rocm6_0-py3_8-build:\n    name: linux-focal-rocm6.0-py3.8\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-rocm6.0-py3.8\n      docker-image-name: pytorch-linux-focal-rocm-n-py3\n      sync-tag: rocm-build\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 1, runner: \"linux.rocm.gpu\" },\n        ]}\n\n  linux-focal-rocm6_0-py3_8-test:\n    permissions:\n      id-token: write\n      contents: read\n    name: linux-focal-rocm6.0-py3.8\n    uses: ./.github/workflows/_rocm-test.yml\n    needs:\n      - linux-focal-rocm6_0-py3_8-build\n      - target-determination\n    with:\n      build-environment: linux-focal-rocm6.0-py3.8\n      docker-image: ${{ needs.linux-focal-rocm6_0-py3_8-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-rocm6_0-py3_8-build.outputs.test-matrix }}\n      tests-to-include: \"test_nn test_torch test_cuda test_ops test_unary_ufuncs test_binary_ufuncs test_autograd inductor/test_torchinductor\"\n",
    "source": "zal-orz/pytorch",
    "path": ".github/workflows/trunk.yml",
    "url": "https://github.com/zal-orz/pytorch/blob/78b4793c965fe640e37d80530eb78f07d67492e8/.github/workflows/trunk.yml",
    "retrieved_at": "2025-08-14T12:50:14.763071Z"
  },
  {
    "question": "What specific files are linted by the workflow when a pull request is made?",
    "answer": "name: Lint\n\non:\n  pull_request:\n\nenv:\n  PYTHON_VERSION: \"3.8\"\n\ndefaults:\n  run:\n    shell: bash -l -eo pipefail {0}\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repo\n        uses: actions/checkout@v3\n      - name: Setup python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Update pip\n        run: python -m pip install --upgrade pip\n      - name: Install lint utilities\n        run: |\n          python -m pip install pre-commit\n          pre-commit install-hooks\n      - id: file_changes\n        uses: trilom/file-changes-action@v1.2.4\n        with:\n          prNumber: ${{ github.event.number }}\n          output: ' '\n      - name: Lint modified files\n        run: pre-commit run --files ${{ steps.file_changes.outputs.files }}\n  type_checking:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repo\n        uses: actions/checkout@v3\n      - name: Setup conda env\n        uses: conda-incubator/setup-miniconda@v2\n        with:\n          auto-update-conda: true\n          miniconda-version: \"latest\"\n          activate-environment: lint\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Install dependencies\n        run: |\n          conda install pytorch torchvision torchtext cpuonly -c pytorch-nightly\n          pip install -e \".[dev]\"\n      - name: Type check all files\n        run: python -m mypy --install-types --non-interactive --config-file mypy.ini\n",
    "source": "ravich3373/flava",
    "path": ".github/workflows/lint.yaml",
    "url": "https://github.com/ravich3373/flava/blob/8c0e16dd6b8b944ca718655d772850f27de1f4b8/.github/workflows/lint.yaml",
    "retrieved_at": "2025-08-14T12:51:44.555655Z"
  },
  {
    "question": "What CSS styling rules and conventions are enforced by this workflow?",
    "answer": "# @Author: Roni Laukkarinen\n# @Date:   2023-02-15 17:39:37\n# @Last Modified by:   Roni Laukkarinen\n# @Last Modified time: 2023-03-03 20:01:10\nname: CSS\n\non: [push, pull_request]\n\njobs:\n  build:\n    name: Test styles\n    runs-on: ubuntu-20.04\n\n    steps:\n    - name: Checkout the repository\n      uses: actions/checkout@v3\n\n    - name: Read .nvmrc\n      run: echo ::set-output name=NVMRC::$(cat .nvmrc)\n      id: nvm\n\n    - name: Setup node\n      uses: actions/setup-node@v1\n      with:\n        node-version: '${{ steps.nvm.outputs.NVMRC }}'\n\n    - name: Get package.json from devpackages\n      run: |\n        rm package.json\n        wget https://raw.githubusercontent.com/digitoimistodude/devpackages/master/package.json\n        sed -i 's/PROJECTNAME/air-light/g' package.json\n        npm install --global\n\n    - name: Install stylelint packages\n      run: |\n        npm install --global \\\n        postcss@^8.4.21 \\\n        postcss-scss@^4.0.6 \\\n        stylelint@^15.1.0 \\\n        stylelint-config-recommended@^10.0.1 \\\n        stylelint-config-recommended-scss@^9.0.0 \\\n        stylelint-config-standard@^30.0.1 \\\n        stylelint-config-standard-scss@^7.0.0 \\\n        stylelint-file-max-lines@^1.0.0 \\\n        stylelint-order@^6.0.2 \\\n        stylelint-rem-over-px@^0.0.4 \\\n        stylelint-scss@^4.4.0 \\\n        @ronilaukkarinen/stylelint-a11y@^1.2.7 \\\n        @ronilaukkarinen/stylelint-declaration-strict-value@^1.9.2 \\\n        @ronilaukkarinen/stylelint-value-no-unknown-custom-properties@^4.0.1\n\n    - name: Run stylelint\n      run: |\n        npx stylelint . --max-warnings 0 --config .stylelintrc\n",
    "source": "raikasdev/hydrogen-starter",
    "path": ".github/workflows/styles.yml",
    "url": "https://github.com/raikasdev/hydrogen-starter/blob/72d41cebb6124e1cb192c681fff61e6f7fd0eb96/.github/workflows/styles.yml",
    "retrieved_at": "2025-08-14T12:51:45.263631Z"
  },
  {
    "question": "What types of changes trigger this workflow to run?",
    "answer": "name: Linter\n\non: [push]\n\nenv:\n  NODE_VERSION: 20\n\njobs:\n  build:\n    name: JS & CSS & TS & D.TS\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # https://github.com/actions/checkout/releases/tag/v4.1.1\n\n      - name: Use Node.js ${{ env.NODE_VERSION }}\n        uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8 # https://github.com/actions/setup-node/releases/tag/v4.0.2\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: |\n          npm ci --no-audit\n\n      - name: JavaScript lint\n        run: |\n          npm run lint\n\n      - name: CSS lint\n        run: |\n          npm run in handsontable stylelint\n",
    "source": "Slimua/handsontable",
    "path": ".github/workflows/linter.yml",
    "url": "https://github.com/Slimua/handsontable/blob/8595b3432cbe4ce05393661f225db5d68cac2bce/.github/workflows/linter.yml",
    "retrieved_at": "2025-08-14T12:51:45.956441Z"
  },
  {
    "question": "Under what conditions will the \"Build and publish\" job execute?",
    "answer": "name: Publish prerelease in NPM\n\non:\n  workflow_dispatch:\n  workflow_run:\n    workflows: [Tests]\n    types: [completed]\n    branches:\n      - 'develop'\n      - 'release/**'\n      - '!release/**-**'\n\nenv:\n  NODE_VERSION: 20\n\njobs:\n  build:\n    name: Build and publish\n    runs-on: ubuntu-latest\n    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}\n    steps:\n      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # https://github.com/actions/checkout/releases/tag/v4.1.1\n        with:\n          ref: ${{ github.event.workflow_run.head_branch }}\n\n      - name: Use Node.js ${{ env.NODE_VERSION }}\n        uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8 # https://github.com/actions/setup-node/releases/tag/v4.0.2\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          registry-url: https://registry.npmjs.org/\n          cache: 'npm'\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPM_PUBLISH_AUTOMATION }}\n      - run: npm install\n      - run: npm run pre-release\n",
    "source": "Slimua/handsontable",
    "path": ".github/workflows/publish.yml",
    "url": "https://github.com/Slimua/handsontable/blob/8595b3432cbe4ce05393661f225db5d68cac2bce/.github/workflows/publish.yml",
    "retrieved_at": "2025-08-14T12:51:46.657106Z"
  },
  {
    "question": "What are the different build configurations and compiler versions used within the compatibility job matrix?",
    "answer": "name: Windows\n\non:\n  push:\n    paths-ignore:\n      - '**/*.md'\n  pull_request:\n    paths-ignore:\n      - '**/*.md'\n\njobs:\n  compatibility:\n    runs-on: windows-latest\n    strategy:\n      matrix:\n        VER: [v142, v143]\n        EXT: [ON, OFF]\n        GEN: [Visual Studio 17 2022]\n        BIN: [x64, x86]\n        STD: [99, 11, 17]\n        include:\n          - VER: v141\n            EXT: OFF\n            GEN: Ninja Multi-Config\n            BIN: x64\n            STD: 89 # /Za\n    env:\n      NINJA_URL: https://github.com/ninja-build/ninja/releases/download/v1.10.2/ninja-win.zip\n      NINJA_ROOT: C:\\Tools\\Ninja\n      VS_ROOT: 'C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise'\n      UseMultiToolTask: true # Better parallel MSBuild execution\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Cache Ninja install\n      if: matrix.GEN == 'Ninja Multi-Config'\n      id: ninja-install\n      uses: actions/cache@v2\n      with:\n        path: |\n          C:\\Tools\\Ninja\n        key: ${{runner.os}}-ninja-${{env.NINJA_URL}}\n\n    - name: Install Ninja\n      if: matrix.GEN == 'Ninja Multi-Config' && steps.ninja-install.outputs.cache-hit != 'true'\n      shell: pwsh\n      run: |\n        Invoke-WebRequest ${env:NINJA_URL} -OutFile ~\\Downloads\\ninja-win.zip\n        Expand-Archive ~\\Downloads\\ninja-win.zip -DestinationPath ${env:NINJA_ROOT}\\\n        Remove-Item ~\\Downloads\\*\n\n    - name: Configure (MSBuild)\n      if: matrix.GEN == 'Visual Studio 17 2022'\n      shell: pwsh\n      run: |\n        $BIN = if('${{matrix.BIN}}' -eq 'x86') {'Win32'} else {'x64'}\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -A $BIN `\n          -T ${{matrix.VER}} `\n          -D BUILD_TESTING=ON `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -S \"${env:GITHUB_WORKSPACE}\" `\n          -B \"${env:GITHUB_WORKSPACE}\\build\"\n\n    - name: Configure (Ninja Multi-Config)\n      if: matrix.GEN == 'Ninja Multi-Config'\n      shell: pwsh\n      run: |\n        $VER = switch ('${{matrix.VER}}') { `\n          'v141' {'14.1'} `\n          'v142' {'14.2'} `\n          'v143' {'14.3'} }\n        Import-Module \"${env:VS_ROOT}\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\"\n        Enter-VsDevShell -VsInstallPath ${env:VS_ROOT} -SkipAutomaticLocation -DevCmdArguments \"-host_arch=x64 -arch=${{matrix.BIN}} -vcvars_ver=${VER}\"\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -D CMAKE_MAKE_PROGRAM=\"${env:NINJA_ROOT}\\ninja.exe\" `\n          -D BUILD_TESTING=ON `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_EXE_LINKER_FLAGS='/INCREMENTAL' `\n          -S \"${env:GITHUB_WORKSPACE}\" `\n          -B \"${env:GITHUB_WORKSPACE}\\build\"\n\n    - name: Build (MSBuild)\n      if: matrix.GEN == 'Visual Studio 17 2022'\n      shell: pwsh\n      run: |\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\build\" `\n            --config ${Config} `\n            -- `\n            /verbosity:minimal `\n            /maxCpuCount `\n            /noLogo\n        }\n\n    - name: Build (Ninja)\n      if: matrix.GEN == 'Ninja Multi-Config'\n      shell: pwsh\n      run: |\n        $VER = switch ('${{matrix.VER}}') { `\n          'v141' {'14.1'} `\n          'v142' {'14.2'} `\n          'v143' {'14.3'} }\n        Import-Module \"${env:VS_ROOT}\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\"\n        Enter-VsDevShell -VsInstallPath ${env:VS_ROOT} -SkipAutomaticLocation -DevCmdArguments \"-host_arch=x64 -arch=${{matrix.BIN}} -vcvars_ver=${VER}\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\build\" `\n            --config ${Config} `\n            -- `\n            -j ${env:NUMBER_OF_PROCESSORS}\n        }\n\n    - name: Test\n      shell: pwsh\n      run: |\n        foreach ($Config in 'Release','Debug') { `\n          & ctest `\n            --test-dir \"${env:GITHUB_WORKSPACE}\\build\" `\n            --build-config ${Config} `\n            --output-on-failure `\n            --parallel ${env:NUMBER_OF_PROCESSORS}\n        }\n\n    - name: Install\n      shell: pwsh\n      run: |\n        & cmake `\n          --install \"${env:GITHUB_WORKSPACE}\\build\" `\n          --prefix \"${env:GITHUB_WORKSPACE}\\install\" `\n          --config Release\n\n    - name: Consume (PkgConfig - bare MSBuild)\n      if: matrix.GEN == 'Visual Studio 17 2022'\n      shell: pwsh\n      run: |\n        $BIN = if('${{matrix.BIN}}' -eq 'x86') {'Win32'} else {'x64'}\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -A $BIN `\n          -T ${{matrix.VER}} `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_PREFIX_PATH=\"${env:GITHUB_WORKSPACE}\\install\" `\n          -S \"${env:GITHUB_WORKSPACE}\\tests\\pkgconfig\\bare\" `\n          -B \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\bare\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\bare\" `\n            --config ${Config} `\n            -- `\n            /verbosity:minimal `\n            /maxCpuCount `\n            /noLogo `\n        }\n\n    - name: Consume (PkgConfig - bare Ninja)\n      if: matrix.GEN == 'Ninja Multi-Config'\n      shell: pwsh\n      run: |\n        $VER = switch ('${{matrix.VER}}') { `\n          'v141' {'14.1'} `\n          'v142' {'14.2'} `\n          'v143' {'14.3'} }\n        Import-Module \"${env:VS_ROOT}\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\"\n        Enter-VsDevShell -VsInstallPath ${env:VS_ROOT} -SkipAutomaticLocation -DevCmdArguments \"-host_arch=x64 -arch=${{matrix.BIN}} -vcvars_ver=${VER}\"\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -D CMAKE_MAKE_PROGRAM=\"${env:NINJA_ROOT}\\ninja.exe\" `\n          -D BUILD_TESTING=ON `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_EXE_LINKER_FLAGS='/INCREMENTAL' `\n          -D CMAKE_PREFIX_PATH=\"${env:GITHUB_WORKSPACE}\\install\" `\n          -S \"${env:GITHUB_WORKSPACE}\\tests\\pkgconfig\\bare\" `\n          -B \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\bare\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\bare\" `\n            --config ${Config} `\n            -- `\n            -j ${env:NUMBER_OF_PROCESSORS} `\n        }\n\n    - name: Consume (Emulate SDK presence)\n      shell: pwsh\n      run: |\n        New-Item -Type Directory -Path ${env:GITHUB_WORKSPACE}\\install\\share\\cmake\\OpenCL\n        New-Item -Type File -Path ${env:GITHUB_WORKSPACE}\\install\\share\\cmake\\OpenCL\\OpenCLConfig.cmake -Value 'include(\"${CMAKE_CURRENT_LIST_DIR}/../OpenCLHeaders/OpenCLHeadersTargets.cmake\")'\n\n    - name: Consume (PkgConfig - SDK MSBuild)\n      if: matrix.GEN == 'Visual Studio 17 2022'\n      shell: pwsh\n      run: |\n        $BIN = if('${{matrix.BIN}}' -eq 'x86') {'Win32'} else {'x64'}\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -A $BIN `\n          -T ${{matrix.VER}} `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_PREFIX_PATH=\"${env:GITHUB_WORKSPACE}\\install\" `\n          -S \"${env:GITHUB_WORKSPACE}\\tests\\pkgconfig\\sdk\" `\n          -B \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\sdk\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\sdk\" `\n            --config ${Config} `\n            -- `\n            /verbosity:minimal `\n            /maxCpuCount `\n            /noLogo `\n        }\n\n    - name: Consume (PkgConfig - SDK Ninja)\n      if: matrix.GEN == 'Ninja Multi-Config'\n      shell: pwsh\n      run: |\n        $VER = switch ('${{matrix.VER}}') { `\n          'v141' {'14.1'} `\n          'v142' {'14.2'} `\n          'v143' {'14.3'} }\n        Import-Module \"${env:VS_ROOT}\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\"\n        Enter-VsDevShell -VsInstallPath ${env:VS_ROOT} -SkipAutomaticLocation -DevCmdArguments \"-host_arch=x64 -arch=${{matrix.BIN}} -vcvars_ver=${VER}\"\n        $C_FLAGS = '/W4 /WX'\n        & cmake `\n          -G '${{matrix.GEN}}' `\n          -D CMAKE_MAKE_PROGRAM=\"${env:NINJA_ROOT}\\ninja.exe\" `\n          -D BUILD_TESTING=ON `\n          -D CMAKE_C_FLAGS=\"${C_FLAGS}\" `\n          -D CMAKE_C_EXTENSIONS='${{matrix.EXT}}' `\n          -D CMAKE_EXE_LINKER_FLAGS='/INCREMENTAL' `\n          -D CMAKE_PREFIX_PATH=\"${env:GITHUB_WORKSPACE}\\install\" `\n          -S \"${env:GITHUB_WORKSPACE}\\tests\\pkgconfig\\sdk\" `\n          -B \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\sdk\"\n        foreach ($Config in 'Release','Debug') { `\n          & cmake `\n            --build \"${env:GITHUB_WORKSPACE}\\downstream\\pkgconfig\\sdk\" `\n            --config ${Config} `\n            -- `\n            -j ${env:NUMBER_OF_PROCESSORS} `\n        }",
    "source": "msft-mirror-aosp/platform.external.OpenCL-Headers",
    "path": ".github/workflows/windows.yml",
    "url": "https://github.com/msft-mirror-aosp/platform.external.OpenCL-Headers/blob/dceb741b02c8788107db18f343b46d542406467a/.github/workflows/windows.yml",
    "retrieved_at": "2025-08-14T12:51:47.502862Z"
  },
  {
    "question": "Under what conditions does the `build-protocol` job automatically commit changes to the repository?",
    "answer": "name: Protocol\n\non:\n  pull_request:\n    types: [opened, synchronize, reopened, ready_for_review]\n    paths:\n      - \"packages/protocol/**\"\n      - \"!packages/protocol/contract_layout_*\"\n      - \"!packages/protocol/audit/**\"\n      - \"!packages/protocol/docs/**\"\n      - \"!packages/protocol/simulation/**\"\n      - \"!packages/protocol/deployments/**\"\n      - \"!packages/protocol/script/layer2/hekla/**\"\n      - \"!packages/protocol/script/layer2/mainnet/**\"\n      - \"!packages/protocol/script/layer1/hekla/**\"\n      - \"!packages/protocol/script/layer1/mainnet/**\"\n      - \"!packages/protocol/script/layer1/preconf/**\"\n      - \"!packages/protocol/script/layer1/provers/**\"\n      - \"!packages/protocol/script/layer1/team/**\"\n    branches-ignore:\n      - release-please--branches--**\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build-protocol:\n    if: ${{ github.event.pull_request.draft == false  && !startsWith(github.head_ref, 'release-please') && github.actor != 'dependabot[bot]' }}\n    runs-on: [arc-runner-set]\n    permissions:\n      # Give the necessary permissions for stefanzweifel/git-auto-commit-action.\n      contents: write\n    steps:\n      - name: Cancel previous runs\n        uses: styfle/cancel-workflow-action@0.12.1\n        with:\n          access_token: ${{ github.token }}\n\n      - name: Prepare environment\n        continue-on-error: true\n        run: sudo apt-get update && sudo apt-get install -y git wget\n\n      - name: Checkout repository\n        uses: actions/checkout@v4\n        with:\n          submodules: recursive\n\n      - name: Install Foundry\n        uses: foundry-rs/foundry-toolchain@v1.2.0\n        with:\n          version: stable\n\n      - name: Install pnpm dependencies\n        uses: ./.github/actions/install-pnpm-dependencies\n\n      - name: Clean up and fmt\n        working-directory: ./packages/protocol\n        run: pnpm clean && forge fmt\n\n      - name: L2-Unit tests\n        working-directory: ./packages/protocol\n        run: pnpm compile:l2 && pnpm test:l2 && pnpm layout:l2\n\n      - name: L1-Unit tests\n        working-directory: ./packages/protocol\n        run: pnpm compile:l1 && pnpm snapshot:l1 && pnpm layout:l1\n\n      - name: Check for changes\n        id: git_status\n        run: |\n          git add -N .  # Simulate staging to detect untracked files\n          if [ -n \"$(git status --porcelain)\" ]; then\n            echo \"changes=true\" >> $GITHUB_ENV\n          else\n            echo \"changes=false\" >> $GITHUB_ENV\n          fi\n\n      - name: Commit contract layout table\n        if: env.changes == 'true'\n        uses: stefanzweifel/git-auto-commit-action@v5\n        with:\n          commit_message: \"forge fmt & update contract layout tables\"\n\n      - name: L1-Deploy contracts\n        working-directory: ./packages/protocol\n        timeout-minutes: 2\n        run: |\n          anvil --hardfork cancun &\n          until cast chain-id --rpc-url \"http://localhost:8545\" 2> /dev/null; do\n            sleep 1\n          done\n          pnpm test:deploy:l1\n\n  genesis-docker:\n    if: ${{ github.event.pull_request.draft == false  && !startsWith(github.head_ref, 'release-please') && github.actor != 'dependabot[bot]' }}\n    runs-on: [taiko-runner]\n    permissions:\n      # Give the necessary permissions for stefanzweifel/git-auto-commit-action.\n      contents: write\n    steps:\n      - name: Cancel previous runs\n        uses: styfle/cancel-workflow-action@0.12.1\n        with:\n          access_token: ${{ github.token }}\n\n      - name: Checkout repository\n        uses: actions/checkout@v4\n        with:\n          submodules: recursive\n\n      - name: Install Foundry\n        uses: foundry-rs/foundry-toolchain@v1.3.1\n\n      - name: Install pnpm dependencies\n        uses: ./.github/actions/install-pnpm-dependencies\n\n      - name: Compile\n        working-directory: ./packages/protocol\n        run: pnpm clean && pnpm compile\n\n      - name: L2-Generate Genesis (using docker)\n        working-directory: ./packages/protocol\n        run: pnpm genesis:test\n",
    "source": "NethermindEth/preconf-taiko-mono",
    "path": ".github/workflows/protocol.yml",
    "url": "https://github.com/NethermindEth/preconf-taiko-mono/blob/5ac7eda8202115e23bf76fd554c0a62b5aee765c/.github/workflows/protocol.yml",
    "retrieved_at": "2025-08-14T12:51:48.275454Z"
  },
  {
    "question": "What branches, besides `trying`, `trying.tmp`, `staging`, and `staging.tmp`, trigger this workflow?",
    "answer": "on:\n  push:\n    branches:\n      - '!trying'\n      - '!trying.tmp'\n      - '!staging'\n      - '!staging.tmp'\n\nname: Coverage\n\nenv:\n  RUST_BACKTRACE: 1\n  RUSTFLAGS: \"-Ccodegen-units=1 -Clink-dead-code -Coverflow-checks=off\"\n\njobs:\n  coverage:\n    name: Coverage\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          profile: minimal\n          override: true\n      - name: Install LLVM (Linux)\n        run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz -L -o llvm.tar.xz\n          mkdir -p /opt/llvm-10\n          tar xf llvm.tar.xz --strip-components=1 -C /opt/llvm-10\n          echo '/opt/llvm-10/bin' >> $GITHUB_PATH\n          echo 'LLVM_SYS_100_PREFIX=/opt/llvm-10' >> $GITHUB_ENV\n      - name: Generate Coverage Report\n        run: |\n          cargo install cargo-tarpaulin\n          cargo tarpaulin --forward --release -t120 --out Xml --ignore-tests --workspace --exclude wasmer-wasi-experimental-io-devices --exclude wasmer-c-api -- --skip traps:: --skip spec::linking --test-threads=1\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v1\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          file: ./cobertura.xml\n",
    "source": "ailisp/near-wasmer",
    "path": ".github/workflows/coverage.yaml",
    "url": "https://github.com/ailisp/near-wasmer/blob/b75a0b651abd1b271142dc6ebc870b0090434972/.github/workflows/coverage.yaml",
    "retrieved_at": "2025-08-14T12:51:49.118560Z"
  },
  {
    "question": "Under what conditions will Dependabot pull requests be automatically merged by this workflow?",
    "answer": "name: Dependabot Auto Merge\n\non:\n  pull_request_target:\n\njobs:\n  auto-merge:\n    timeout-minutes: 5\n    runs-on: ubuntu-latest\n    if: github.actor == 'dependabot[bot]'\n    steps:\n      - uses: actions/checkout@v2\n      - uses: ahmadnassri/action-dependabot-auto-merge@v2\n        with:\n          target: minor\n          github-token: ${{ secrets.DEP_AUTOMERGE }}\n",
    "source": "ttttonyhe/ouorz-mono",
    "path": ".github/workflows/auto-merge.yml",
    "url": "https://github.com/ttttonyhe/ouorz-mono/blob/19ccccede781c71f3a3c5c4bbfc27368cc43be03/.github/workflows/auto-merge.yml",
    "retrieved_at": "2025-08-14T12:51:49.878293Z"
  },
  {
    "question": "Under what conditions will the code formatting check be executed?",
    "answer": "name: '🔎 Check Code Formatting'\n\non:\n  push:\n    branches:\n      - 'master'\n    paths:\n      - 'src/**'\n      - '!**/README.md'\n      - '!**.rst'\n\n  pull_request:\n    types:\n      - opened\n      - edited\n      - reopened\n      - synchronize\n    branches:\n      - 'master'\n\njobs:\n  formatting-check:\n    runs-on: ubuntu-20.04\n    steps:\n    - name: '⏳ Checkout repository'\n      uses: actions/checkout@v4\n      with:\n        submodules: false\n        persist-credentials: false\n\n    - name: '♻ Caching dependencies'\n      uses: actions/cache@v4.0.2\n      id: cache\n      with:\n        path: ~/cache/deps/bin\n        key: 'uncrustify'\n\n    - name: '🛠 Install dependencies'\n      if: steps.cache.outputs.cache-hit != 'true'\n      run: source tools/ci.sh && ci_install_code_format_deps\n\n    - name: '📜 Get list of changed files'\n      id: changed-files\n      uses: tj-actions/changed-files@v44\n      with:\n        files: |\n            src/**/*.c\n            src/**/*.h\n            !src/hal/**\n            !src/uvc/**\n            !src/lib/**\n            !src/drivers/**\n            !src/micropython/**\n            !src/stm32cubeai/**\n\n    - name: '📜 Show list of changed files'\n      run: |\n        echo \"${{ toJSON(steps.changed-files.outputs) }}\"\n      shell:\n        bash\n\n    - name: '🔎 Check code formatting'\n      if: steps.changed-files.outputs.any_changed == 'true'\n      run: |\n        source tools/ci.sh && ci_run_code_format_check ${{ steps.changed-files.outputs.all_changed_files }}\n",
    "source": "Omarsevenway/openmv-master",
    "path": ".github/workflows/codeformat.yml",
    "url": "https://github.com/Omarsevenway/openmv-master/blob/8e3c88119e4b813a2abceddcfb30370d9db5f36c/.github/workflows/codeformat.yml",
    "retrieved_at": "2025-08-14T12:51:50.622680Z"
  },
  {
    "question": "What specific benchmarks are run by the workflow, and how are their results stored and sent to Datadog?",
    "answer": "name: Haystack 1.x Benchmarks\n\non:\n  workflow_dispatch:\n\npermissions:\n  id-token: write\n  contents: read\n\nenv:\n  AWS_REGION: eu-central-1\n\njobs:\n  deploy-runner:\n    runs-on: ubuntu-latest\n    outputs:\n      cml_runner_id: ${{ steps.deploy.outputs.cml_runner_id }}\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: iterative/setup-cml@v3\n\n      - name: AWS authentication\n        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}\n\n      - name: Launch EC2 instance and deploy runner\n        id: deploy\n        env:\n          repo_token: ${{ secrets.HAYSTACK_BOT_TOKEN }}\n        run: |\n          OUTPUT=$(cml runner launch \\\n          --cloud aws \\\n          --cloud-region ${{ env.AWS_REGION }} \\\n          --cloud-type=p3.2xlarge \\\n          --cloud-hdd-size=64 \\\n          --labels=cml 2>&1 | tee /dev/fd/2)\n          # Extract 'id' from the log and set it as an environment variable\n          ID_VALUE=$(echo \"$OUTPUT\" | jq -r '.message? | fromjson? | select(.id != null) | .id // empty')\n          echo \"cml_runner_id=$ID_VALUE\" >> \"$GITHUB_OUTPUT\"\n\n  run-reader-benchmarks:\n    needs: deploy-runner\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/reader/*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-reader\n          path: test/benchmarks/out/\n\n  run-elasticsearch-benchmarks:\n    needs:\n      - deploy-runner\n      - run-reader-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      elasticsearch:\n        image: elasticsearch:7.17.6\n        env:\n          discovery.type: \"single-node\"\n        ports:\n          - 9201:9200\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,elasticsearch,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-elasticsearch-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-elasticsearch\n          path: test/benchmarks/out/\n\n  run-weaviate-benchmarks:\n    needs:\n      - deploy-runner\n      - run-elasticsearch-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      weaviate:\n        image: semitechnologies/weaviate:1.17.2\n        env:\n          AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: \"true\"\n          PERSISTENCE_DATA_PATH: \"/var/lib/weaviate\"\n        ports:\n          - 8080:8080\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,weaviate,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-weaviate-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-weaviate\n          path: test/benchmarks/out/\n\n  run-opensearch-benchmarks:\n    needs:\n      - deploy-runner\n      - run-weaviate-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      opensearch:\n        image: opensearchproject/opensearch:1.3.5\n        env:\n          discovery.type: \"single-node\"\n          OPENSEARCH_JAVA_OPTS: \"-Xms4096m -Xmx4096m\"\n        ports:\n          - 9200:9200\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,opensearch,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-opensearch-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-opensearch\n          path: test/benchmarks/out/\n\n  terminate-runner:\n    if: always()\n    needs:\n      - deploy-runner\n      - run-opensearch-benchmarks\n    runs-on: ubuntu-latest\n    steps:\n      - name: AWS authentication\n        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}\n\n      - name: Terminate EC2 instance\n        env:\n          CML_RUNNER_ID: ${{needs.deploy-runner.outputs.cml_runner_id}}\n        run: |\n          # Get the instance ID using its Name tag and terminate the instance\n          INSTANCE_ID=$(aws ec2 describe-instances --filters \"Name=tag:Name,Values=${{ env.CML_RUNNER_ID }}\" --query \"Reservations[*].Instances[*].[InstanceId]\" --output text)\n          aws ec2 terminate-instances --instance-ids \"$INSTANCE_ID\"\n",
    "source": "SAKTHICPT/deepset-ai-haystack",
    "path": ".github/workflows/benchmarks.yml",
    "url": "https://github.com/SAKTHICPT/deepset-ai-haystack/blob/9e18c822e07f2038397aabeff9a176aa69995f4b/.github/workflows/benchmarks.yml",
    "retrieved_at": "2025-08-14T12:51:51.333091Z"
  },
  {
    "question": "Under what conditions does the `delete-preview` job execute and what artifact does it upload?",
    "answer": "name: Preview (build)\non:\n  pull_request:\n    types: [opened, synchronize, reopened, closed]\n    paths-ignore:\n      - 'microsite/**'\n      - '*.md'\n\njobs:\n  build-backstage:\n    env:\n      NODE_OPTIONS: --max-old-space-size=4096\n      UFFIZZI_URL: https://uffizzi.com\n    name: Build PR image\n    runs-on: ubuntu-latest\n    if: ${{ github.event_name != 'pull_request' || github.event.action != 'closed' }}\n    outputs:\n      tags: ${{ steps.meta.outputs.tags }}\n    steps:\n      - name: checkout\n        uses: actions/checkout@v3\n\n      - name: setup-node\n        uses: actions/setup-node@v3\n        with:\n          node-version: 16.x\n          registry-url: https://registry.npmjs.org/\n\n      - name: yarn install\n        uses: backstage/actions/yarn-install@v0.6.4\n        with:\n          cache-prefix: linux-v16\n\n      - name: Use Uffizzi's backstage app config\n        run: |\n          cp -f ./.github/uffizzi/uffizzi.production.app-config.yaml ./app-config.yaml\n\n      - name: typescript build\n        run: |\n          yarn tsc\n\n      - name: backstage build\n        run: |\n          yarn workspace example-backend build\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n\n      - name: Generate UUID image name\n        id: uuid\n        run: echo \"UUID_TAG_APP=$(uuidgen)\" >> $GITHUB_ENV\n\n      - name: Docker metadata\n        id: meta\n        uses: docker/metadata-action@v4\n        with:\n          images: registry.uffizzi.com/${{ env.UUID_TAG_APP }}\n          tags: type=raw,value=60d\n\n      - name: Build Image\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          file: packages/backend/Dockerfile\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n          push: true\n\n  render-compose-file:\n    name: Render Docker Compose File\n    runs-on: ubuntu-latest\n    needs:\n      - build-backstage\n    outputs:\n      compose-file-cache-key: ${{ steps.hash.outputs.hash }}\n    steps:\n      - name: Checkout git repo\n        uses: actions/checkout@v3\n      - name: Render Compose File\n        run: |\n          BACKSTAGE_IMAGE=$(echo ${{ needs.build-backstage.outputs.tags }})\n          export BACKSTAGE_IMAGE\n          # Render simple template from environment variables.\n          envsubst '$BACKSTAGE_IMAGE $GITHUB_SHA' < .github/uffizzi/docker-compose.uffizzi.yml > docker-compose.rendered.yml\n          cat docker-compose.rendered.yml\n      - name: Upload Rendered Compose File as Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: preview-spec\n          path: docker-compose.rendered.yml\n          retention-days: 2\n      - name: Upload PR Event as Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: preview-spec\n          path: ${{ github.event_path }}\n          retention-days: 2\n\n  delete-preview:\n    name: Call for Preview Deletion\n    runs-on: ubuntu-latest\n    if: ${{ github.event.action == 'closed' }}\n    steps:\n      # If this PR is closing, we will not render a compose file nor pass it to the next workflow.\n      - name: Upload PR Event as Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: preview-spec\n          path: ${{ github.event_path }}\n          retention-days: 2\n",
    "source": "davidosantos/backstage",
    "path": ".github/workflows/uffizzi-build.yml",
    "url": "https://github.com/davidosantos/backstage/blob/d820584c6df4aa66c927622f86898aa2c1a4fc3f/.github/workflows/uffizzi-build.yml",
    "retrieved_at": "2025-08-15T01:51:56.361055Z"
  },
  {
    "question": "What criteria determine the folders tested in the `run_tests_gpu` job based on the `setup` job's output?",
    "answer": "name: Self-hosted runner (scheduled)\n\non:\n  repository_dispatch:\n  schedule:\n    - cron: \"0 2 * * *\"\n\nenv:\n  HF_HOME: /mnt/cache\n  TRANSFORMERS_IS_CI: yes\n  OMP_NUM_THREADS: 8\n  MKL_NUM_THREADS: 8\n  RUN_SLOW: yes\n  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n  TF_FORCE_GPU_ALLOW_GROWTH: true\n  RUN_PT_TF_CROSS_TESTS: 1\n\njobs:\n  setup:\n    name: Setup\n    strategy:\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: |\n          git fetch && git checkout ${{ github.sha }}\n\n      - name: Cleanup\n        working-directory: /transformers\n        run: |\n          rm -rf tests/__pycache__\n          rm -rf reports\n\n      - id: set-matrix\n        name: Identify models to test\n        working-directory: /transformers/tests\n        run: |\n          echo \"::set-output name=matrix::$(python3 -c 'import os; x = list(filter(os.path.isdir, os.listdir(os.getcwd()))); x.sort(); print(x)')\"\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: GPU visibility\n        working-directory: /transformers\n        run: |\n          utils/print_env_pt.py\n          TF_CPP_MIN_LOG_LEVEL=3 python3 -c \"import tensorflow as tf; print('TF GPUs available:', bool(tf.config.list_physical_devices('GPU')))\"\n          TF_CPP_MIN_LOG_LEVEL=3 python3 -c \"import tensorflow as tf; print('Number of TF GPUs available:', len(tf.config.list_physical_devices('GPU')))\"\n\n  run_tests_gpu:\n    name: Model tests\n    strategy:\n      fail-fast: false\n      matrix:\n        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Echo folder ${{ matrix.folders }}\n        run: echo \"${{ matrix.folders }}\"\n\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all non-slow tests on GPU\n        working-directory: /transformers\n        run: python3 -m pytest -v --make-reports=${{ matrix.machines }}_tests_gpu_${{ matrix.folders }} tests/${{ matrix.folders }}\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/${{ matrix.machines }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_all_tests_gpu_${{ matrix.folders }}_test_reports\n          path: /transformers/reports/${{ matrix.machines }}_tests_gpu_${{ matrix.folders }}\n\n  run_examples_gpu:\n    name: Examples directory\n    runs-on: [self-hosted, single-gpu-docker]\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run examples tests on GPU\n        working-directory: /transformers\n        run: |\n          pip install -r examples/pytorch/_tests_requirements.txt\n          python3 -m pytest -v --make-reports=examples_gpu examples/pytorch\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/examples_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_examples_gpu\n          path: /transformers/reports/examples_gpu\n\n  run_pipelines_torch_gpu:\n    name: PyTorch pipelines\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-pytorch-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all pipeline tests on GPU\n        working-directory: /transformers\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python3 -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=${{ matrix.machines }}_tests_torch_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/${{ matrix.machines }}_tests_torch_pipeline_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_torch_pipeline_gpu\n          path: /transformers/reports/${{ matrix.machines }}_tests_torch_pipeline_gpu\n\n  run_pipelines_tf_gpu:\n    name: TensorFlow pipelines\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-tensorflow-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: |\n          git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all pipeline tests on GPU\n        working-directory: /transformers\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python3 -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=${{ matrix.machines }}_tests_tf_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: |\n          cat /transformers/reports/${{ matrix.machines }}_tests_tf_pipeline_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_tf_pipeline_gpu\n          path: /transformers/reports/${{ matrix.machines }}_tests_tf_pipeline_gpu\n\n  run_all_tests_torch_cuda_extensions_gpu:\n    name: Torch CUDA extension tests\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    needs: setup\n    container:\n      image: huggingface/transformers-pytorch-deepspeed-latest-gpu\n      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Update clone\n        working-directory: /workspace/transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all tests on GPU\n        working-directory: /workspace/transformers\n        run: |\n          python -m pytest -v --make-reports=${{ matrix.machines }}_tests_torch_cuda_extensions_gpu tests/deepspeed tests/extended\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /workspace/transformers/reports/${{ matrix.machines }}_tests_torch_cuda_extensions_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_torch_cuda_extensions_gpu_test_reports\n          path: /workspace/transformers/reports/${{ matrix.machines }}_tests_torch_cuda_extensions_gpu\n\n\n  send_results:\n    name: Send results to webhook\n    runs-on: ubuntu-latest\n    if: always()\n    needs: [setup, run_tests_gpu, run_examples_gpu, run_pipelines_tf_gpu, run_pipelines_torch_gpu, run_all_tests_torch_cuda_extensions_gpu]\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/download-artifact@v2\n      - name: Send message to Slack\n        env:\n          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n        run: |\n          pip install slack_sdk\n          python utils/notification_service.py \"${{ needs.setup.outputs.matrix }}\"\n",
    "source": "bhargaviparanjape/robust-transformers",
    "path": ".github/workflows/self-scheduled.yml",
    "url": "https://github.com/bhargaviparanjape/robust-transformers/blob/7195cc405c3339d1ed58895cc0eecc229fa49d22/.github/workflows/self-scheduled.yml",
    "retrieved_at": "2025-08-15T01:51:57.297370Z"
  },
  {
    "question": "What triggers this workflow to run and check the status of endpoints?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "caliwyr/Software",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/caliwyr/Software/blob/b98328872a986013090c7f130f3cb9eb56c82fe2/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-15T01:51:57.912738Z"
  },
  {
    "question": "What triggers this workflow to run and deploy updates?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Updates CI\non:\n  schedule:\n    - cron: \"0 3 * * *\"\n  repository_dispatch:\n    types: [updates]\n  workflow_dispatch:\njobs:\n  release:\n    name: Deploy updates\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Update code\n        uses: upptime/updates@master\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n",
    "source": "caliwyr/Software",
    "path": ".github/workflows/updates.yml",
    "url": "https://github.com/caliwyr/Software/blob/b98328872a986013090c7f130f3cb9eb56c82fe2/.github/workflows/updates.yml",
    "retrieved_at": "2025-08-15T01:51:58.669791Z"
  },
  {
    "question": "Under what conditions will the \"Analytics App Tests\" workflow be triggered?",
    "answer": "name: Analytics App Tests\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'apps/analytics/**'\n      - '.github/workflows/**'\n  pull_request:\n    branches:\n      - main\n    paths:\n      - 'apps/analytics/**'\n      - '.github/workflows/**'\n\njobs:\n  run-smoke-test:\n    name: Smoke test\n    runs-on: ubuntu-latest\n    env:\n      TURBO_TOKEN: ${{ secrets.TURBO_TOKEN }}\n      TURBO_TEAM: ${{ secrets.TURBO_TEAM }}\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Install Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n\n      - uses: pnpm/action-setup@v2\n        name: Install pnpm\n        id: pnpm-install\n        with:\n          version: 8\n          run_install: false\n\n      - name: Get pnpm store directory\n        id: pnpm-cache\n        shell: bash\n        run: |\n          echo \"STORE_PATH=$(pnpm store path)\" >> $GITHUB_OUTPUT\n\n      - uses: actions/cache@v3\n        name: Setup pnpm cache\n        with:\n          path: |\n            ~/.cache/Cypress\n            ${{ steps.pnpm-cache.outputs.STORE_PATH }}\n          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}\n          restore-keys: |\n            ${{ runner.os }}-pnpm-store-\n\n      - name: Install dependencies\n        run: pnpm install\n\n      - name: Setup environment variables\n        run: |\n          touch apps/analytics/.env\n          echo DATABASE_URL=${{ secrets.DATABASE_URL }} >> apps/analytics/.env\n          echo HASH_SALT=${{ secrets.HASH_SALT }} >> apps/analytics/.env\n          echo MAXMIND_LICENSE_KEY=${{ secrets.MAXMIND_LICENSE_KEY }} >> apps/analytics/.env\n\n      - name: Build Prisma client\n        run: cd apps/analytics && pnpm run build-postgresql-client\n\n      - name: Build app\n        run: pnpm run build:analytics\n",
    "source": "ttttonyhe/ouorz-mono",
    "path": ".github/workflows/analytics-app-testing.yml",
    "url": "https://github.com/ttttonyhe/ouorz-mono/blob/19ccccede781c71f3a3c5c4bbfc27368cc43be03/.github/workflows/analytics-app-testing.yml",
    "retrieved_at": "2025-08-15T01:51:59.300015Z"
  },
  {
    "question": "Under what conditions are the \"-push-ci\" tagged images built and pushed to DockerHub?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "kssteven418/SqueezeLLM-gradients",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/kssteven418/SqueezeLLM-gradients/blob/5f2a16698b93cddddf858a78bef61fd5c6271055/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-15T01:52:00.230533Z"
  },
  {
    "question": "Under what conditions does the workflow execute `terraform apply -auto-approve`?",
    "answer": "# This workflow installs the latest version of Terraform CLI and configures the Terraform CLI configuration file\n# with an API token for Terraform Cloud (app.terraform.io). On pull request events, this workflow will run\n# `terraform init`, `terraform fmt`, and `terraform plan` (speculative plan via Terraform Cloud). On push events\n# to the main branch, `terraform apply` will be executed.\n#\n# Documentation for `hashicorp/setup-terraform` is located here: https://github.com/hashicorp/setup-terraform\n#\n# To use this workflow, you will need to complete the following setup steps.\n#\n# 1. Create a `main.tf` file in the root of this repository with the `remote` backend and one or more resources defined.\n#   Example `main.tf`:\n#     # The configuration for the `remote` backend.\n#     terraform {\n#       backend \"remote\" {\n#         # The name of your Terraform Cloud organization.\n#         organization = \"example-organization\"\n#\n#         # The name of the Terraform Cloud workspace to store Terraform state files in.\n#         workspaces {\n#           name = \"example-workspace\"\n#         }\n#       }\n#     }\n#\n#     # An example resource that does nothing.\n#     resource \"null_resource\" \"example\" {\n#       triggers = {\n#         value = \"A example resource that does nothing!\"\n#       }\n#     }\n#\n#\n# 2. Generate a Terraform Cloud user API token and store it as a GitHub secret (e.g. TF_API_TOKEN) on this repository.\n#   Documentation:\n#     - https://www.terraform.io/docs/cloud/users-teams-organizations/api-tokens.html\n#     - https://help.github.com/en/actions/configuring-and-managing-workflows/creating-and-storing-encrypted-secrets\n#\n# 3. Reference the GitHub secret in step using the `hashicorp/setup-terraform` GitHub Action.\n#   Example:\n#     - name: Setup Terraform\n#       uses: hashicorp/setup-terraform@v1\n#       with:\n#         cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\nname: 'Terraform'\n\non:\n  push:\n    branches:\n    - main\n  pull_request:\n\njobs:\n  terraform:\n    name: 'Terraform'\n    runs-on: ubuntu-latest\n    environment: production\n\n    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest\n    defaults:\n      run:\n        shell: bash\n\n    steps:\n    # Checkout the repository to the GitHub Actions runner\n    - name: Checkout\n      uses: actions/checkout@v2\n\n    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token\n    - name: Setup Terraform\n      uses: hashicorp/setup-terraform@v1\n      with:\n        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\n    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.\n    - name: Terraform Init\n      run: terraform init\n\n    # Checks that all Terraform configuration files adhere to a canonical format\n    - name: Terraform Format\n      run: terraform fmt -check\n\n    # Generates an execution plan for Terraform\n    - name: Terraform Plan\n      run: terraform plan\n\n      # On push to main, build or change infrastructure according to Terraform configuration files\n      # Note: It is recommended to set up a required \"strict\" status check in your repository for \"Terraform Cloud\". See the documentation on \"strict\" required status checks for more information: https://help.github.com/en/github/administering-a-repository/types-of-required-status-checks\n    - name: Terraform Apply\n      if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n      run: terraform apply -auto-approve\n",
    "source": "pabranch/sandbox-github-actions",
    "path": ".github/workflows/install-terraform.yml",
    "url": "https://github.com/pabranch/sandbox-github-actions/blob/b8607ccea20c8e2fa5c4b73374108404710003de/.github/workflows/install-terraform.yml",
    "retrieved_at": "2025-08-15T01:52:00.838262Z"
  },
  {
    "question": "Under what conditions does the workflow execute, given the `on` trigger configuration?",
    "answer": "name: rAPId Dev Deployment\n\non:\n  push:\n    branches:\n      - '**'\n\n  workflow_dispatch:\n\njobs:\n  setup:\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Log commit SHA\n        run: echo $GITHUB_SHA\n\n  security-check:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - run: pip install -r requirements.txt\n\n      - name: Run security checks\n        run: make security-check\n\n  api-dev:\n    needs:\n      - setup\n      - security-check\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          cp ./.github/.github.env .env\n          echo AWS_ACCOUNT=${{ secrets.AWS_ACCOUNT }} >> .env\n          echo AWS_REGION=${{ secrets.AWS_REGION }} >> .env\n          echo AWS_DEFAULT_REGION=${{ secrets.AWS_REGION }} >> .env\n\n      - name: Build API Image\n        run: make api/create-image\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - name: Setup API environment\n        run: make api/setup\n\n      - name: API Static Analysis\n        run: make api/lint\n\n      - name: API Tests\n        run: make api/test\n\n      - name: API Tag and Upload\n        run: make api/tag-and-upload\n\n  sdk-dev:\n    needs:\n      - setup\n      - security-check\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          echo \"TWINE_USERNAME=${{ secrets.TWINE_USERNAME_TEST }}\" >> .env\n          echo \"TWINE_PASSWORD=${{ secrets.TWINE_PASSWORD_TEST }}\" >> .env\n          echo \"TWINE_NON_INTERACTIVE=true\" >> .env\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - name: Setup Python Environment\n        run: |\n          make sdk/setup\n\n      - name: SDK Test\n        run: make sdk/test\n\n      - name: Set env variable\n        run: echo \"TEST_SDK_VERSION=$(date +%Y%m%d%H%M%S)\" >> $GITHUB_ENV\n\n      - name: SDK Test Deploy\n        run: make sdk/release-test\n\n  ui-dev:\n    needs:\n      - setup\n      - security-check\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20\n\n      - name: Install UI Packages\n        run: make ui/setup\n\n      - name: UI Test\n        run: make ui/test\n\n  cleanup:\n    needs:\n      - setup\n      - security-check\n      - api-dev\n      - sdk-dev\n      - ui-dev\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Clean Docker Context\n        if: always()\n        run: make api/clean-docker\n",
    "source": "no10ds/rapid",
    "path": ".github/workflows/dev.yml",
    "url": "https://github.com/no10ds/rapid/blob/c763da3d92b4371effa90f6df460c4d027b899c6/.github/workflows/dev.yml",
    "retrieved_at": "2025-08-15T01:52:01.597911Z"
  },
  {
    "question": "Under what conditions does the `setup` job run?",
    "answer": "name: rAPId Release\n\non:\n  release:\n    types: [released]\n\njobs:\n  setup:\n    if: \"${{ startsWith(github.event.release.name, 'API: ') }}\"\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Log commit SHA\n        run: echo $GITHUB_SHA\n\n  api-release:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          cp ./.github/.github.env .env\n          echo AWS_ACCOUNT=${{ secrets.AWS_ACCOUNT }} >> .env\n          echo AWS_REGION=${{ secrets.AWS_REGION }} >> .env\n\n      - name: Build API Image\n        run: make api/create-image\n\n      - name: API Tag and Upload Release Image\n        run: make api/tag-and-upload-release-image\n\n  ui-release:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20\n\n      - name: Install UI Packages\n        run: make ui/setup\n\n      - name: UI Build Static Files\n        run: make ui/create-static-out\n\n      - name: UI Zip and Release\n        env:\n          TAG: ${{ github.event.release.tag_name }}\n          GH_TOKEN: ${{ github.token }}\n        run: make ui/zip-and-release tag=$TAG\n\n  cleanup:\n    needs:\n      - setup\n      - api-release\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Clean Docker Context\n        if: always()\n        run: make api/clean-docker\n",
    "source": "no10ds/rapid",
    "path": ".github/workflows/release_api.yml",
    "url": "https://github.com/no10ds/rapid/blob/c763da3d92b4371effa90f6df460c4d027b899c6/.github/workflows/release_api.yml",
    "retrieved_at": "2025-08-15T01:52:02.117771Z"
  },
  {
    "question": "On what branches will this workflow run when a push or pull request is made?",
    "answer": "name: Clippy\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - dev\n\njobs:\n  clippy:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y webkit2gtk-4.0\n\n      - name: Install clippy with stable toolchain\n        uses: dtolnay/rust-toolchain@stable\n        with:\n          components: clippy\n\n      - uses: Swatinem/rust-cache@v2\n\n      - run: cargo clippy --manifest-path=Cargo.toml --all-targets --all-features -- -D warnings\n",
    "source": "bagindo/tauri-plugin-keygen",
    "path": ".github/workflows/clippy.yml",
    "url": "https://github.com/bagindo/tauri-plugin-keygen/blob/e0de03a76a5c82c36adb347e43e4bff4f78c87de/.github/workflows/clippy.yml",
    "retrieved_at": "2025-08-15T01:52:02.822724Z"
  },
  {
    "question": "What Docker images are built and pushed by this workflow, and under what conditions does the workflow trigger?",
    "answer": "name: push-poetry-container\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}-ci\n  POETRY_VERSION: 1.3.2\n  PYTHON_PRIMARY_VERSION: 3.10.15\n  PYTHON_PRIMARY_TAG: py310\n  PYTHON_SECONDARY_VERSION: 3.12.4\n  PYTHON_SECONDARY_TAG: py312\n  DEBIAN_VERSION: bullseye\n\non:\n  push:\n    paths:\n      - .devcontainer/Dockerfile\n    branches:\n      - devel\n  workflow_dispatch:\n\njobs:\n  push-poetry-container:\n    runs-on: ubuntu-24.04\n    permissions: \n      contents: read\n      packages: write \n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n    - name: Login to GCR\n      uses: docker/login-action@v3\n      with:\n        registry: ghcr.io\n        username: ${{ github.actor }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n    - name: Metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n    - name: Echo\n      run: |\n        echo \"USER: ${{ github.actor }}\"\n        echo \"REPOSITORY: ${{ github.repository }}\"\n        echo \"POETRY_VERSION: ${POETRY_VERSION}\"\n        echo \"PYTHON_PRIMARY_VERSION: ${PYTHON_PRIMARY_VERSION}\"\n        echo \"PYTHON_SECONDARY_VERSION: ${PYTHON_SECONDARY_VERSION}\"\n        echo \"TAGS: ${{ steps.meta.outputs.tags }}\"\n        echo \"LABELS: ${{ steps.meta.outputs.labels }}\"\n    - name: Image - poetry${{ env.POETRY_VERSION }}-python${{ env.PYTHON_PRIMARY_VERSION }}\n      uses: docker/build-push-action@v6\n      with:\n        file: ./.devcontainer/Dockerfile\n        push: true\n        build-args: |\n          PYTHON_VERSION=${{ env.PYTHON_PRIMARY_VERSION }}\n          POETRY_VERSION=${{ env.POETRY_VERSION }}\n          DEBIAN_VERSION=${{ env.DEBIAN_VERSION }}\n        tags: |\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.PYTHON_PRIMARY_TAG }}-poetry-${{ env.DEBIAN_VERSION }}\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:python${{ env.PYTHON_PRIMARY_VERSION }}-poetry${{ env.POETRY_VERSION }}-${{ env.DEBIAN_VERSION }}\n        labels: ${{ steps.meta.outputs.labels }}\n    - name: Image - poetry${{ env.POETRY_VERSION }}-python${{ env.PYTHON_SECONDARY_VERSION }}\n      uses: docker/build-push-action@v6\n      with:\n        file: ./.devcontainer/Dockerfile\n        push: true\n        build-args: |\n          PYTHON_VERSION=${{ env.PYTHON_SECONDARY_VERSION }}\n          POETRY_VERSION=${{ env.POETRY_VERSION }}\n          DEBIAN_VERSION=${{ env.DEBIAN_VERSION }}\n        tags: |\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.PYTHON_SECONDARY_TAG }}-poetry-${{ env.DEBIAN_VERSION }}\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:python${{ env.PYTHON_SECONDARY_VERSION }}-poetry${{ env.POETRY_VERSION }}-${{ env.DEBIAN_VERSION }}\n        labels: ${{ steps.meta.outputs.labels }}",
    "source": "splintered-reality/py_trees",
    "path": ".github/workflows/push_poetry_container.yaml",
    "url": "https://github.com/splintered-reality/py_trees/blob/d998975a302f0466405aaf7aef185ff5a4a57e3f/.github/workflows/push_poetry_container.yaml",
    "retrieved_at": "2025-08-16T01:46:30.022488Z"
  },
  {
    "question": "How does the workflow ensure consistent versioning across different platforms and build configurations?",
    "answer": "name: Build\n\non: [push, pull_request]\n\njobs:\n  Build:\n    name: ${{ matrix.platform.name }}\n    runs-on: ${{ matrix.platform.os }}\n\n    defaults:\n      run:\n        shell: ${{ matrix.platform.shell }}\n\n    strategy:\n      fail-fast: false\n      matrix:\n        platform:\n        - { name: Windows (mingw32),        os: windows-latest, shell: 'msys2 {0}', msystem: mingw32, msys-env: mingw-w64-i686 }\n        - { name: Windows (mingw64),        os: windows-latest, shell: 'msys2 {0}', msystem: mingw64, msys-env: mingw-w64-x86_64 }\n        - { name: Windows (clang32),        os: windows-latest, shell: 'msys2 {0}', msystem: clang32, msys-env: mingw-w64-clang-i686 }\n        - { name: Windows (clang64),        os: windows-latest, shell: 'msys2 {0}', msystem: clang64, msys-env: mingw-w64-clang-x86_64 }\n        - { name: Windows (ucrt64),         os: windows-latest, shell: 'msys2 {0}', msystem: ucrt64,  msys-env: mingw-w64-ucrt-x86_64 }\n        - { name: Ubuntu 20.04 (CMake),     os: ubuntu-20.04,   shell: sh }\n        - { name: Ubuntu 20.04 (autotools), os: ubuntu-20.04,   shell: sh,    autotools: true }\n        - { name: Ubuntu 22.04 (CMake),     os: ubuntu-22.04,   shell: sh }\n        - { name: Ubuntu 22.04 (autotools), os: ubuntu-22.04,   shell: sh,    autotools: true }\n        - { name: MacOS (CMake),            os: macos-latest,   shell: sh,    cmake: '-DCMAKE_OSX_ARCHITECTURES=\"x86_64;arm64\"' }\n        - { name: MacOS (autotools),        os: macos-latest,   shell: sh,    autotools: true }\n\n    steps:\n    - name: Set up MSYS2\n      if: matrix.platform.shell == 'msys2 {0}'\n      uses: msys2/setup-msys2@v2\n      with:\n        msystem: ${{ matrix.platform.msystem }}\n        install: >-\n          ${{ matrix.platform.msys-env }}-cc\n          ${{ matrix.platform.msys-env }}-cmake\n          ${{ matrix.platform.msys-env }}-ninja\n          ${{ matrix.platform.msys-env }}-pkg-config\n\n    - name: Setup Linux dependencies\n      if: runner.os == 'Linux'\n      run: |\n        sudo apt-get update\n        sudo apt-get install build-essential git make autoconf automake libtool \\\n            pkg-config cmake ninja-build gnome-desktop-testing libasound2-dev libpulse-dev \\\n            libaudio-dev libjack-dev libsndio-dev libsamplerate0-dev libx11-dev libxext-dev \\\n            libxrandr-dev libxcursor-dev libxfixes-dev libxi-dev libxss-dev libwayland-dev \\\n            libxkbcommon-dev libdrm-dev libgbm-dev libgl1-mesa-dev libgles2-mesa-dev \\\n            libegl1-mesa-dev libdbus-1-dev libibus-1.0-dev libudev-dev fcitx-libs-dev\n\n    - name: Setup extra Ubuntu 22.04 dependencies\n      if: matrix.platform.os == 'ubuntu-22.04'\n      run: |\n        sudo apt-get install libpipewire-0.3-dev libdecor-0-dev\n\n    - name: Setup Macos dependencies\n      if: runner.os == 'macOS'\n      run: |\n        brew install \\\n          ninja\n    - uses: actions/checkout@v3\n    - name: Check that versioning is consistent\n      # We only need to run this once: arbitrarily use the Linux/CMake build\n      if: \"runner.os == 'Linux' && ! matrix.platform.autotools\"\n      run: ./build-scripts/test-versioning.sh\n    - name: Configure (CMake)\n      if: \"! matrix.platform.autotools\"\n      run: |\n        cmake -S . -B build -G Ninja \\\n        -DSDL_TESTS=ON \\\n        -DSDL_WERROR=ON \\\n        -DSDL_INSTALL_TESTS=ON \\\n        -DSDL_VENDOR_INFO=\"Github Workflow\" \\\n        -DCMAKE_INSTALL_PREFIX=cmake_prefix \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        ${{ matrix.platform.cmake }}\n    - name: Build (CMake)\n      if: \"! matrix.platform.autotools\"\n      run: |\n        cmake --build build/ --config Release --verbose --parallel\n    - name: Run build-time tests (CMake)\n      if: \"! matrix.platform.autotools\"\n      run: |\n        set -eu\n        export SDL_TESTS_QUICK=1\n        ctest -VV --test-dir build/\n        if test \"${{ runner.os }}\" = \"Linux\"; then\n          # This should show us the SDL_REVISION\n          strings build/libSDL2-2.0.so.0 | grep SDL-\n        fi\n    - name: Install (CMake)\n      if: \"! matrix.platform.autotools\"\n      run: |\n        set -eu\n        cmake --install build/ --config Release\n        echo \"SDL2_DIR=$(pwd)/cmake_prefix\" >> $GITHUB_ENV\n        ( cd cmake_prefix; find ) | LC_ALL=C sort -u\n    - name: Configure (Autotools)\n      if: matrix.platform.autotools\n      run: |\n        set -eu\n        rm -fr build-autotools\n        mkdir build-autotools\n        ./autogen.sh\n        (\n          cd build-autotools\n          ${{ github.workspace }}/configure \\\n            --enable-vendor-info=\"Github Workflow\" \\\n            --enable-werror \\\n            --prefix=${{ github.workspace }}/autotools_prefix \\\n        )\n        if test \"${{ runner.os }}\" != \"macOS\" ; then\n          curdir=\"$(pwd)\"\n          multiarch=\"$(dpkg-architecture -qDEB_HOST_MULTIARCH)\"\n          (\n            mkdir -p build-autotools/test\n            cd build-autotools/test\n            ${{ github.workspace }}/test/configure \\\n              --enable-werror \\\n              --x-includes=/usr/include \\\n              --x-libraries=\"/usr/lib/${multiarch}\" \\\n              --prefix=${{ github.workspace }}/autotools_prefix \\\n              SDL_CFLAGS=\"-I${curdir}/include\" \\\n              SDL_LIBS=\"-L${curdir}/build-autotools/build/.libs -lSDL2\" \\\n              ac_cv_lib_SDL2_ttf_TTF_Init=no \\\n              ${NULL+}\n          )\n        fi\n    - name: Build (Autotools)\n      if: matrix.platform.autotools\n      run: |\n        set -eu\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        make -j\"${parallel}\" -C build-autotools V=1\n        if test \"${{ runner.os }}\" != \"macOS\" ; then\n          make -j\"${parallel}\" -C build-autotools/test V=1\n        fi\n    - name: Run build-time tests (Autotools)\n      if: ${{ matrix.platform.autotools && (runner.os != 'macOS') }}\n      run: |\n        set -eu\n        curdir=\"$(pwd)\"\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        export SDL_TESTS_QUICK=1\n        make -j\"${parallel}\" -C build-autotools/test check LD_LIBRARY_PATH=\"${curdir}/build-autotools/build/.libs\"\n        if test \"${{ runner.os }}\" = \"Linux\"; then\n          # This should show us the SDL_REVISION\n          strings \"${curdir}/build-autotools/build/.libs/libSDL2-2.0.so.0\" | grep SDL-\n        fi\n    - name: Install (Autotools)\n      if: matrix.platform.autotools\n      run: |\n        set -eu\n        curdir=\"$(pwd)\"\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        make -j\"${parallel}\" -C build-autotools install V=1\n        if test \"${{ runner.os }}\" != \"macOS\" ; then\n          make -j\"${parallel}\" -C build-autotools/test install V=1\n        fi\n        ( cd autotools_prefix; find . ) | LC_ALL=C sort -u\n        echo \"SDL2_DIR=$(pwd)/autotools_prefix\" >> $GITHUB_ENV\n    - name: Verify CMake configuration files\n      run: |\n        cmake -S cmake/test -B cmake_config_build -G Ninja \\\n          -DCMAKE_BUILD_TYPE=Release \\\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }}\n        cmake --build cmake_config_build --verbose\n    - name: Verify sdl2-config\n      run: |\n        export PATH=${{ env.SDL2_DIR }}/bin:$PATH\n        cmake/test/test_sdlconfig.sh\n    - name: Verify sdl2.pc\n      run: |\n        export PKG_CONFIG_PATH=${{ env.SDL2_DIR }}/lib/pkgconfig\n        cmake/test/test_pkgconfig.sh\n    - name: Distcheck (Autotools)\n      if: matrix.platform.autotools\n      run: |\n        set -eu\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        make -j\"${parallel}\" -C build-autotools dist V=1\n        # Similar to Automake `make distcheck`: check that the tarball\n        # release is sufficient to do a new build\n        mkdir distcheck\n        tar -C distcheck -zxf build-autotools/SDL2-*.tar.gz\n        ( cd distcheck/SDL2-* && ./configure )\n        make -j\"${parallel}\" -C distcheck/SDL2-*\n    - name: Run installed-tests (Autotools)\n      if: \"runner.os == 'Linux' && matrix.platform.autotools\"\n      run: |\n        set -eu\n        parallel=\"$(getconf _NPROCESSORS_ONLN)\"\n        sudo make -j\"${parallel}\" -C build-autotools install\n        sudo make -j\"${parallel}\" -C build-autotools/test install\n        export SDL_TESTS_QUICK=1\n        # We need to set LD_LIBRARY_PATH because it isn't in the default\n        # linker search path. We don't need to set XDG_DATA_DIRS for\n        # ginsttest-runner, because /usr/local/share *is* in the default\n        # search path for that.\n        env --chdir=/ \\\n            LD_LIBRARY_PATH=/usr/local/lib \\\n            SDL_AUDIODRIVER=dummy \\\n            SDL_VIDEODRIVER=dummy \\\n            ginsttest-runner --tap SDL2\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/main.yml",
    "retrieved_at": "2025-08-16T01:46:30.986219Z"
  },
  {
    "question": "What triggers this workflow to run, and what specific event type is it looking for?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "Guizimo/d2-manage-client",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/Guizimo/d2-manage-client/blob/260866da20e5e876de825e6e3fadc8b5b7736ce6/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-16T01:46:31.623756Z"
  },
  {
    "question": "What specific action does the `notify_simulator_failure.py` script perform using the provided environment variables and payload data?",
    "answer": "name: Notify Notifications chat\n\non:\n  repository_dispatch:\n    types: [notify-simulator-failure]\n\njobs:\n  send-chat-message:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v2\n\n      - name: Set up JDK 17.0.7\n        uses: actions/setup-java@v2\n        with:\n          distribution: 'temurin'\n          java-version: '17.0.7'\n\n      - name: Set up Python 3.x\n        uses: actions/setup-python@v1\n        with:\n          python-version: '3.x'\n\n      - name: Install Python packages\n        run: |\n          pip install cryptography\n          pip install httplib2\n          pip install PyGithub\n\n      - name: Wget required files\n        run: |\n          python3 dependabot/notify_simulator_failure.py ${{ github.event.client_payload.branch }} ${{ github.event.client_payload.runId }} \n        env:\n          CHAT_ID: ${{ secrets.NOTIFICATIONS_CHAT_ID }}\n          CHAT_KEY: ${{ secrets.NOTIFICATIONS_CHAT_KEY }}\n          CHAT_TOKEN: ${{ secrets.NOTIFICATIONS_CHAT_TOKEN }}\n          ENV_USER_ENCRYPTION_KEY: ${{secrets.USER_ENCRYPTION_KEY}}\n          BALLERINA_BOT_TOKEN: ${{ secrets.BALLERINA_BOT_TOKEN }}\n",
    "source": "ballerina-platform/ballerina-release",
    "path": ".github/workflows/notify-simulator-failure.yml",
    "url": "https://github.com/ballerina-platform/ballerina-release/blob/fe827e460fa2d4cf8bd36a15fe14035fea3c78a2/.github/workflows/notify-simulator-failure.yml",
    "retrieved_at": "2025-08-16T01:46:32.516931Z"
  },
  {
    "question": "What specific static checks and security checks are performed by the `staticcheck` job?",
    "answer": "on:\n  push:\n    branches:\n      - 'master'\n      - 'develop'\nname: push_master_develop\njobs:\n  staticcheck:\n    runs-on: ubuntu-latest\n    steps:\n    - name: install Go\n      uses: actions/setup-go@v2\n      with:\n        go-version: 1.18.x\n    - name: checkout code\n      uses: actions/checkout@v2\n      with:\n        fetch-depth: 0\n    - uses: actions/cache@v2\n      with:\n        path: |\n          ~/go/pkg/mod\n          ~/.cache/go-build\n          ~/Library/Caches/go-build\n          %LocalAppData%\\go-build\n        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}\n        restore-keys: |\n          ${{ runner.os }}-go-\n    - name: install deps\n      run: go install golang.org/x/tools/cmd/goimports@latest && go install github.com/klauspost/asmfmt/cmd/asmfmt@latest\n    - name: gofmt\n      run: if [[ -n $(gofmt -l .) ]]; then echo \"please run gofmt\"; exit 1; fi\n    - name: go vet\n      run: go vet ./...\n    - name: staticcheck\n      run: |\n        go install honnef.co/go/tools/cmd/staticcheck@23e1086441d24fed9f668ad1cd4374245118b590\n        staticcheck ./...\n    - name: gosec\n      run: |\n        go install github.com/securego/gosec/v2/cmd/gosec@latest\n        gosec -exclude G204 ./...\n    - name: generated files should not be modified\n      run: |\n        go generate ./...\n        git update-index --assume-unchanged go.mod\n        git update-index --assume-unchanged go.sum\n        if [[ -n $(git status --porcelain) ]]; then echo \"git repo is dirty after runing go generate -- please don't modify generated files\"; echo $(git diff);echo $(git status --porcelain); exit 1; fi\n  \n  test:\n    strategy:\n      matrix:\n        go-version: [1.17.x, 1.18.x]\n        os: [ubuntu-latest, windows-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    needs:\n      - staticcheck\n    steps:\n    - name: install Go\n      uses: actions/setup-go@v2\n      with:\n        go-version: ${{ matrix.go-version }}\n    - name: checkout code\n      uses: actions/checkout@v2\n    - uses: actions/cache@v2\n      with:\n        path: |\n          ~/go/pkg/mod\n          ~/.cache/go-build\n          ~/Library/Caches/go-build\n          %LocalAppData%\\go-build\n        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}\n        restore-keys: |\n          ${{ runner.os }}-go-\n    - name: install deps\n      run: go install golang.org/x/tools/cmd/goimports@latest && go install github.com/klauspost/asmfmt/cmd/asmfmt@latest\n    - name: Test\n      run: |\n        go test -v -timeout=30m ./...\n    - name: Test (race)\n      if: matrix.os == 'ubuntu-latest'\n      run: |\n        go test -v -timeout=50m -race -short ./...\n  \n  slack-workflow-status:\n    if: always()\n    name: post workflow status to slack\n    needs:\n      - staticcheck\n      - test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Build notification\n        uses: Gamesight/slack-workflow-status@master\n        with:\n          repo_token: ${{secrets.GITHUB_TOKEN}}\n          slack_webhook_url: ${{secrets.SLACK_WEBHOOK_URL}}\n          channel: '#team-gnark-build'",
    "source": "dreamATD/pianist-gnark",
    "path": ".github/workflows/push.yml",
    "url": "https://github.com/dreamATD/pianist-gnark/blob/5685973eda77c38fbf2a4d58168e4ec531474b86/.github/workflows/push.yml",
    "retrieved_at": "2025-08-16T01:46:33.214967Z"
  },
  {
    "question": "Under what conditions will the \"Snifftest\" workflow be triggered?",
    "answer": "name: Snifftest\n\non:\n  push:\n    tags:\n      - v*\n    branches:\n      - main\n\npermissions:\n  contents: read\n  pull-requests: read\n\njobs:\n  Snifftest:\n    name: Run Snifftest\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/setup-go@v5\n        with:\n          go-version: \"1.22\"\n      - uses: actions/checkout@v4\n      - name: Run Snifftest\n        run: make snifftest\n",
    "source": "Marshal-EASM/webssi",
    "path": ".github/workflows/snifftest.yml",
    "url": "https://github.com/Marshal-EASM/webssi/blob/25053d6b6cd204954d4caf23a5fabbca2d8324b0/.github/workflows/snifftest.yml",
    "retrieved_at": "2025-08-16T01:46:33.998778Z"
  },
  {
    "question": "Under what conditions will this workflow automatically approve a pull request?",
    "answer": "name: Auto approve\n\non:\n  pull_request_target:\n    types: [labeled]\n\njobs:\n  # Auto-approve dependabot PRs since this repo requires at least one approving review.\n  # Dependabot will automatically merge minor version upgrades\n  # (see .dependabot/config.yml for more info).\n  auto-approve-dependabot:\n    runs-on: ubuntu-latest\n    if: github.actor == 'dependabot[bot]' && contains(github.event.pull_request.labels.*.name, 'dependencies')\n    steps:\n      - uses: hmarr/auto-approve-action@v2.1.0\n        with:\n          github-token: \"${{ secrets.GITHUB_TOKEN }}\"\n",
    "source": "aws-actions/action-cloudwatch-metrics",
    "path": ".github/workflows/autoapprove.yml",
    "url": "https://github.com/aws-actions/action-cloudwatch-metrics/blob/36cfe8e2237e43b1129fbd3807621508657ff430/.github/workflows/autoapprove.yml",
    "retrieved_at": "2025-08-16T01:46:34.711437Z"
  },
  {
    "question": "What tracking methods and YOLO models are tested in the \"Tests all tracking options\" step?",
    "answer": "# name of the workflow, what it is doing (optional)\nname: CI CPU testing\n\n# events that trigger the workflow (required)\non:\n  push:\n    branches: [master, CIdebug]\n  pull_request:\n    # pull request where master is target\n    branches: [master]\n\nenv:\n  # Directory of PyPi package to be tested\n  PACKAGE_DIR: boxmot\n  # Minimum acceptable test coverage\n  # Increase as you add more tests to increase coverage\n  COVERAGE_FAIL_UNDER: 29\n\n# the workflow that gets triggerd\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]   # skip windows-latest for\n        python-version: ['3.8', '3.9', '3.10']\n        #model: ['yolov8n', 'yolo_nas_s', yolox_n]  # yolo models to test\n        #tracking-methods: ['deepocsort', 'ocsort', 'botsort', 'strongsort', 'bytetrack']  # tracking methods to  test\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 50\n    steps:\n\n      - uses: actions/checkout@v4  # Check out the repository\n      - uses: actions/setup-python@v4  # Prepare environment with python 3.9\n        with:\n          python-version: ${{ matrix.python-version }}\n          cache: 'pip' # caching pip dependencies\n      - name: Install requirements\n        shell: bash  # for Windows compatibility\n        run: |\n          python -m pip install --upgrade pip setuptools wheel\n          pip install -e . pytest pytest-cov --extra-index-url https://download.pytorch.org/whl/cpu\n          python --version\n          pip --version\n          pip list\n\n      - name: Tests all tracking options\n        shell: bash  # for Windows compatibility\n        env:\n          IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n        run: |\n          # deepocsort fro all supported yolo models\n          python examples/track.py --tracking-method deepocsort --source $IMG --imgsz 320 --reid-model examples/weights/clip_market1501.pt\n          python examples/track.py --yolo-model yolo_nas_s --tracking-method deepocsort --source $IMG --imgsz 320\n          python examples/track.py --yolo-model yolox_n --tracking-method deepocsort --source $IMG --imgsz 320\n\n          # hybridsort\n          python examples/track.py --tracking-method hybridsort --source $IMG --imgsz 320\n\n          # botsort\n          python examples/track.py --tracking-method botsort --source $IMG --imgsz 320\n\n          # strongsort\n          python examples/track.py --tracking-method strongsort --source $IMG --imgsz 320\n\n          # ocsort\n          python examples/track.py --tracking-method ocsort --source $IMG --imgsz 320\n\n          # bytetrack\n          python examples/track.py --tracking-method bytetrack --source $IMG --imgsz 320\n\n      - name: Pytest tests  # after tracking options as this does not download models\n        shell: bash  # for Windows compatibility\n        run: |\n\n          # needed in TFLite export\n          wget https://github.com/PINTO0309/onnx2tf/releases/download/1.7.3/flatc.tar.gz\n          tar -zxvf flatc.tar.gz\n          sudo chmod +x flatc\n          sudo mv flatc /usr/bin/\n\n          pytest --cov=$PACKAGE_DIR --cov-report=html -v tests\n          coverage report --fail-under=$COVERAGE_FAIL_UNDER\n\n      - name: Tests exported reid models\n        env:\n            IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n        shell: bash  # for Windows compatibility\n        run: |\n\n          # test exported reid model\n          python examples/track.py --reid-model examples/weights/osnet_x0_25_msmt17.torchscript                                   --source $IMG --imgsz 320\n          python examples/track.py --reid-model examples/weights/osnet_x0_25_msmt17.onnx                                          --source $IMG --imgsz 320\n          python examples/track.py --reid-model examples/weights/osnet_x0_25_msmt17_saved_model/osnet_x0_25_msmt17_float16.tflite --source $IMG --imgsz 320\n          python examples/track.py --reid-model examples/weights/osnet_x0_25_msmt17_openvino_model                                --source $IMG --imgsz 320\n\n      - name: Test tracking with seg models\n        env:\n            IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n        shell: bash  # for Windows compatibility\n        run: |\n          # tracking with SEG models\n          python examples/track.py --tracking-method deepocsort --yolo-model yolov8n-seg.pt --source $IMG\n\n      - name: Test tracking with pose models\n        env:\n          IMG: ./assets/MOT17-mini/train/MOT17-05-FRCNN/img1/000001.jpg\n        shell: bash  # for Windows compatibility\n        run: |\n          # tracking with POSE models\n          python3 examples/track.py --yolo-model weights/yolov8n.pt --source $IMG --imgsz 320\n\n      - name: Test validation on MOT17 subset\n        shell: bash  # for Windows compatibility\n        run: |\n          # validation on a few MOT17 imges\n          python examples/val.py --tracking-method deepocsort --yolo-model yolov8n.pt --benchmark MOT17-mini --imgsz 320 --conf 0.2\n\n      - name: Test evolution on MOT17 subset\n        shell: bash  # for Windows compatibility\n        run: |\n          # evolve a for a single set of parameters\n          python examples/evolve.py --objectives HOTA,MOTA,IDF1 --benchmark MOT17-mini --n-trials 1 --imgsz 320 --conf 0.2\n",
    "source": "rustoneee/Yolo-Tracking",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/rustoneee/Yolo-Tracking/blob/a64e0c5886927c3c3d6080e50a5cffd63a343e52/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-16T01:46:35.410384Z"
  },
  {
    "question": "What platforms are targeted by this workflow and what artifacts are produced for each?",
    "answer": "# This is a basic workflow to help you get started with Actions\n\nname: Build\n\n# Controls when the action will run.\non:\n  # Triggers the workflow on push or pull request events but only for the master branch\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n  buildLinux:\n    # The type of runner that the job will run on\n    runs-on: ubuntu-latest\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      - uses: krdlab/setup-haxe@master\n        with:\n          haxe-version: 4.2.5\n      # Runs a set of commands using the runners shell\n      - name: Install Haxelib\n        run: |\n          sudo apt-get install libvlc-dev\n          sudo apt-get install libvlccore-dev\n          haxelib setup ~/haxelib\n          haxelib install hxcpp > /dev/null\n          haxelib install lime\n          haxelib install openfl\n          haxelib --never install flixel\n          haxelib run lime setup flixel\n          haxelib run lime setup\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install flixel-addons\n          haxelib install tjson\n          haxelib install hxjsonast\n          haxelib install hxCodec\n          haxelib git linc_luajit https://github.com/nebulazorua/linc_luajit\n          haxelib install hscript\n          haxelib git hscript-ex https://github.com/ianharrigan/hscript-ex\n          haxelib git discord_rpc https://github.com/Aidan63/linc_discord-rpc\n          haxelib install hxcpp-debug-server\n          haxelib list\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n      - name: Compile\n        run: haxelib run lime build Project.xml linux --app-version=\"4.0.0-${{ github.run_id}}\"\n      - name: Publish Artifact\n        uses: actions/upload-artifact@v2.2.4\n        with:\n          name: linuxBuild\n          path: 'export/release/linux/bin'\n  buildWindows:\n    runs-on: windows-latest\n\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2.3.0\n\n      - uses: krdlab/setup-haxe@master\n        with:\n          haxe-version: 4.2.5\n      # Runs a set of commands using the runners shell\n      - name: Install Haxelib\n        run: |\n          haxelib setup C:/haxelib\n          haxelib install hxcpp > nul\n          haxelib install lime\n          haxelib install openfl\n          haxelib --never install flixel\n          haxelib run lime setup flixel\n          haxelib run lime setup\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install flixel-addons\n          haxelib install tjson\n          haxelib install hxjsonast\n          haxelib install hxCodec\n          haxelib git linc_luajit https://github.com/nebulazorua/linc_luajit\n          haxelib install hscript\n          haxelib git hscript-ex https://github.com/ianharrigan/hscript-ex\n          haxelib git discord_rpc https://github.com/Aidan63/linc_discord-rpc\n          haxelib install hxcpp-debug-server\n          haxelib list\n        shell: cmd\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n      - name: Compile\n        run: haxelib run lime build windows --app-version=\"4.0.0-${{ github.run_id}}\"\n      - name: Publish Artifact\n        uses: actions/upload-artifact@v2.2.4\n        with:\n          name: windowsBuild\n          path: export/release/windows/bin\n  buildMac:\n    runs-on: macos-latest\n\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      - uses: krdlab/setup-haxe@master\n        with:\n          haxe-version: 4.2.5\n      # Runs a set of commands using the runners shell\n      - name: Install Haxelib\n        run: |\n          haxelib setup ~/haxelib\n          haxelib install hxcpp > /dev/null\n          haxelib install lime\n          haxelib install openfl\n          haxelib --never install flixel\n          haxelib run lime setup flixel\n          haxelib run lime setup\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install flixel-addons\n          haxelib install tjson\n          haxelib install hxjsonast\n          haxelib install hxCodec\n          haxelib git linc_luajit https://github.com/nebulazorua/linc_luajit\n          haxelib install hscript\n          haxelib git hscript-ex https://github.com/ianharrigan/hscript-ex\n          haxelib git discord_rpc https://github.com/Aidan63/linc_discord-rpc\n          haxelib install hxcpp-debug-server\n          haxelib list\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n      - name: Compile\n        run: haxelib run lime build mac --app-version=\"4.0.0-${{ github.run_id}}\"\n      - name: Publish Artifact\n        uses: actions/upload-artifact@v2.2.4\n        with:\n          name: macBuild\n          path: export/release/macos/bin\n  buildAndroid:\n    name: buildAndroid\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.1\n\n      - name: Setup Android NDK\n        uses: nttld/setup-ndk@v1\n        id: setup-ndk\n        with:\n          ndk-version: r21e\n\n      - name: Setup Java JDK\n        uses: actions/setup-java@v1\n        with:\n          java-version: 11\n\n      - name: Setup Android SDK\n        uses: android-actions/setup-android@v2\n\n      - name: Setup Haxe\n        uses: krdlab/setup-haxe@v1.1.5\n        with:\n          haxe-version: 4.2.5\n\n      - name: Install Haxelib\n        run: |\n          haxelib setup ~/haxelib\n          haxelib install hxcpp > /dev/null\n          haxelib install lime\n          haxelib install openfl\n          haxelib --never install flixel\n          haxelib run lime setup flixel\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install hscript\n          haxelib install flixel-addons\n          haxelib install hxCodec\n          haxelib git linc_luajit https://github.com/jigsaw-4277821/linc_luajit.git\n          haxelib git extension-androidtools https://github.com/InsKal/extension-androidtools.git\n          haxelib list\n\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n\n      - name: Setup Lime\n        run: |\n          haxelib run lime setup -alias -y\n          haxelib run lime config ANDROID_SDK $ANDROID_HOME\n          haxelib run lime config ANDROID_NDK_ROOT $ANDROID_NDK_HOME\n          haxelib run lime config JAVA_HOME $JAVA_HOME\n          haxelib run lime config ANDROID_SETUP true\n        env:\n          ANDROID_NDK_HOME: ${{ steps.setup-ndk.outputs.ndk-path }}\n\n      - name: Compile\n        run: haxelib run lime build android --app-version=\"4.0.0-${{ github.run_id}}\"\n\n      - name: Publish Artifact\n        uses: actions/upload-artifact@v2.2.4\n        with:\n          name: androidBuild\n          path: export/release/android/bin/app/build/outputs/apk/debug\n\n",
    "source": "InsKal/Psych-Engine-With-Android-Shaders-And-More-Callbacks",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/InsKal/Psych-Engine-With-Android-Shaders-And-More-Callbacks/blob/e2e219d9e6c51bf17faa07fac99f7eb5d5106ad9/.github/workflows/main.yml",
    "retrieved_at": "2025-08-16T01:46:36.367065Z"
  },
  {
    "question": "What is the purpose of the `build_cpu`, `build_cu101`, and `build_cu102` jobs, and how do they differ?",
    "answer": "name: build\n\non: [push, pull_request]\n\njobs:\n  lint:\n    runs-on: ubuntu-18.04\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python 3.7\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.7\n      - name: Install pre-commit hook\n        run: |\n          pip install pre-commit\n          pre-commit install\n      - name: Linting\n        run: pre-commit run --all-files\n      - name: Check docstring coverage\n        run: |\n          pip install interrogate\n          interrogate -v --ignore-init-method --ignore-module --ignore-nested-functions --ignore-regex \"__repr__\" --fail-under 80 mmaction\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.7.0, 1.9.0]\n        include:\n          - torch: 1.5.0\n            torchvision: 0.6.0\n          - torch: 1.7.0\n            torchvision: 0.8.1\n          - torch: 1.9.0\n            torchvision: 0.10.0\n            python-version: 3.7\n          - torch: 1.9.0\n            torchvision: 0.10.0\n            python-version: 3.8\n          - torch: 1.9.0\n            torchvision: 0.10.0\n            python-version: 3.9\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install soundfile lib\n        run: sudo apt-get install -y libsndfile1\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install librosa and soundfile\n        run: pip install librosa soundfile\n      - name: Install lmdb\n        run: pip install lmdb\n      - name: Install TurboJpeg lib\n        run: sudo apt-get install -y libturbojpeg\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch${{matrix.torch}}/index.html\n      - name: Install MMDet\n        run: pip install git+https://github.com/open-mmlab/mmdetection/\n      - name: Install MMCls\n        run: pip install git+https://github.com/open-mmlab/mmclassification/\n      - name: Install unittest dependencies\n        run: pip install -r requirements/tests.txt -r requirements/optional.txt\n      - name: Install PytorchVideo\n        run: pip install pytorchvideo\n        if: ${{matrix.torchvision == '0.10.0'}}\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --source mmaction -m pytest tests/\n          coverage xml\n          coverage report -m\n  build_cu101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0+cu101, 1.6.0+cu101, 1.7.0+cu101]\n        include:\n          - torch: 1.5.0+cu101\n            torch_version: torch1.5.0\n            torchvision: 0.6.0+cu101\n          - torch: 1.6.0+cu101\n            torch_version: torch1.6.0\n            torchvision: 0.7.0+cu101\n          - torch: 1.7.0+cu101\n            torch_version: torch1.7.0\n            torchvision: 0.8.1+cu101\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install CUDA\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libturbojpeg libsndfile1 libsm6 libxrender-dev libxext6 python${{matrix.python-version}}-dev\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Install librosa and soundfile\n        run: python -m pip install librosa soundfile\n      - name: Install lmdb\n        run: python -m pip install lmdb\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmaction dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -q git+https://github.com/open-mmlab/mmdetection/\n          python -m pip install -q git+https://github.com/open-mmlab/mmclassification/\n          python -m pip install -r requirements.txt\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --source mmaction -m pytest tests/\n          coverage xml\n          coverage report -m\n      # Only upload coverage report for python3.7 && pytorch1.5\n      - name: Upload coverage to Codecov\n        if: ${{matrix.torch == '1.5.0+cu101' && matrix.python-version == '3.7'}}\n        uses: codecov/codecov-action@v1.0.14\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cu102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.9.0+cu102]\n        include:\n          - torch: 1.9.0+cu102\n            torch_version: torch1.9.0\n            torchvision: 0.10.0+cu102\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install CUDA\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libturbojpeg libsndfile1 libsm6 libxrender-dev libxext6 python${{matrix.python-version}}-dev\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Install librosa and soundfile\n        run: python -m pip install librosa soundfile\n      - name: Install lmdb\n        run: python -m pip install lmdb\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}} torchvision==${{matrix.torchvision}} -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmaction dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -q git+https://github.com/open-mmlab/mmdetection/\n          python -m pip install -q git+https://github.com/open-mmlab/mmclassification/\n          python -m pip install -r requirements.txt\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install PytorchVideo\n        run: python -m pip install pytorchvideo\n        if: ${{matrix.torchvision == '0.10.0+cu102'}}\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --source mmaction -m pytest tests/\n          coverage xml\n          coverage report -m\n",
    "source": "rehohoho/mmaction2",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/rehohoho/mmaction2/blob/b1e962ab97d4844452c90d57800e733f3425128a/.github/workflows/build.yml",
    "retrieved_at": "2025-08-16T01:46:37.485900Z"
  },
  {
    "question": "What specific tests are executed by the `run-multiple.sh` script?",
    "answer": "# Runs randomly generated E2E testnets nightly on the 0.34.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-34x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01']\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.18'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.34.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"::set-output name=branch::`git branch --show-current`\"\n          echo \"::set-output name=commit::`git rev-parse HEAD`\"\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 2 -d networks/nightly -p\n\n      - name: Run testnets in group ${{ matrix.group }}\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n      git-commit: ${{ steps.git-info.outputs.commit }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMIT_URL: \"${{ github.server_url }}/${{ github.repository }}/commit/${{ needs.e2e-nightly-test.outputs.git-commit }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMIT_URL }}|commit> that caused the failure.\"\n                  }\n                }\n              ]\n            }\n\n  e2e-nightly-success:  # may turn this off once they seem to pass consistently\n    needs: e2e-nightly-test\n    if: ${{ success() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on success\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":white_check_mark: Nightly E2E tests for `${{ env.BRANCH }}` passed.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "lightmos/lightmosbft",
    "path": ".github/workflows/e2e-nightly-34x.yml",
    "url": "https://github.com/lightmos/lightmosbft/blob/6f48b680f6d1bc32abf7f2ad91e84d3d53e7f96d/.github/workflows/e2e-nightly-34x.yml",
    "retrieved_at": "2025-08-17T01:57:17.819463Z"
  },
  {
    "question": "Under what conditions will the workflow's build job be skipped?",
    "answer": "#name: Release\n#on:\n#  push:\n#    branches:\n#      - main\n#\n#jobs:\n#  build:\n#    if: \"!contains(github.event.commits[0].message, '[skip ci]')\"\n#    strategy:\n#      matrix:\n#        include:\n#          - os: macos-latest\n#            gradle_args: assemble publishIosX64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishIosArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishIosSimulatorArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishTvosX64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishTvosArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishTvosSimulatorArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishWatchosArm32PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishWatchosArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishWatchosX64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishWatchosSimulatorArm64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishMacosX64PublicationToSonatypeRepository\n#          - os: macos-latest\n#            gradle_args: assemble publishMacosArm64PublicationToSonatypeRepository\n#          - os: ubuntu-latest\n#            gradle_args: assemble publishKotlinMultiplatformPublicationToSonatypeRepository publishJvmPublicationToSonatypeRepository publishLinuxX64PublicationToSonatypeRepository publishLinuxArm64PublicationToSonatypeRepository publishMingwX64PublicationToSonatypeRepository\n#\n#    runs-on: ${{ matrix.os }}\n#    steps:\n#      - name: Checkout project sources\n#        uses: actions/checkout@v3\n#      - uses: actions/setup-java@v3\n#        with:\n#          distribution: liberica\n#          java-version: 8\n#      - name: Build using gradle\n#        uses: gradle/gradle-build-action@v2\n#        with:\n#          arguments: ${{ matrix.gradle_args }}\n#        env:\n#          SIGNING_SECRET_KEY: ${{ secrets.SIGNING_SECRET_KEY }}\n#          SIGNING_PASSWORD: ${{ secrets.SIGNING_PASSWORD }}\n#          OSSRH_USERNAME: ${{ secrets.OSSRH_USERNAME }}\n#          OSSRH_PASSWORD: ${{ secrets.OSSRH_PASSWORD }}\n#          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "source": "ton-blockchain/ton-kotlin",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/ton-blockchain/ton-kotlin/blob/29b1b1d3cc91ebafab70af54cc4a19b7ec81ca5a/.github/workflows/release.yml",
    "retrieved_at": "2025-08-17T01:57:18.530171Z"
  },
  {
    "question": "What triggers the `Mac MPS` workflow, and how does it handle concurrent runs?",
    "answer": "name: Mac MPS\n\non:\n  push:\n    tags:\n      - ciflow/mps/*\n  workflow_dispatch:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref_name }}-${{ github.ref_type == 'branch' && github.sha }}-${{ github.event_name == 'workflow_dispatch' }}\n  cancel-in-progress: true\n\njobs:\n  macos-12-py3-arm64-build:\n    name: macos-12-py3-arm64\n    uses: ./.github/workflows/_mac-build.yml\n    with:\n      sync-tag: macos-12-py3-arm64-build\n      build-environment: macos-12-py3-arm64\n      xcode-version: \"13.3.1\"\n      runner-type: macos-12-xl\n      build-generates-artifacts: true\n      # To match the one pre-installed in the m1 runners\n      python_version: 3.9.12\n      # We need to set the environment file here instead of trying to detect it automatically because\n      # MacOS arm64 is cross-compiled from x86-64. Specifically, it means that arm64 conda environment\n      # is needed when building PyTorch MacOS arm64 from x86-64\n      environment-file: .github/requirements/conda-env-macOS-ARM64\n    secrets:\n      MACOS_SCCACHE_S3_ACCESS_KEY_ID: ${{ secrets.MACOS_SCCACHE_S3_ACCESS_KEY_ID }}\n      MACOS_SCCACHE_S3_SECRET_ACCESS_KEY: ${{ secrets.MACOS_SCCACHE_S3_SECRET_ACCESS_KEY }}\n\n  macos-12-py3-arm64-mps-test:\n    name: macos-12-py3-arm64-mps\n    uses: ./.github/workflows/_mac-test-mps.yml\n    needs: macos-12-py3-arm64-build\n    with:\n      sync-tag: macos-12-py3-arm64-mps-test\n      build-environment: macos-12-py3-arm64\n\n  macos-13-py3-arm64-mps-test:\n    name: macos-13-py3-arm64-mps\n    uses: ./.github/workflows/_mac-test-mps.yml\n    needs: macos-12-py3-arm64-build\n    with:\n      build-environment: macos-12-py3-arm64\n      runs-on: macos-m1-13\n",
    "source": "UEFI-code/PyTorch_For_PoorGuys",
    "path": ".github/workflows/mac-mps.yml",
    "url": "https://github.com/UEFI-code/PyTorch_For_PoorGuys/blob/a66ed97b99aa81e94e5700e495e62fdaa97ecf00/.github/workflows/mac-mps.yml",
    "retrieved_at": "2025-08-17T01:57:19.222156Z"
  },
  {
    "question": "What actions are triggered when a pull request is made to the `main` branch?",
    "answer": "name: Release Charts\n\non:\n  pull_request:\n    branches:\n      - main\n\njobs:\n  lint-test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: Set up Helm\n        uses: azure/setup-helm@v3\n        with:\n          version: v3.9.0\n\n      - name: Add Bitnami\n        run: helm repo add bitnami https://charts.bitnami.com/bitnami\n\n      - uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n\n      - name: Set up chart-testing\n        uses: helm/chart-testing-action@v2.3.0\n\n      - name: Run chart-testing (list-changed)\n        id: list-changed\n        run: |\n          changed=$(ct list-changed)\n          if [[ -n \"$changed\" ]]; then\n            echo \"::set-output name=changed::true\"\n          fi\n\n      - name: Run chart-testing (lint)\n        run: ct lint --check-version-increment=false\n\n      - name: Create kind cluster\n        uses: helm/kind-action@v1.3.0\n        # if: steps.list-changed.outputs.changed == 'true'\n        # TODO for some reason, the earlier chart-testing logic is never seeing any changes, though running it locally\n        # does show changes. This remains a mystery between a lack of any debug logging, but simply running tests\n        # always is fine\n\n      - name: Run chart-testing (install)\n        run: ct install --all\n\n  integration-test:\n    # Not a real requirement, but due to worker CPU contention, this can fail if it runs concurrent with lint-test\n    needs: lint-test\n    runs-on: ubuntu-latest\n    steps:\n      - name: checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: setup helm\n        uses: azure/setup-helm@v3\n        with:\n          version: v3.9.0\n\n      - name: setup testing environment (kind-cluster)\n        run: ./scripts/test-env.sh\n\n      - name: run integration tests (integration)\n        run: ./scripts/test-run.sh\n\n      - name: run upgrade integration tests (integration-upgrade)\n        run: ./scripts/test-upgrade.sh\n\n      - name: cleanup integration tests (cleanup)\n        run: ./scripts/test-env.sh cleanup\n",
    "source": "harmonicai/kong-fork",
    "path": ".github/workflows/main-pr.yaml",
    "url": "https://github.com/harmonicai/kong-fork/blob/b43e8e3cd9683644b22afe50d7a84e277090d6a3/.github/workflows/main-pr.yaml",
    "retrieved_at": "2025-08-17T01:57:19.783047Z"
  },
  {
    "question": "Does this workflow post a comment on new pull requests if their descriptions are empty or consist only of specific characters and formatting?",
    "answer": "name: Lint new PR\n\non:\n    pull_request:\n        types: [opened]\n\njobs:\n    check-description:\n        name: Check that PR has description\n        runs-on: ubuntu-20.04\n\n        steps:\n            - name: Check if PR is shame-worthy\n              id: is-shame-worthy\n              run: |\n                  FILTERED_BODY=$( \\\n                      sed -r -e \\\n                      '/^(\\.\\.\\.)|(\\*)|(#+ )|(- )|(<!--)|(👉)/d' \\\n                      <<< $RAW_BODY \\\n                  )\n                  echo \"::debug::Filtered PR body to $FILTERED_BODY\"\n                  if [[ -z \"${FILTERED_BODY//[[:space:]]/}\" ]]; then\n                      echo \"is-shame-worthy=true\" >> $GITHUB_OUTPUT\n                  else\n                      echo \"is-shame-worthy=false\" >> $GITHUB_OUTPUT\n                  fi\n              env:\n                  RAW_BODY: ${{ github.event.pull_request.body }}\n\n            - name: Shame if PR has no description\n              if: steps.is-shame-worthy.outputs.is-shame-worthy == 'true'\n              run: |\n                  SHAME_BODY=\"Hey @${{ github.actor }}! 👋\\nThis pull request seems to contain no description. Please add useful context, rationale, and/or any other information that will help make sense of this change now and in the distant Mars-based future.\"\n                  curl -s -u posthog-bot:${{ secrets.POSTHOG_BOT_GITHUB_TOKEN || secrets.GITHUB_TOKEN }} -X POST -d \"{ \\\"body\\\": \\\"$SHAME_BODY\\\" }\" \"https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments\"\n",
    "source": "gooditworks/posthog",
    "path": ".github/workflows/lint-new-pr.yml",
    "url": "https://github.com/gooditworks/posthog/blob/3d6709ee73ddfb3f0d0c23bdde2da1ced904285b/.github/workflows/lint-new-pr.yml",
    "retrieved_at": "2025-08-17T01:57:20.464698Z"
  },
  {
    "question": "What is the purpose of this workflow and how frequently does it run?",
    "answer": "name: 'Lock threads'\n\non:\n  schedule:\n    - cron: '0 9 * * *'\n\njobs:\n  lock:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: dessant/lock-threads@v2\n        with:\n          github-token: ${{ github.token }}\n          issue-lock-inactive-days: '90'\n          issue-lock-reason: ''\n          pr-lock-inactive-days: '90'\n          pr-lock-reason: ''\n",
    "source": "authok/node-oidc-provider",
    "path": ".github/workflows/lock.yml",
    "url": "https://github.com/authok/node-oidc-provider/blob/492c5b897f1eac8616e397148a12ee5fdd2ac1ae/.github/workflows/lock.yml",
    "retrieved_at": "2025-08-17T01:57:21.173846Z"
  },
  {
    "question": "What actions are performed when a pull request targets the `master` branch?",
    "answer": "---\nname: Check\n\non:\n  pull_request:\n    branches: [\"master\"]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    env:\n      HUGO_VERSION: 0.102.3\n    steps:\n      - name: Install Hugo CLI\n        run: |\n          wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_Linux-64bit.deb \\\n          && sudo dpkg -i ${{ runner.temp }}/hugo.deb\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          submodules: recursive\n      - name: Build with Hugo\n        env:\n          HUGO_ENVIRONMENT: production\n          HUGO_ENV: production\n        run: |\n          set -x\n          pwd\n          ls\n          hugo \\\n            --minify \\\n            --baseURL grotius.example.com \\\n            --theme hugo-theme-notrack/ \\\n            --themesDir ../../ \\\n            --printUnusedTemplates \\\n        working-directory: ./exampleSite\n",
    "source": "jssuzuki1/assignment_2b",
    "path": ".github/workflows/check.yaml",
    "url": "https://github.com/jssuzuki1/assignment_2b/blob/33d32bc2cd39cd2d6364441b25948827fbdf5ede/.github/workflows/check.yaml",
    "retrieved_at": "2025-08-17T01:57:21.694065Z"
  },
  {
    "question": "What specific actions are performed by the `make` commands referenced throughout the workflow?",
    "answer": "name: rAPId Deployment\n\non:\n  push:\n    branches: [main]\n\n  workflow_dispatch:\n\njobs:\n  setup:\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Log commit SHA\n        run: echo $GITHUB_SHA\n\n  security-check:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - run: pip install -r requirements.txt\n\n      - name: Run security checks\n        run: make security-check\n\n  api-deployment:\n    needs:\n      - setup\n      - security-check\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          cp ./.github/.github.env .env\n          echo AWS_ACCOUNT=${{ secrets.AWS_ACCOUNT }} >> .env\n          echo AWS_REGION=${{ secrets.AWS_REGION }} >> .env\n          echo AWS_DEFAULT_REGION=${{ secrets.AWS_REGION }} >> .env\n\n      - name: Build API Image\n        run: make api/create-image\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - name: Setup API environment\n        run: make api/setup\n\n      - name: API Static Analysis\n        run: make api/lint\n\n      - name: API Tests\n        run: make api/test\n\n      - name: API Tag and Upload\n        run: make api/tag-and-upload\n\n      - name: API Check Image Scan for Vulnerabilities\n        run: make api/scan-for-vulns-and-tag\n\n      - name: API Tag PROD Candidate\n        run: make api/tag-prod-candidate\n\n      - name: API Deploy Image to Prod\n        run: make api/app-live-in-prod\n\n      - name: API Allow for Application to Start\n        run: sleep 120\n\n      - name: API Wait for Running Application\n        id: await-running-app\n        run: make api/check-app-is-running\n\n      - name: API E2E Tests\n        id: e2e-tests\n        env:\n          E2E_DOMAIN_NAME: ${{ secrets.E2E_DOMAIN_NAME }}\n          E2E_RESOURCE_PREFIX: ${{ secrets.E2E_RESOURCE_PREFIX }}\n        run: |\n          # Export AWS credentials to env for e2e tests\n          eval \"$(aws configure export-credentials --format env)\"\n          make api/test-e2e\n\n      - name: API Tag Image as Failure\n        if: always() && steps.await-running-app.conclusion == 'failure' || steps.e2e-tests.conclusion == 'failure'\n        run: make api/tag-prod-failure\n\n  cleanup:\n    needs:\n      - setup\n      - security-check\n      - api-deployment\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Clean Docker Context\n        if: always()\n        run: make api/clean-docker\n",
    "source": "no10ds/rapid",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/no10ds/rapid/blob/c763da3d92b4371effa90f6df460c4d027b899c6/.github/workflows/main.yml",
    "retrieved_at": "2025-08-17T01:57:22.380959Z"
  },
  {
    "question": "Under what conditions does the `deploy` job execute, and what deployment tool is used?",
    "answer": "name: deploy\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\non:\n  push:\n    branches:\n      - 'main'\n  pull_request:\n    branches:\n      - 'main'\njobs:\n  setup:\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - name: ⬇️ Checkout repo\n        uses: actions/checkout@v4\n\n      - name: ⎔ Setup node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20\n\n      - name: ▶️ Run setup script\n        run: npm run setup\n\n      - name: ʦ TypeScript\n        run: npm run typecheck\n\n      - name: ⬣ ESLint\n        run: npm run lint\n\n  deploy:\n    name: 🚀 Deploy\n    runs-on: ubuntu-latest\n    # only deploy main branch on pushes\n    if: ${{ github.ref == 'refs/heads/main' && github.event_name == 'push' }}\n\n    steps:\n      - name: ⬇️ Checkout repo\n        uses: actions/checkout@v4\n\n      - name: 🎈 Setup Fly\n        uses: superfly/flyctl-actions/setup-flyctl@1.5\n\n      - name: 🚀 Deploy\n        run: flyctl deploy --remote-only\n        working-directory: ./epicshop\n        env:\n          FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}\n",
    "source": "undyingkevin/react-api",
    "path": ".github/workflows/validate.yml",
    "url": "https://github.com/undyingkevin/react-api/blob/494643901e26d669e951f337fe927b29d460b2df/.github/workflows/validate.yml",
    "retrieved_at": "2025-08-17T01:57:23.082976Z"
  },
  {
    "question": "What specific code formatting and linting checks are performed by the `invoke project.pre-commit` command?",
    "answer": "# Check the code against the formatter and linter\nname: Code format and lint check\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\npermissions:\n  contents: read\n\njobs:\n  check:\n    name: Check Code\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n      fail-fast: false\n\n    steps:\n    - name: Check out from Git\n      uses: actions/checkout@v3\n    - name: Get history and tags for SCM versioning to work\n      run: |\n        git fetch --prune --unshallow\n        git fetch --depth=1 origin +refs/tags/*:refs/tags/*\n    - name: Set up Python\n      uses: actions/setup-python@v3\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        python -m pip install \".[build,test,development,pandora]\"\n    - name: Check\n      run: |\n        invoke project.pre-commit\n",
    "source": "msft-mirror-aosp/platform.external.python.bumble",
    "path": ".github/workflows/code-check.yml",
    "url": "https://github.com/msft-mirror-aosp/platform.external.python.bumble/blob/0d9d4b75e37ccc486b25830a3ef8f917730a0cb2/.github/workflows/code-check.yml",
    "retrieved_at": "2025-08-17T01:57:23.732725Z"
  },
  {
    "question": "Does this workflow update and push the index on every push or pull request to the master branch?",
    "answer": "name: CI\n\non:\n  # Trigger the workflow on push or pull request,\n  # but only for the master branch\n  push:\n    branches:\n      - master\n  pull_request:\n    branches:\n      - master\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n      with:\n        ref: ${{ github.head_ref }}\n\n    - name: Run gen index\n      run: \"./index_gen.sh\"\n      shell: bash\n\n    - name: Add & Commit\n      uses: github-actions-x/commit@v2.4\n      with:\n        commit-message: 'Index updated'\n        name: '${{ secrets.GH_USER }}'\n        email: '${{ secrets.GH_EMAIL }}'\n        push-branch: ${{ github.head_ref }}\n        github-token: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Push\n      uses: ad-m/github-push-action@v0.5.0\n      with:\n        github_token: ${{ secrets.GH_TOKEN }}\n        branch: ${{ github.head_ref }}\n",
    "source": "Yara-Rules/rules",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/Yara-Rules/rules/blob/0f93570194a80d2f2032869055808b0ddcdfb360/.github/workflows/main.yml",
    "retrieved_at": "2025-08-18T01:58:01.982381Z"
  },
  {
    "question": "Under what conditions does the `target-determination` job run within the workflow?",
    "answer": "# This workflow is dedicated to host slow jobs that are run only periodically because\n# they are too slow to run in every commit.  The list of slow tests can be found in\n# https://github.com/pytorch/test-infra/blob/generated-stats/stats/slow-tests.json\nname: slow\n\non:\n  schedule:\n    - cron: 45 0,4,8,12,16,20 * * *\n    - cron: 29 8 * * *  # about 1:29am PDT, for mem leak check and rerun disabled tests\n  push:\n    tags:\n      - ciflow/slow/*\n    branches:\n      - release/*\n  workflow_dispatch:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref_name }}-${{ github.ref_type == 'branch' && github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}-${{ github.event.schedule }}\n  cancel-in-progress: true\n\npermissions: read-all\n\njobs:\n  target-determination:\n    name: before-test\n    uses: ./.github/workflows/target_determination.yml\n    permissions:\n      id-token: write\n      contents: read\n\n  linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-build:\n    name: linux-focal-cuda12.1-py3-gcc9-slow-gradcheck\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-cuda12.1-py3-gcc9-slow-gradcheck\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      cuda-arch-list: 8.6\n      test-matrix: |\n        { include: [\n          { config: \"default\", shard: 1, num_shards: 4, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 2, num_shards: 4, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 3, num_shards: 4, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n          { config: \"default\", shard: 4, num_shards: 4, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n        ]}\n\n  linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-test:\n    name: linux-focal-cuda12.1-py3-gcc9-slow-gradcheck\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-build\n      - target-determination\n    with:\n      build-environment: linux-focal-cuda12.1-py3-gcc9-slow-gradcheck\n      docker-image: ${{ needs.linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-cuda12_1-py3-gcc9-slow-gradcheck-build.outputs.test-matrix }}\n      timeout-minutes: 300\n\n  linux-focal-cuda12_1-py3_10-gcc9-sm86-build:\n    name: linux-focal-cuda12.1-py3.10-gcc9-sm86\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9-sm86\n      docker-image-name: pytorch-linux-focal-cuda12.1-cudnn8-py3-gcc9\n      cuda-arch-list: 8.6\n      test-matrix: |\n        { include: [\n          { config: \"slow\", shard: 1, num_shards: 2, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n          { config: \"slow\", shard: 2, num_shards: 2, runner: \"linux.g5.4xlarge.nvidia.gpu\" },\n        ]}\n\n  linux-focal-cuda12_1-py3_10-gcc9-sm86-test:\n    name: linux-focal-cuda12.1-py3.10-gcc9-sm86\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-focal-cuda12_1-py3_10-gcc9-sm86-build\n      - target-determination\n    with:\n      build-environment: linux-focal-cuda12.1-py3.10-gcc9-sm86\n      docker-image: ${{ needs.linux-focal-cuda12_1-py3_10-gcc9-sm86-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-cuda12_1-py3_10-gcc9-sm86-build.outputs.test-matrix }}\n\n  linux-focal-py3_8-clang10-build:\n    name: linux-focal-py3.8-clang10\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-py3.8-clang10\n      docker-image-name: pytorch-linux-focal-py3.8-clang10\n      test-matrix: |\n        { include: [\n          { config: \"slow\", shard: 1, num_shards: 1, runner: \"linux.2xlarge\" },\n        ]}\n\n  linux-focal-py3_8-clang10-test:\n    name: linux-focal-py3.8-clang10\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-focal-py3_8-clang10-build\n      - target-determination\n    with:\n      build-environment: linux-focal-py3.8-clang10\n      docker-image: ${{ needs.linux-focal-py3_8-clang10-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-py3_8-clang10-build.outputs.test-matrix }}\n\n  linux-focal-rocm6_0-py3_8-build:\n    name: linux-focal-rocm6.0-py3.8\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-focal-rocm6.0-py3.8\n      docker-image-name: pytorch-linux-focal-rocm-n-py3\n      test-matrix: |\n        { include: [\n          { config: \"slow\", shard: 1, num_shards: 1, runner: \"linux.rocm.gpu\" },\n        ]}\n\n  linux-focal-rocm6_0-py3_8-test:\n    permissions:\n      id-token: write\n      contents: read\n    name: linux-focal-rocm6.0-py3.8\n    uses: ./.github/workflows/_rocm-test.yml\n    needs:\n      - linux-focal-rocm6_0-py3_8-build\n      - target-determination\n    with:\n      build-environment: linux-focal-rocm6.0-py3.8\n      docker-image: ${{ needs.linux-focal-rocm6_0-py3_8-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-focal-rocm6_0-py3_8-build.outputs.test-matrix }}\n\n  linux-jammy-py3_10-clang15-asan-build:\n    name: linux-jammy-py3.10-clang15-asan\n    uses: ./.github/workflows/_linux-build.yml\n    with:\n      build-environment: linux-jammy-py3.10-clang15-asan\n      docker-image-name: pytorch-linux-jammy-py3-clang15-asan\n      test-matrix: |\n        { include: [\n          { config: \"slow\", shard: 1, num_shards: 2, runner: \"linux.4xlarge\" },\n          { config: \"slow\", shard: 2, num_shards: 2, runner: \"linux.4xlarge\" },\n        ]}\n      sync-tag: asan-build\n\n  linux-jammy-py3_10-clang15-asan-test:\n    name: linux-jammy-py3.10-clang15-asan\n    uses: ./.github/workflows/_linux-test.yml\n    needs:\n      - linux-jammy-py3_10-clang15-asan-build\n      - target-determination\n    with:\n      build-environment: linux-jammy-py3.10-clang15-asan\n      docker-image: ${{ needs.linux-jammy-py3_10-clang15-asan-build.outputs.docker-image }}\n      test-matrix: ${{ needs.linux-jammy-py3_10-clang15-asan-build.outputs.test-matrix }}\n      sync-tag: asan-test\n",
    "source": "zal-orz/pytorch",
    "path": ".github/workflows/slow.yml",
    "url": "https://github.com/zal-orz/pytorch/blob/78b4793c965fe640e37d80530eb78f07d67492e8/.github/workflows/slow.yml",
    "retrieved_at": "2025-08-18T01:58:02.922897Z"
  },
  {
    "question": "Under what conditions will the workflow upload coverage data to Codecov?",
    "answer": "# This is a basic workflow to help you get started with Actions\n\nname: CI\n\n# Controls when the action will run.\non:\n  # Triggers the workflow on push or pull request events but only for the master branch\n  push:\n    branches: [master]\n  pull_request:\n    branches: [master]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n  build:\n    # The type of runner that the job will run on\n    runs-on: macos-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        channel: [stable, beta, dev]\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      - name: Flutter action\n        uses: subosito/flutter-action@v1\n        with:\n          channel: ${{ matrix.channel }}\n\n      - name: Run Tests\n        run: |\n          flutter pub get\n          flutter format --dry-run --set-exit-if-changed .\n          flutter analyze --no-pub\n          flutter test --no-pub --coverage\n      - name: Upload coverage to Codecov\n        if: ${{ matrix.channel == 'stable' }}\n        uses: codecov/codecov-action@v1\n        with:\n          file: coverage/lcov.info\n",
    "source": "danvick/flutter_chips_input",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/danvick/flutter_chips_input/blob/4953c0f6ffcb35308738909aeab5d503a062c9be/.github/workflows/main.yml",
    "retrieved_at": "2025-08-18T01:58:03.768919Z"
  },
  {
    "question": "Under what conditions does the \"CLA Assistant\" step execute, and what actions does it perform?",
    "answer": "name: \"CLA Assistant\"\non:\n  issue_comment:\n    types: [created]\n  pull_request_target:\n    types: [opened,closed,synchronize]\n\njobs:\n  CLAssistant:\n    runs-on: ubuntu-latest\n    steps:\n      - name: \"CLA Assistant\"\n        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'\n        # Beta Release\n        uses: cla-assistant/github-action@v2.1.3-beta\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          # the below token should have repo scope and must be manually added by you in the repository's secret\n          PERSONAL_ACCESS_TOKEN : ${{ secrets.PERSONAL_ACCESS_TOKEN }}\n        with:\n          path-to-signatures: 'cla.json'\n          path-to-document: 'https://github.com/0xPolygon/polygon-edge/blob/develop/CLA.md'\n          branch: 'cla-signatures'\n          allowlist: dependabot[bot],dependabot-preview[bot]\n",
    "source": "blockstars-tech/konsta-core",
    "path": ".github/workflows/cla.yml",
    "url": "https://github.com/blockstars-tech/konsta-core/blob/c96334781453708dbdbabf2b221b63af2cdb05a1/.github/workflows/cla.yml",
    "retrieved_at": "2025-08-18T01:58:04.642762Z"
  },
  {
    "question": "What tests are performed by this workflow to ensure SDL2 compatibility for the Sony Playstation Vita?",
    "answer": "name: Build (Sony Playstation Vita)\n\non: [push, pull_request]\n\ndefaults:\n  run:\n    shell: sh\n\njobs:\n  vita:\n    runs-on: ubuntu-latest\n    container: \n      image: vitasdk/vitasdk:latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Install build requirements\n      run: |\n        apk update \n        apk add cmake ninja pkgconf bash\n    - name: Configure CMake\n      run: |\n        cmake -S . -B build -G Ninja \\\n          -DCMAKE_TOOLCHAIN_FILE=${VITASDK}/share/vita.toolchain.cmake \\\n          -DSDL_WERROR=ON \\\n          -DSDL_TESTS=ON \\\n          -DSDL_INSTALL_TESTS=ON \\\n          -DCMAKE_BUILD_TYPE=Release \\\n          -DCMAKE_INSTALL_PREFIX=prefix\n    - name: Build\n      run: cmake --build build --verbose\n    - name: Install CMake\n      run: |\n        echo \"SDL2_DIR=$(pwd)/prefix\" >> $GITHUB_ENV\n        cmake --install build/\n        ( cd prefix; find ) | LC_ALL=C sort -u\n    - name: Verify CMake configuration files\n      run: |\n        cmake -S cmake/test -B cmake_config_build -G Ninja \\\n          -DCMAKE_TOOLCHAIN_FILE=${VITASDK}/share/vita.toolchain.cmake \\\n          -DTEST_SHARED=FALSE \\\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }} \\\n          -DCMAKE_BUILD_TYPE=Release\n        cmake --build cmake_config_build --verbose\n    - name: Verify sdl2-config\n      run: |\n        export CC=arm-vita-eabi-gcc\n        export PATH=${{ env.SDL2_DIR }}/bin:$PATH\n        cmake/test/test_sdlconfig.sh\n    - name: Verify sdl2.pc\n      run: |\n        export CC=arm-vita-eabi-gcc\n        export PKG_CONFIG_PATH=${{ env.SDL2_DIR }}/lib/pkgconfig\n        cmake/test/test_pkgconfig.sh\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/vita.yaml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/vita.yaml",
    "retrieved_at": "2025-08-18T01:58:05.488972Z"
  },
  {
    "question": "Under what conditions does this workflow update PyTorch labels in S3?",
    "answer": "name: Update PyTorch Labels in S3\n\non:\n  label:\n  workflow_dispatch:\n\nconcurrency:\n  group: 1\n  cancel-in-progress: true\n\njobs:\n  update-labels-in-S3:\n    runs-on: ubuntu-22.04\n    if: ${{ github.repository == 'pytorch/pytorch' }}\n    steps:\n      - name: Checkout PyTorch\n        uses: pytorch/pytorch/.github/actions/checkout-pytorch@master\n        with:\n          fetch-depth: 1\n          submodules: false\n      - name: Update PyTorch labels list in S3\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_OSSCI_METRICS_V2_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_OSSCI_METRICS_V2_SECRET_ACCESS_KEY }}\n        run: |\n          python3 -m pip install boto3==1.19.12\n          .github/scripts/export_pytorch_labels.py\n",
    "source": "UEFI-code/PyTorch_For_PoorGuys",
    "path": ".github/workflows/update_pytorch_labels.yml",
    "url": "https://github.com/UEFI-code/PyTorch_For_PoorGuys/blob/a66ed97b99aa81e94e5700e495e62fdaa97ecf00/.github/workflows/update_pytorch_labels.yml",
    "retrieved_at": "2025-08-18T01:58:06.318803Z"
  },
  {
    "question": "What branches trigger this workflow on push events, and what other event triggers it?",
    "answer": "name: Secret Scan\n\non:\n  push:\n    branches:\n      - main\n      - master\n  pull_request:\n\njobs:\n  trufflehog:\n    name: Trufflehog\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    steps:\n      - name: Setup jq\n        uses: dcarbone/install-jq-action@v2.1.0\n\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: TruffleHog OSS\n        uses: trufflesecurity/trufflehog@main\n        with:\n          extra_args: --debug --only-verified",
    "source": "teknologi-umum/tgif",
    "path": ".github/workflows/Secret-Scan.yml",
    "url": "https://github.com/teknologi-umum/tgif/blob/83762eef67af2401fc8cb29c303569c083c6fac4/.github/workflows/Secret-Scan.yml",
    "retrieved_at": "2025-08-18T01:58:07.162858Z"
  },
  {
    "question": "What triggers this workflow to build and publish documentation?",
    "answer": "name: Docs\n\non:\n  push:\n    branches: [main]\n    paths-ignore:\n      - \".github/**\" # Ignore changes towards the .github directory\n  workflow_dispatch: # run on request (no need for PR)\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\n# Allow one concurrent deployment\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: true\n\njobs:\n  Build-and-Publish-Documentation:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: [docs]\n    steps:\n      - name: CHECKOUT REPOSITORY\n        uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n\n      - name: Install requirements\n        run: |\n          pip install -r requirements/docs.txt\n          pip install \".[full]\"\n\n      - name: Link dataset path to local directory as nbsphinx runs the notebook\n        run: ln -s $ANOMALIB_DATASET_PATH ./datasets\n      - name: Build and Commit Docs\n        run: |\n          cd docs\n          make html\n          cd ..\n\n      - name: Clean directory\n        run: |\n          mkdir -p /tmp/docs_build\n          cp -r docs/build/html/* /tmp/docs_build\n          rm -rf ./*\n          cp -r /tmp/docs_build/* ./\n          rm -rf /tmp/docs_build\n          touch .nojekyll\n\n      - name: Setup Pages\n        uses: actions/configure-pages@v2\n\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v1\n        with:\n          # Upload entire repository\n          path: \".\"\n\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v1\n",
    "source": "versusic/anomalib",
    "path": ".github/workflows/docs.yml",
    "url": "https://github.com/versusic/anomalib/blob/9287d3bb07f8569f3c1cb2e9212da12508929468/.github/workflows/docs.yml",
    "retrieved_at": "2025-08-18T01:58:07.992674Z"
  },
  {
    "question": "What specific code quality checks are performed by this workflow beyond formatting and bad word detection?",
    "answer": "name: CPP project with GTest CI\n\non: [push]\n\njobs:\n  build:\n\n    runs-on: ubuntu-20.04\n\n    env:\n      CC: clang\n      CXX: clang++\n\n    steps:\n    - name: Setup dependencies\n      run: |\n        sudo apt-get install -y clang-tidy-10\n        sudo update-alternatives --install /usr/bin/clang-tidy clang-tidy /usr/bin/clang-tidy-10 100\n    - name: Checkout submodules\n      uses: actions/checkout@v1\n      with:\n        submodules: recursive\n    - name: Check for bad words\n      run: \"! grep -R -n -w -f .bad_words src include\"\n    - name: Run clang format\n      uses: DoozyX/clang-format-lint-action@v0.11\n      with:\n        source: '.'\n        exclude: './googletest ./test'\n        extensions: 'h,cpp'\n        clangFormatVersion: 11\n    - name: Setup ICA\n      uses: actions/checkout@v2\n      with:\n        repository: 'Kurkin/ica-lint-action'\n        path: ica\n    - name: Prepare build dir\n      run: mkdir build\n    - name: Generate build files using cmake\n      run: cmake .. -DUSE_CLANG_TIDY=TRUE -DPATH_TO_ICA=\"${{ github.workspace }}/ica/ica/libica-plugin.so\"\n      working-directory: ./build\n    - name: Run make\n      run: make\n      working-directory: ./build\n    - name: Run tests\n      timeout-minutes: 3\n      run: ./test/runUnitTests\n      working-directory: ./build\n    - name: Prepare ASAN build dir\n      run: mkdir build_asan\n    - name: Generate ASAN build files using cmake\n      run: cmake .. -DCMAKE_BUILD_TYPE=ASAN\n      working-directory: ./build_asan\n    - name: Run ASAN make\n      run: make\n      working-directory: ./build_asan\n    - name: Run ASAN tests\n      timeout-minutes: 5\n      run: ./test/runUnitTests\n      working-directory: ./build_asan\n    - name: Prepare USAN build dir\n      run: mkdir build_usan\n    - name: Generate USAN build files using cmake\n      run: cmake .. -DCMAKE_BUILD_TYPE=USAN\n      working-directory: ./build_usan\n    - name: Run USAN make\n      run: make\n      working-directory: ./build_usan\n    - name: Run USAN tests\n      timeout-minutes: 3\n      run: ./test/runUnitTests\n      working-directory: ./build_usan\n",
    "source": "AgafonovVadim/ouch-executed-order",
    "path": ".github/workflows/cpp_project.yml",
    "url": "https://github.com/AgafonovVadim/ouch-executed-order/blob/0a9c529e3a19f8c77c5cc1b856130a02959c3b91/.github/workflows/cpp_project.yml",
    "retrieved_at": "2025-08-18T01:58:08.835981Z"
  },
  {
    "question": "Under what conditions will the \"Build, test, and publish unstable release\" job be skipped?",
    "answer": "name: push-master\n\non:\n  push:\n    branches:\n      - master\n\njobs:\n  build_test_publish:\n    name: Build, test, and publish unstable release\n    if: \"! contains(github.event.head_commit.message, 'chore(release): publish')\"\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [11.x]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Use Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n          registry-url: \"https://registry.npmjs.org\"\n      - run: yarn install --frozen-lockfile\n      - run: yarn build:release\n      - run: yarn test\n      - run: git config user.name \"Mattr CI\"\n      - run: git config user.email \"npmjs_ci_mattr_public@mattr.global\"\n      - run: npm whoami\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPMJS_PUBLIC_TOKEN }}\n      - run: yarn publish:unstable\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPMJS_PUBLIC_TOKEN }}\n      - name: Report Coverage\n        uses: codecov/codecov-action@v1\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n",
    "source": "rodrigodg1/broker-bbs",
    "path": ".github/workflows/push-master.yaml",
    "url": "https://github.com/rodrigodg1/broker-bbs/blob/58e27be9391bc0c6e585cd6824e150f8f897fae4/.github/workflows/push-master.yaml",
    "retrieved_at": "2025-08-18T01:58:09.736728Z"
  },
  {
    "question": "Under what conditions does the `docker_ephemeral_env` job, which pushes a Docker image to ECR, execute?",
    "answer": "name: Push ephemeral env image\n\non:\n  workflow_run:\n    workflows: [\"Docker\"]\n    types:\n      - completed\n\njobs:\n  config:\n    runs-on: \"ubuntu-latest\"\n    if: github.event.workflow_run.event == 'pull_request' && github.event.workflow_run.conclusion == 'success'\n    outputs:\n      has-secrets: ${{ steps.check.outputs.has-secrets }}\n    steps:\n      - name: \"Check for secrets\"\n        id: check\n        shell: bash\n        run: |\n          if [ -n \"${{ (secrets.AWS_ACCESS_KEY_ID != '' &&\n            secrets.AWS_SECRET_ACCESS_KEY != '') || '' }}\" ]; then\n            echo \"has-secrets=1\" >> \"$GITHUB_OUTPUT\"\n            echo \"has secrets!\"\n          else\n            echo \"has-secrets=0\" >> \"$GITHUB_OUTPUT\"\n            echo \"no secrets!\"\n          fi\n\n  docker_ephemeral_env:\n    needs: config\n    if: needs.config.outputs.has-secrets\n    name: Push ephemeral env Docker image to ECR\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: \"Download artifact\"\n        uses: actions/github-script@v3.1.0\n        with:\n          script: |\n            const artifacts = await github.actions.listWorkflowRunArtifacts({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                run_id: ${{ github.event.workflow_run.id }},\n            });\n\n            core.info('*** artifacts')\n            core.info(JSON.stringify(artifacts))\n\n            const matchArtifact = artifacts.data.artifacts.filter((artifact) => {\n              return artifact.name == \"build\"\n            })[0];\n            if(!matchArtifact) return core.setFailed(\"Build artifacts not found\")\n\n            const download = await github.actions.downloadArtifact({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                artifact_id: matchArtifact.id,\n                archive_format: 'zip',\n            });\n            var fs = require('fs');\n            fs.writeFileSync('${{github.workspace}}/build.zip', Buffer.from(download.data));\n\n      - run: unzip build.zip\n\n      - name: Display downloaded files (debug)\n        run: ls -la\n\n      - name: Get SHA\n        id: get-sha\n        run: echo \"::set-output name=sha::$(cat ./SHA)\"\n\n      - name: Get PR\n        id: get-pr\n        run: echo \"::set-output name=num::$(cat ./PR-NUM)\"\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v1\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-west-2\n\n      - name: Login to Amazon ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v1\n\n      - name: Load, tag and push image to ECR\n        id: push-image\n        env:\n          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}\n          ECR_REPOSITORY: superset-ci\n          SHA: ${{ steps.get-sha.outputs.sha }}\n          IMAGE_TAG: pr-${{ steps.get-pr.outputs.num }}\n        run: |\n          docker load < $SHA.tar.gz\n          docker tag $SHA $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\n          docker tag $SHA $ECR_REGISTRY/$ECR_REPOSITORY:$SHA\n          docker push -a $ECR_REGISTRY/$ECR_REPOSITORY\n",
    "source": "IsraelBoka/superset-engeem",
    "path": ".github/workflows/docker-ephemeral-env.yml",
    "url": "https://github.com/IsraelBoka/superset-engeem/blob/891dee627d506c89d7274c286489ad646ca4ba03/.github/workflows/docker-ephemeral-env.yml",
    "retrieved_at": "2025-08-19T01:45:31.497564Z"
  },
  {
    "question": "What specific changes to files under `csrc/` or `demo/csrc/`, or to `CMakeLists.txt`, will trigger this workflow?",
    "answer": "name: backend-coreml\n\non:\n  push:\n    paths:\n      - \"csrc/**\"\n      - \"demo/csrc/**\"\n      - \"CMakeLists.txt\"\n\n  pull_request:\n    paths:\n      - \"csrc/**\"\n      - \"demo/csrc/**\"\n      - \"CMakeLists.txt\"\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\nenv:\n  DEVELOPER_DIR: /Applications/Xcode_13.4.1.app/Contents/Developer\npermissions:\n  contents: read\n\njobs:\n  build_macos_arm64:\n    runs-on: macos-12\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v3\n        with:\n          submodules: 'recursive'\n      - name: install opencv\n        run: |\n          wget https://github.com/irexyc/mmdeploy-ci-resource/releases/download/opencv/opencv-osx-arm64-4.6.0.tar.gz\n          mkdir $GITHUB_WORKSPACE/opencv-install\n          tar xf opencv-osx-arm64-4.6.0.tar.gz -C $GITHUB_WORKSPACE/opencv-install\n      - name: install libtorch\n        run: |\n          wget https://github.com/irexyc/mmdeploy-ci-resource/releases/download/libtorch/libtorch-osx-arm64-1.8.0.tar.gz\n          mkdir $GITHUB_WORKSPACE/libtorch-install\n          tar xf libtorch-osx-arm64-1.8.0.tar.gz -C $GITHUB_WORKSPACE/libtorch-install\n      - name: build\n        run: |\n          mkdir build && cd build\n          cmake .. -DCMAKE_OSX_ARCHITECTURES=\"arm64\" \\\n            -DCMAKE_SYSTEM_PROCESSOR=\"arm64\" \\\n            -DMMDEPLOY_BUILD_SDK=ON \\\n            -DMMDEPLOY_TARGET_DEVICES=\"cpu\" \\\n            -DMMDEPLOY_CODEBASES=all \\\n            -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4 \\\n            -DTorch_DIR=$GITHUB_WORKSPACE/libtorch-install/share/cmake/Torch \\\n            -DMMDEPLOY_TARGET_BACKENDS=\"coreml\" \\\n            -DMMDEPLOY_BUILD_EXAMPLES=ON \\\n            -DMMDEPLOY_SHARED_LIBS=OFF\n          cmake --build . -j 3\n          cmake --build . --target install\n      - name: build-shared\n        run: |\n          mkdir build-shared && cd build-shared\n          cmake .. -DCMAKE_OSX_ARCHITECTURES=\"arm64\" \\\n            -DCMAKE_SYSTEM_PROCESSOR=\"arm64\" \\\n            -DMMDEPLOY_BUILD_SDK=ON \\\n            -DMMDEPLOY_TARGET_DEVICES=\"cpu\" \\\n            -DMMDEPLOY_CODEBASES=all \\\n            -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4 \\\n            -DTorch_DIR=$GITHUB_WORKSPACE/libtorch-install/share/cmake/Torch \\\n            -DMMDEPLOY_TARGET_BACKENDS=\"coreml\" \\\n            -DMMDEPLOY_BUILD_EXAMPLES=ON \\\n            -DMMDEPLOY_SHARED_LIBS=ON\n          cmake --build . -j 3\n          cmake --build . --target install\n",
    "source": "HuangJunJie2017/mmdeploy",
    "path": ".github/workflows/backend-coreml.yml",
    "url": "https://github.com/HuangJunJie2017/mmdeploy/blob/f70f3af35474929c1cc402c9d058cc40457e5188/.github/workflows/backend-coreml.yml",
    "retrieved_at": "2025-08-19T01:45:32.278070Z"
  },
  {
    "question": "For which Windows platforms does this workflow build and test SDL using CMake and MSBuild?",
    "answer": "name: Build (MSVC)\n\non: [push, pull_request]\n\njobs:\n  Build:\n    name: ${{ matrix.platform.name }}\n    runs-on: windows-latest\n\n    strategy:\n      fail-fast: false\n      matrix:\n        platform:\n        - { name: Windows (x64),          flags: -A x64,   project: VisualC/SDL.sln, projectflags: '/p:Platform=x64' }\n        - { name: Windows (x86),          flags: -A Win32, project: VisualC/SDL.sln, projectflags: '/p:Platform=Win32' }\n        - { name: Windows static VCRT (x64), flags: -A x64 -DSDL_FORCE_STATIC_VCRT=ON }\n        - { name: Windows static VCRT (x86), flags: -A Win32 -DSDL_FORCE_STATIC_VCRT=ON }\n        - { name: Windows (clang-cl x64), flags: -T ClangCL -A x64 }\n        - { name: Windows (clang-cl x86), flags: -T ClangCL -A Win32 }\n        - { name: Windows (ARM),          flags: -A ARM }\n        - { name: Windows (ARM64),        flags: -A ARM64 }\n        - { name: UWP (x64),              flags: -A x64 -DCMAKE_SYSTEM_NAME=WindowsStore -DCMAKE_SYSTEM_VERSION=\"10.0\" -DSDL_TESTS=OFF, nowerror: true,\n            project: VisualC-WinRT/SDL-UWP.sln, projectflags: '/p:Platform=x64 /p:WindowsTargetPlatformVersion=10.0.17763.0' }\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Create CMake project using SDL as a subproject\n      shell: python\n      run: |\n        import os\n        import textwrap\n        srcdir = r\"${{ github.workspace }}\".replace(\"\\\\\", \"/\")\n        builddir = f\"{ srcdir }/build\"\n        os.makedirs(builddir)\n        with open(f\"{ builddir }/CMakeLists.txt\", \"w\") as f:\n          f.write(textwrap.dedent(f\"\"\"\\\n            cmake_minimum_required(VERSION 3.0)\n            project(sdl_user)\n            add_subdirectory(\"{ srcdir }\" SDL)\n          \"\"\"))\n    - name: Configure (CMake)\n      run: cmake -S build -B build `\n        -DSDL_WERROR=${{ !matrix.platform.nowerror }} `\n        -DSDL_TESTS=ON `\n        -DSDL_INSTALL_TESTS=ON `\n        -DSDL_VENDOR_INFO=\"Github Workflow\" `\n        -DSDL2_DISABLE_INSTALL=OFF `\n        ${{ matrix.platform.flags }} `\n        -DCMAKE_INSTALL_PREFIX=prefix\n    - name: Build (CMake)\n      run: cmake --build build/ --config Release --parallel\n    - name: Run build-time tests\n      if: \"! contains(matrix.platform.name, 'ARM')\"\n      run: |\n        $env:SDL_TESTS_QUICK=1\n        ctest -VV --test-dir build/ -C Release\n    - name: Install (CMake)\n      run: |\n        echo \"SDL2_DIR=$Env:GITHUB_WORKSPACE/prefix\" >> $Env:GITHUB_ENV\n        cmake --install build/\n    - name: Verify CMake configuration files\n      if: ${{ !contains(matrix.platform.name, 'UWP') }}  # FIXME: cmake/test/CMakeLists.txt should support UWP\n      run: |\n        cmake -S cmake/test -B cmake_config_build `\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }} `\n          ${{ matrix.platform.flags }}\n        cmake --build cmake_config_build --config Release\n\n    - name: Add msbuild to PATH\n      if: ${{ matrix.platform.project != '' }}\n      uses: microsoft/setup-msbuild@v1.1.3\n    - name: Build msbuild\n      if: ${{ matrix.platform.project != '' }}\n      run: msbuild ${{ matrix.platform.project }} /m /p:BuildInParallel=true /p:Configuration=Release ${{ matrix.platform.projectflags }}\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/msvc.yml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/msvc.yml",
    "retrieved_at": "2025-08-19T01:45:33.281326Z"
  },
  {
    "question": "What triggers this workflow to upload the Python package to PyPI?",
    "answer": "name: Upload Python Package\non:\n  release:\n    types: [published]\n  workflow_dispatch: # run on request (no need for PR)\n\njobs:\n  deploy:\n    runs-on: ubuntu-20.04\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install setuptools wheel twine\n      - name: Build and publish\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_PASSWORD }}\n        run: |\n          python setup.py sdist bdist_wheel\n          twine upload dist/*\n",
    "source": "versusic/anomalib",
    "path": ".github/workflows/publish.yml",
    "url": "https://github.com/versusic/anomalib/blob/9287d3bb07f8569f3c1cb2e9212da12508929468/.github/workflows/publish.yml",
    "retrieved_at": "2025-08-19T01:45:34.144427Z"
  },
  {
    "question": "What specific events trigger the execution of this workflow?",
    "answer": "name: any-pr\n\non: [pull_request]\n\njobs:\n  build_test:\n    name: Build test\n    runs-on: ${{matrix.os}}\n    strategy:\n      matrix:\n        node-version: [11.x]\n        os: [ubuntu-latest]\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: yarn install --frozen-lockfile\n      - run: yarn build:release\n      - run: yarn test\n  build_test_wasm_env_interop:\n    name: Build and Test WASM running cross OS and NodeJS versions\n    runs-on: ${{matrix.os}}\n    strategy:\n      matrix:\n        node-version: [11.x, 12.x, 13.x, 14.x, 15.x, 16.x]\n        os: [ubuntu-latest, macos-latest]\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: yarn install --frozen-lockfile\n      - run: yarn build:release\n      - run: yarn test:wasm\n  build_test_native_node_env_interop:\n    name: Build and Test native module running cross OS and NodeJS versions\n    runs-on: ${{matrix.os}}\n    strategy:\n      matrix:\n        node-version: [11.x, 12.x, 13.x, 14.x, 15.x, 16.x]\n        os: [ubuntu-latest, macos-latest]\n    steps:\n      - uses: actions/checkout@v1\n      - uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: yarn install --frozen-lockfile\n      - run: yarn build:release\n      - run: yarn test:node\n",
    "source": "rodrigodg1/broker-bbs",
    "path": ".github/workflows/any-pr.yaml",
    "url": "https://github.com/rodrigodg1/broker-bbs/blob/58e27be9391bc0c6e585cd6824e150f8f897fae4/.github/workflows/any-pr.yaml",
    "retrieved_at": "2025-08-19T01:45:34.997701Z"
  },
  {
    "question": "What triggers this workflow to run?",
    "answer": "name: Upload Website\non:\n  push:\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [12.x]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Use Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v1\n        with:\n          node-version: ${{ matrix.node-version }}\n      - run: npm install\n      - run: npm run build --if-present\n      - run: npm test\n        env:\n          CI: true\n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@master\n      - uses: jakejarvis/s3-sync-action@master\n        with:\n          args: --acl public-read --follow-symlinks --delete\n        env:\n          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          AWS_REGION: \"us-east-1\" # optional: defaults to us-east-1\n",
    "source": "tjmcode/MITxPRO-GitHubActions",
    "path": ".github/workflows/hello_actions.yml",
    "url": "https://github.com/tjmcode/MITxPRO-GitHubActions/blob/ac052dedade2149c7056d83ca113494f496e2f72/.github/workflows/hello_actions.yml",
    "retrieved_at": "2025-08-19T01:45:35.768459Z"
  },
  {
    "question": "What actions are performed on a pull request to this repository by this workflow?",
    "answer": "name: Checks\n\non:\n    - pull_request\n\njobs:\n    Build:\n        env:\n            BALANCER_SUBGRAPH: https://api.thegraph.com/subgraphs/name/beethovenxfi/beethovenx\n            MASTERCHEF_SUBGRAPH: https://api.thegraph.com/subgraphs/name/beethovenxfi/masterchefv2\n            BLOCKS_SUBGRAPH: https://api.thegraph.com/subgraphs/name/danielmkm/optimism-blocks\n            BEETS_BAR_SUBGRAPH: https://api.thegraph.com/subgraphs/name/beethovenxfi/beets-bar\n            USER_SNAPSHOT_SUBGRAPH: https://api.thegraph.com/subgraphs/name/danielmkm/user-balances-fantom\n            GAUGE_SUBGRAPH: https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-gauges-optimism\n            VEBALLOCKS_SUBGRAPH: https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-gauges\n            RELIQUARY_SUBGRAPH: https://api.thegraph.com/subgraphs/name/beethovenxfi/reliquary\n            SFTMX_SUBGRAPH: 'https://api.thegraph.com/subgraphs/name/beethovenxfi/sftmx'\n        runs-on: ubuntu-latest\n        steps:\n            - uses: actions/checkout@v3\n            - name: Use Node.js\n              uses: actions/setup-node@v3\n              with:\n                  node-version: '18.x'\n            - name: Install deps\n              run: yarn\n            - name: Generate Schema\n              run: yarn generate\n            - name: Prisma Generate\n              run: yarn prisma generate\n            - name: Run build\n              run: yarn build\n",
    "source": "beethovenxfi/beethovenx-backend",
    "path": ".github/workflows/checks.yml",
    "url": "https://github.com/beethovenxfi/beethovenx-backend/blob/4ef0c321ede84bca93c752a7dd1732bbb5addb14/.github/workflows/checks.yml",
    "retrieved_at": "2025-08-19T01:45:36.643664Z"
  },
  {
    "question": "Under what conditions will the workflow build and push the `cometbft/e2e-node` Docker image to Docker Hub?",
    "answer": "name: Docker E2E Node\n# Build & Push rebuilds the e2e Testapp docker image on every push to main and creation of tags\n# and pushes the image to https://hub.docker.com/r/cometbft/e2e-node\non:\n  push:\n    branches:\n      - v0.37.x\n    tags:\n      - \"v[0-9]+.[0-9]+.[0-9]+\"               # Push events to matching v*, i.e. v1.0, v20.15.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-alpha.[0-9]+\"  # e.g. v0.37.0-alpha.1, v0.38.0-alpha.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-beta.[0-9]+\"   # e.g. v0.37.0-beta.1, v0.38.0-beta.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-rc[0-9]+\"      # e.g. v0.37.0-rc1, v0.38.0-rc10\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Prepare\n        id: prep\n        run: |\n          DOCKER_IMAGE=cometbft/e2e-node\n          VERSION=noop\n          if [[ $GITHUB_REF == refs/tags/* ]]; then\n            VERSION=${GITHUB_REF#refs/tags/}\n          elif [[ $GITHUB_REF == refs/heads/* ]]; then\n            VERSION=$(echo ${GITHUB_REF#refs/heads/} | sed -r 's#/+#-#g')\n            if [ \"${{ github.event.repository.default_branch }}\" = \"$VERSION\" ]; then\n              VERSION=latest\n            fi\n          fi\n          TAGS=\"${DOCKER_IMAGE}:${VERSION}\"\n          if [[ $VERSION =~ ^v[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]]; then\n            TAGS=\"$TAGS,${DOCKER_IMAGE}:${VERSION}\"\n          fi\n          echo \"tags=${TAGS}\" >> $GITHUB_OUTPUT\n\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@master\n        with:\n          platforms: all\n\n      - name: Set up Docker Build\n        uses: docker/setup-buildx-action@v2.7.0\n\n      - name: Login to DockerHub\n        if: ${{ github.event_name != 'pull_request' }}\n        uses: docker/login-action@v2.2.0\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Publish to Docker Hub\n        uses: docker/build-push-action@v4.1.1\n        with:\n          context: .\n          file: ./test/e2e/docker/Dockerfile\n          platforms: linux/amd64,linux/arm64\n          push: ${{ github.event_name != 'beep_boop' }}\n          tags: ${{ steps.prep.outputs.tags }}\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/testapp-docker.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/testapp-docker.yml",
    "retrieved_at": "2025-08-19T01:45:37.357877Z"
  },
  {
    "question": "Under what conditions will the workflow build and push the Docker image tagged with `latest`?",
    "answer": "name: Build & Publish VWA Docker image\non:\n  push:\n    branches:\n      - master\n      - v*-branch\n    paths:\n      - components/crud-web-apps/volumes/**\n      - components/crud-web-apps/common/**\n      - releasing/version/VERSION\n\nenv:\n  DOCKER_USER: kubeflownotebookswg\n  IMG: kubeflownotebookswg/volumes-web-app\n  ARCH: linux/ppc64le,linux/amd64\n\njobs:\n  push_to_registry:\n    name: Build & Push Docker image to Docker Hub\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n\n    - uses: dorny/paths-filter@v2\n      id: filter\n      with:\n        filters: |\n          version:\n            - 'releasing/version/VERSION'\n\n    - name: Login to DockerHub\n      uses: docker/login-action@v2\n      with:\n        username: ${{ env.DOCKER_USER }}\n        password: ${{ secrets.KUBEFLOWNOTEBOOKSWG_DOCKER_TOKEN }}\n\n    - name: Setup QEMU\n      uses: docker/setup-qemu-action@v2\n\n    - name: Setup Docker Buildx\n      uses: docker/setup-buildx-action@v2\n\n    - name: Build and push multi-arch docker image\n      run: |\n        cd components/crud-web-apps/volumes\n        make docker-build-push-multi-arch\n\n    - name: Build and push latest multi-arch docker image\n      if: github.ref == 'refs/heads/master'\n      run: |\n        export TAG=latest\n        cd components/crud-web-apps/volumes\n        make docker-build-push-multi-arch\n\n    - name: Build and push multi-arch docker image on Version change\n      id: version\n      if: steps.filter.outputs.version == 'true'\n      run: |\n        export TAG=$(cat releasing/version/VERSION)\n        cd components/crud-web-apps/volumes\n        make docker-build-push-multi-arch\n",
    "source": "Zveroloff/kubeflow",
    "path": ".github/workflows/vwa_docker_publish.yaml",
    "url": "https://github.com/Zveroloff/kubeflow/blob/1552c90ccaee7acdea3495175592fcca46ae3a77/.github/workflows/vwa_docker_publish.yaml",
    "retrieved_at": "2025-08-19T01:45:38.110821Z"
  },
  {
    "question": "Under what conditions will the `Run reviewdog` step execute within this workflow?",
    "answer": "# textlintでの文法チェックのワークフロー\nname: Lint\n\n# Reviewdogでの指摘を有効に使うため、pull requestでのみ有効\non:\n  pull_request:\n    # Pull Requestかつ\n    # 以下のファイルに変更があった場合、このWorkflowを実行\n    paths:\n      - \"prh.yml\"\n      - \".textlintrc\"\n      - \"package.json\"\n      - \"package-lock.json\"\n      - \".github/workflows/lint.yaml\"\n      - \"**.tex\"\n\njobs:\n  lint:\n    permissions:\n      checks: write\n      contents: read\n      pull-requests: write\n    runs-on: ubuntu-20.04\n    timeout-minutes: 10\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Setup node environment\n        uses: actions/setup-node@v3\n        with:\n          cache: npm\n          node-version: \"18.x\"\n\n      - uses: reviewdog/action-setup@v1\n        with:\n          reviewdog_version: latest\n\n      # 依存関係の構築とtextlintの実行\n      - name: Run lint\n        run: |\n          npm install\n          npm run lint\n\n      # ↑のステップでtextlintが違反を検知した場合のみ実行\n      - name: Run reviewdog\n        if: failure()\n        env:\n          REVIEWDOG_GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          npm run --silent lint-style | reviewdog -f=checkstyle -name=\"textlint\" -diff=\"Git diff HEAD^\" -reporter=github-pr-review\n",
    "source": "pddg/latex-template-ja",
    "path": ".github/workflows/lint.yaml",
    "url": "https://github.com/pddg/latex-template-ja/blob/b70233e3e83958d22d7e42ad121d9eafa77c3721/.github/workflows/lint.yaml",
    "retrieved_at": "2025-08-19T01:45:39.022048Z"
  },
  {
    "question": "What triggers this workflow, and how often does it run automatically?",
    "answer": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nname: Publish nightly artifacts\n\non:\n  schedule:\n    - cron: \"0 0 * * *\"\n  workflow_dispatch:\n\npermissions:\n  contents: read\n\njobs:\n  publish-nightly:\n    name: Publish nightly\n    runs-on: ubuntu-latest\n    if: github.repository == 'apache/pekko-persistence-dynamodb'\n    steps:\n      - name: Checkout\n        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n        with:\n          fetch-depth: 0\n          fetch-tags: true\n\n      - name: Setup Java 8\n        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00 # v4.7.1\n        with:\n          distribution: temurin\n          java-version: 8\n\n      - name: Install sbt\n        uses: sbt/setup-sbt@6c68d2fe8dfbc0a0534d70101baa2e0420e1a506 # v1.1.9\n\n      - name: Publish to Apache Maven repo\n        env:\n          NEXUS_USER: ${{ secrets.NEXUS_USER }}\n          NEXUS_PW: ${{ secrets.NEXUS_PW }}\n        run: sbt +publish\n\n      - name: Cache Coursier cache\n        uses: coursier/cache-action@4e2615869d13561d626ed48655e1a39e5b192b3c # v6.4.7\n",
    "source": "apache/pekko-persistence-dynamodb",
    "path": ".github/workflows/publish-nightly.yml",
    "url": "https://github.com/apache/pekko-persistence-dynamodb/blob/f88b6b9b20b4d5cec7285a5f049da220998f2dcb/.github/workflows/publish-nightly.yml",
    "retrieved_at": "2025-08-20T01:44:10.404579Z"
  },
  {
    "question": "Under what conditions will the workflow execute a dry run of the npm publish?",
    "answer": "name: Publish npm package\n\non:\n  workflow_dispatch:\n    inputs:\n      dry-run:\n        description: 'Dry run'\n        required: true\n        type: boolean\n        default: true\n  schedule:\n    - cron: '48 3 * * 1' # 3:48 AM UTC every Monday\n\njobs:\n  preflight:\n    name: Preflight\n    runs-on: ubuntu-latest\n    outputs:\n      dry-run: ${{ steps.get-dry-run.outputs.dry-run }}\n\n    steps:\n      - name: Get dry run\n        id: get-dry-run\n        shell: pwsh\n        run: |\n          $IsDryRun = '${{ github.event.inputs.dry-run }}' -Eq 'true' -Or '${{ github.event_name }}' -Eq 'schedule'\n\n          if ($IsDryRun) {\n            echo \"dry-run=true\" >> $Env:GITHUB_OUTPUT\n          } else {\n            echo \"dry-run=false\" >> $Env:GITHUB_OUTPUT\n          }\n\n  build:\n    name: Build package\n    runs-on: ubuntu-latest\n    needs:\n      - preflight\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup wasm-pack\n        shell: bash\n        run: |\n          curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh      \n\n      - name: Install dependencies\n        shell: pwsh\n        run: |\n          Set-Location -Path ./web-client/iron-remote-gui/\n          npm install\n\n      - name: Build package\n        shell: pwsh\n        run: |\n          Set-PSDebug -Trace 1\n\n          Set-Location -Path ./web-client/iron-remote-gui/\n          npm run build\n          Set-Location -Path ./dist\n          npm pack\n\n      - name: Harvest package\n        shell: pwsh\n        run: |\n          Set-PSDebug -Trace 1\n\n          New-Item -ItemType \"directory\" -Path . -Name \"npm-packages\"\n          Get-ChildItem -Path ./web-client/ -Recurse *.tgz | ForEach { Copy-Item $_ \"./npm-packages\" }\n\n      - name: Upload package artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: npm\n          path: npm-packages/*.tgz\n\n  publish:\n    name: Publish package\n    runs-on: ubuntu-latest\n    if: github.event_name == 'workflow_dispatch'\n    environment: publish\n    needs:\n      - preflight\n      - build\n\n    steps:\n      - name: Download NPM packages artifact\n        uses: actions/download-artifact@v4\n        with:\n          name: npm\n          path: npm-packages\n\n      - name: Prepare npm\n        shell: pwsh\n        run: npm config set \"//registry.npmjs.org/:_authToken=${{ secrets.NPM_TOKEN }}\"\n\n      - name: Publish\n        shell: pwsh\n        run: |\n          Set-PSDebug -Trace 1\n\n          $isDryRun = '${{ needs.preflight.outputs.dry-run }}' -Eq 'true'\n\n          $files = Get-ChildItem -Recurse npm-packages/*.tgz\n\n          foreach ($file in $files) {\n            Write-Host \"Publishing $($File.Name)...\"\n\n            $publishCmd = @(\n              'npm', \n              'publish', \n              \"$File\",\n              '--access=public'\n            )\n\n            if ($isDryRun) {\n              $publishCmd += '--dry-run'\n            }\n\n            $publishCmd = $publishCmd -Join ' '\n            Invoke-Expression $publishCmd\n          }\n\n  notify:\n    name: Notify failure\n    runs-on: ubuntu-latest\n    if: ${{ always() && contains(needs.*.result, 'failure') && github.event_name == 'schedule' }}\n    needs:\n      - preflight\n      - build\n    env:\n      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_ARCHITECTURE }}\n      SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n    steps:\n      - name: Send slack notification\n        id: slack\n        uses: slackapi/slack-github-action@v1.26.0\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \"*${{ github.repository }}* :fire::fire::fire::fire::fire: \\n The scheduled build for *${{ github.repository }}* is <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|broken>\"\n                  }\n                }\n              ]\n            }\n",
    "source": "lidarbtc/ironrdp",
    "path": ".github/workflows/npm-publish.yml",
    "url": "https://github.com/lidarbtc/ironrdp/blob/27650265c811f80b4a23edd448cde487d37f0664/.github/workflows/npm-publish.yml",
    "retrieved_at": "2025-08-20T01:44:11.203949Z"
  },
  {
    "question": "What is the purpose of the `GH_PAT` secret used in the deploy step?",
    "answer": "name: Deploy on GitHub Pages\n\non:\n  push:\n    branches:\n    - master\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n    - name: Yarn install\n      uses: Borales/actions-yarn@v2.3.0\n      with:\n        cmd: install\n    - name: Yarn build\n      uses: Borales/actions-yarn@v2.3.0\n      with:\n        cmd: build:prod\n    - name: Deploy\n      uses: maxheld83/ghpages@v0.3.0\n      env:\n        BUILD_DIR: dist/\n        GH_PAT: ${{ secrets.GH_PAT }}\n",
    "source": "Cardoso-topdev/vue_template",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/Cardoso-topdev/vue_template/blob/6dbd6b9c3c0d9c28e7a88253da87158cca6d72f7/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-20T01:44:11.900755Z"
  },
  {
    "question": "What event triggers this workflow to run?",
    "answer": "name: e2e\n\non: push\n\njobs:\n  e2e:\n    name: E2E tests\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - uses: pnpm/action-setup@v2\n        with:\n          version: 8\n      - name: Install cypress\n        uses: cypress-io/github-action@v5\n        with:\n          runTests: false\n      - run: pnpm build\n      - name: Cypress run\n        uses: cypress-io/github-action@v6\n        with:\n          install: false\n          working-directory: apps/web\n          start: pnpm start\n          wait-on: \"http://localhost:3000\"\n        env:\n          NEXT_PUBLIC_SITE_URL: ${{secrets.NEXT_PUBLIC_SITE_URL}}\n          DATABASE_URL: ${{secrets.DATABASE_URL}}\n          STRIPE_SECRET_KEY: ${{secrets.STRIPE_SECRET_KEY}}\n",
    "source": "284247028/supastarter-nextjs",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/284247028/supastarter-nextjs/blob/efd0e6fec2a94bfdefa9e99ca2725ce6ee3b292b/.github/workflows/main.yml",
    "retrieved_at": "2025-08-20T01:44:12.588345Z"
  },
  {
    "question": "What is the purpose of the `docs:build` script executed during the build step?",
    "answer": "name: Qiankun Github Pages Deploy\non:\n  push:\n    branches:\n      - master\njobs:\n  deploy-gh-pages:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: build\n        run: |\n          yarn\n          yarn docs:build\n\n      - name: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./dist\n",
    "source": "baron65/qianKun",
    "path": ".github/workflows/github-pages.yml",
    "url": "https://github.com/baron65/qianKun/blob/513293336bb536e4ab329cd3db9653b4b5c4df49/.github/workflows/github-pages.yml",
    "retrieved_at": "2025-08-20T01:44:13.384986Z"
  },
  {
    "question": "Under what conditions does this workflow deploy the Hugo site to GitHub Pages?",
    "answer": "name: GitHub Pages\n\non:\n  push:\n    branches:\n      - main  # Set a branch to deploy\n  pull_request:\n\njobs:\n  deploy:\n    runs-on: ubuntu-20.04\n    concurrency:\n      group: ${{ github.workflow }}-${{ github.ref }}\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          submodules: true  # Fetch Hugo themes (true OR recursive)\n          fetch-depth: 0    # Fetch all history for .GitInfo and .Lastmod\n\n      - name: Setup Hugo\n        uses: peaceiris/actions-hugo@v2\n        with:\n          hugo-version: '0.85.0'\n          # extended: true\n\n      - name: Build\n        run: hugo --minify\n\n      - name: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        if: ${{ github.ref == 'refs/heads/main' }}\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./public",
    "source": "sharadcodes/hugo-theme-serial-programmer",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/sharadcodes/hugo-theme-serial-programmer/blob/df0f7ab4cb4a48d5a9fabdae21ed8d5d83e02b99/.github/workflows/main.yml",
    "retrieved_at": "2025-08-20T01:44:14.050529Z"
  },
  {
    "question": "What actions are performed to build the Hugo site in this workflow?",
    "answer": "name: Build\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  workflow_dispatch:\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]\n        hugo_version: [\"latest\"]\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Hugo setup\n        uses: peaceiris/actions-hugo@v2\n        with:\n          hugo-version: ${{ matrix.hugo_version }}\n          extended: true\n\n      - uses: actions/setup-node@v3\n        with:\n          node-version: '16'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Build\n        run: hugo -v --source=exampleSite --themesDir ../.. --gc\n",
    "source": "minseong0609/hugo_blog",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/minseong0609/hugo_blog/blob/f925223e9480ed0556ad8a8d1b5efd6559aa8dfc/.github/workflows/build.yml",
    "retrieved_at": "2025-08-20T01:44:14.795432Z"
  },
  {
    "question": "Does this workflow run molecule tests on every push and pull request to the master branch?",
    "answer": "---\nname: ipr-cnrs.nftables.molecule\n\non:\n  push:\n    branches: [master]\n  pull_request:\n    branches: [master]\n\n  workflow_dispatch:\n\njobs:\n  test:\n    runs-on:  ubuntu-latest\n    steps:\n\n      - name: checkout\n        uses: actions/checkout@v2\n        with:\n          path: \"${{ github.repository }}\"\n\n      - name: Ansible Molecule\n        uses: robertdebock/molecule-action@4.0.9\n        with:\n          molecule_working_dir: \"${{ github.repository }}\"\n",
    "source": "chrisvanmeer/ansible-role-nftables",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/chrisvanmeer/ansible-role-nftables/blob/c4da513d08a24df4ae042b85a10d9d3f7a27ca8a/.github/workflows/main.yml",
    "retrieved_at": "2025-08-20T01:44:15.432056Z"
  },
  {
    "question": "Under what conditions does the workflow build and push the \"latest\" tagged Docker image?",
    "answer": "name: Build & Publish PodDefaults Docker image\non:\n  push:\n    branches:\n      - master\n      - v*-branch\n    paths:\n      - components/admission-webhook/**\n      - releasing/version/VERSION\n\nenv:\n  DOCKER_USER: kubeflownotebookswg\n  IMG: kubeflownotebookswg/poddefaults-webhook\n  ARCH: linux/ppc64le,linux/amd64\n\njobs:\n  push_to_registry:\n    name: Build & Push Docker image to Docker Hub\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n\n    - uses: dorny/paths-filter@v2\n      id: filter\n      with:\n        filters: |\n          version:\n            - 'releasing/version/VERSION'\n\n    - name: Login to DockerHub\n      uses: docker/login-action@v2\n      with:\n        username: ${{ env.DOCKER_USER }}\n        password: ${{ secrets.KUBEFLOWNOTEBOOKSWG_DOCKER_TOKEN }}\n\n    - name: Setup QEMU\n      uses: docker/setup-qemu-action@v2\n\n    - name: Setup Docker Buildx\n      uses: docker/setup-buildx-action@v2\n\n    - name: Build and push multi-arch docker image\n      run: |\n        cd components/admission-webhook\n        make docker-build-push-multi-arch\n\n    - name: Build and push latest multi-arch docker image\n      if: github.ref == 'refs/heads/master'\n      run: |\n        export TAG=latest\n        cd components/admission-webhook\n        make docker-build-push-multi-arch\n\n    - name: Build and push multi-arch docker image on Version change\n      id: version\n      if: steps.filter.outputs.version == 'true'\n      run: |\n        export TAG=$(cat releasing/version/VERSION)\n        cd components/admission-webhook\n        make docker-build-push-multi-arch\n",
    "source": "Zveroloff/kubeflow",
    "path": ".github/workflows/poddefaults_docker_publish.yaml",
    "url": "https://github.com/Zveroloff/kubeflow/blob/1552c90ccaee7acdea3495175592fcca46ae3a77/.github/workflows/poddefaults_docker_publish.yaml",
    "retrieved_at": "2025-08-20T01:44:16.317719Z"
  },
  {
    "question": "What specific contribution graph is generated and pushed to the repository by this workflow?",
    "answer": "name: GitHub-Profile-3D-Contrib\n\non:\n  schedule: # 03:00 JST == 18:00 UTC\n    - cron: \"0 18 * * *\"\n  workflow_dispatch:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    name: generate-github-profile-3d-contrib\n    steps:\n      - uses: actions/checkout@v2\n      - uses: yoshi389111/github-profile-3d-contrib@0.7.0\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          USERNAME: ${{ github.repository_owner }}\n      - name: Commit & Push\n        run: |\n          git config user.name github-actions\n          git config user.email github-actions@github.com\n          git add -A .\n          git commit -m \"generated\"\n          git push",
    "source": "Maximilian-Stefan-Ernst/Maximilian-Stefan-Ernst",
    "path": ".github/workflows/contributions.yml",
    "url": "https://github.com/Maximilian-Stefan-Ernst/Maximilian-Stefan-Ernst/blob/89c52643a1b5bc4a3eb2e4143f6e020acfe79ac0/.github/workflows/contributions.yml",
    "retrieved_at": "2025-08-20T01:44:17.095344Z"
  },
  {
    "question": "What is the purpose of the `send_results` job, and what kind of information does it send to Slack?",
    "answer": "name: Self-hosted runner (scheduled)\n\non:\n  push:\n    branches:\n      - multi_ci_*\n  repository_dispatch:\n  schedule:\n    - cron: \"0 0 * * *\"\n\nenv:\n  HF_HOME: /mnt/cache\n  TRANSFORMERS_IS_CI: yes\n  RUN_SLOW: yes\n  OMP_NUM_THREADS: 16\n  MKL_NUM_THREADS: 16\n  PYTEST_TIMEOUT: 600\n  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n\njobs:\n  run_all_tests_torch_gpu:\n    runs-on: [self-hosted, docker-gpu, single-gpu]\n    container:\n      image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev git espeak-ng\n          pip install --upgrade pip\n          pip install .[integrations,sklearn,testing,onnxruntime,sentencepiece,torch-speech,vision,timm]\n          pip install https://github.com/kpu/kenlm/archive/master.zip\n          python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n          wandb login ${{ secrets.WANDB_API_KEY }}\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          utils/print_env_pt.py\n\n      - name: Run all tests on GPU\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_gpu_durations.txt\n\n      - name: Run examples tests on GPU\n        if: ${{ always() }}\n        env:\n          OMP_NUM_THREADS: 16\n          MKL_NUM_THREADS: 16\n          RUN_SLOW: yes\n          HF_HOME: /mnt/cache\n          TRANSFORMERS_IS_CI: yes\n        run: |\n          pip install -r examples/pytorch/_tests_requirements.txt\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=examples_torch_gpu examples\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/examples_torch_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/examples_torch_gpu_durations.txt\n\n      - name: Run all pipeline tests on GPU\n        if: ${{ always() }}\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=tests_torch_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_pipeline_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_pipeline_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_tests_torch_gpu_test_reports\n          path: reports\n\n#  run_all_tests_flax_gpu:\n#    runs-on: [self-hosted, docker-gpu-test, single-gpu]\n#    container:\n#      image: tensorflow/tensorflow:2.4.1-gpu\n#      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n#    steps:\n#      - name: Launcher docker\n#        uses: actions/checkout@v2\n#\n#      - name: NVIDIA-SMI\n#        continue-on-error: true\n#        run: |\n#          nvidia-smi\n#\n#      - name: Install dependencies\n#        run: |\n#          pip install --upgrade pip\n#          pip install --upgrade \"jax[cuda111]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n#          pip install .[flax,integrations,sklearn,testing,sentencepiece,flax-speech,vision]\n#          pip install https://github.com/kpu/kenlm/archive/master.zip\n#\n#      - name: Are GPUs recognized by our DL frameworks\n#        run: |\n#          python -c \"from jax.lib import xla_bridge; print('GPU available:', xla_bridge.get_backend().platform)\"\n#          python -c \"import jax; print('Number of GPUs available:', len(jax.local_devices()))\"\n#\n#      - name: Run all tests on GPU\n#        run: |\n#          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_flax_gpu tests\n#\n#      - name: Failure short reports\n#        if: ${{ always() }}\n#        run: cat reports/tests_flax_gpu_failures_short.txt\n#\n#      - name: Test durations\n#        if: ${{ always() }}\n#        run: cat reports/tests_flax_gpu_durations.txt\n#\n#      - name: Test suite reports artifacts\n#        if: ${{ always() }}\n#        uses: actions/upload-artifact@v2\n#        with:\n#          name: run_all_tests_flax_gpu_test_reports\n#          path: reports\n\n  run_all_tests_tf_gpu:\n    runs-on: [self-hosted, docker-gpu, single-gpu]\n    container:\n      image: tensorflow/tensorflow:2.4.1-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev git espeak-ng\n          pip install --upgrade pip\n          pip install .[sklearn,testing,onnx,sentencepiece,tf-speech,vision]\n          pip install https://github.com/kpu/kenlm/archive/master.zip\n\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          TF_CPP_MIN_LOG_LEVEL=3 python -c \"import tensorflow as tf; print('TF GPUs available:', bool(tf.config.list_physical_devices('GPU')))\"\n          TF_CPP_MIN_LOG_LEVEL=3 python -c \"import tensorflow as tf; print('Number of TF GPUs available:', len(tf.config.list_physical_devices('GPU')))\"\n\n      - name: Run all tests on GPU\n        env:\n          TF_NUM_INTEROP_THREADS: 1\n          TF_NUM_INTRAOP_THREADS: 16\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_tf_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_tf_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_tf_gpu_durations.txt\n\n      - name: Run all pipeline tests on GPU\n        if: ${{ always() }}\n        env:\n          RUN_PIPELINE_TESTS: yes\n          TF_NUM_INTEROP_THREADS: 1\n          TF_NUM_INTRAOP_THREADS: 16\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=tests_tf_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_tf_pipeline_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_tf_pipeline_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_tests_tf_gpu_test_reports\n          path: reports\n\n  run_all_examples_torch_xla_tpu:\n    runs-on: [self-hosted, docker-tpu-test, tpu-v3-8]\n    container:\n      image: gcr.io/tpu-pytorch/xla:nightly_3.8_tpuvm\n      options: --privileged -v \"/lib/libtpu.so:/lib/libtpu.so\" -v /mnt/cache/.cache/huggingface:/mnt/cache/ --shm-size 16G\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: Install dependencies\n        run: |\n          pip install --upgrade pip\n          pip install .[testing]\n\n      - name: Are TPUs recognized by our DL frameworks\n        env:\n          XRT_TPU_CONFIG: localservice;0;localhost:51011\n        run: |\n          python -c \"import torch_xla.core.xla_model as xm; print(xm.xla_device())\"\n\n      - name: Run example tests on TPU\n        env:\n          XRT_TPU_CONFIG: \"localservice;0;localhost:51011\"\n          MKL_SERVICE_FORCE_INTEL: \"1\"  # See: https://github.com/pytorch/pytorch/issues/37377\n\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_xla_tpu examples/pytorch/test_xla_examples.py\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_xla_tpu_failures_short.txt\n\n      - name: Tests durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_xla_tpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_examples_torch_xla_tpu\n          path: reports\n\n  run_all_tests_torch_multi_gpu:\n    runs-on: [self-hosted, docker-gpu, multi-gpu]\n    container:\n      image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime\n      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        continue-on-error: true\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev git espeak-ng\n          pip install --upgrade pip\n          pip install .[integrations,sklearn,testing,onnxruntime,sentencepiece,torch-speech,vision,timm]\n          pip install https://github.com/kpu/kenlm/archive/master.zip\n          python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n          wandb login ${{ secrets.WANDB_API_KEY }}\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          utils/print_env_pt.py\n\n      - name: Run all tests on GPU\n        env:\n          MKL_SERVICE_FORCE_INTEL: 1\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_multi_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_multi_gpu_durations.txt\n\n      - name: Run all pipeline tests on GPU\n        if: ${{ always() }}\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=tests_torch_pipeline_multi_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_pipeline_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_pipeline_multi_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_tests_torch_multi_gpu_test_reports\n          path: reports\n\n  run_all_tests_tf_multi_gpu:\n    runs-on: [self-hosted, docker-gpu, multi-gpu]\n    container:\n      image: tensorflow/tensorflow:2.4.1-gpu\n      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        continue-on-error: true\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev git espeak-ng\n          pip install --upgrade pip\n          pip install .[sklearn,testing,onnx,sentencepiece,tf-speech,vision]\n          pip install https://github.com/kpu/kenlm/archive/master.zip\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          TF_CPP_MIN_LOG_LEVEL=3 python -c \"import tensorflow as tf; print('TF GPUs available:', bool(tf.config.list_physical_devices('GPU')))\"\n          TF_CPP_MIN_LOG_LEVEL=3 python -c \"import tensorflow as tf; print('Number of TF GPUs available:', len(tf.config.list_physical_devices('GPU')))\"\n\n      - name: Run all tests on GPU\n        env:\n          TF_NUM_INTEROP_THREADS: 1\n          TF_NUM_INTRAOP_THREADS: 16\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_tf_multi_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_tf_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_tf_multi_gpu_durations.txt\n\n      - name: Run all pipeline tests on GPU\n        if: ${{ always() }}\n        env:\n          RUN_PIPELINE_TESTS: yes\n          TF_NUM_INTEROP_THREADS: 1\n          TF_NUM_INTRAOP_THREADS: 16\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=tests_tf_pipeline_multi_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_tf_pipeline_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_tf_pipeline_multi_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_all_tests_tf_multi_gpu_test_reports\n          path: reports\n\n#  run_all_tests_flax_multi_gpu:\n#    runs-on: [self-hosted, docker-gpu, multi-gpu]\n#    container:\n#      image: tensorflow/tensorflow:2.4.1-gpu\n#      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n#    steps:\n#      - name: Launcher docker\n#        uses: actions/checkout@v2\n#\n#      - name: NVIDIA-SMI\n#        run: |\n#          nvidia-smi\n#\n#      - name: Install dependencies\n#        run: |\n#          pip install --upgrade pip\n#          pip install --upgrade \"jax[cuda111]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n#          pip install .[flax,integrations,sklearn,testing,sentencepiece,flax-speech,vision]\n#\n#      - name: Are GPUs recognized by our DL frameworks\n#        run: |\n#          python -c \"from jax.lib import xla_bridge; print('GPU available:', xla_bridge.get_backend().platform)\"\n#          python -c \"import jax; print('Number of GPUs available:', len(jax.local_devices()))\"\n#\n#      - name: Run all tests on GPU\n#        run: |\n#          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_flax_gpu tests\n#\n#      - name: Failure short reports\n#        if: ${{ always() }}\n#        run: cat reports/tests_flax_gpu_failures_short.txt\n#\n#      - name: Test suite reports artifacts\n#        if: ${{ always() }}\n#        uses: actions/upload-artifact@v2\n#        with:\n#          name: run_all_tests_flax_gpu_test_reports\n#          path: reports\n\n  run_all_tests_torch_cuda_extensions_gpu:\n    runs-on: [self-hosted, docker-gpu, single-gpu]\n    container:\n      image: nvcr.io/nvidia/pytorch:21.03-py3\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libaio-dev\n          pip install --upgrade pip\n          pip install .[testing,deepspeed]\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          utils/print_env_pt.py\n\n      - name: Run all tests on GPU\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_cuda_extensions_gpu tests/deepspeed tests/extended\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_cuda_extensions_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_cuda_extensions_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_tests_torch_cuda_extensions_gpu_test_reports\n          path: reports\n\n  run_all_tests_torch_cuda_extensions_multi_gpu:\n    runs-on: [self-hosted, docker-gpu, multi-gpu]\n    container:\n      image: nvcr.io/nvidia/pytorch:21.03-py3\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        continue-on-error: true\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libaio-dev\n          pip install --upgrade pip\n          rm -rf ~/.cache/torch_extensions/ # shared between conflicting builds\n          pip install .[testing,deepspeed,fairscale]\n\n      - name: Are GPUs recognized by our DL frameworks\n        run: |\n          utils/print_env_pt.py\n\n      - name: Run all tests on GPU\n        run: |\n          python -m pytest -n 1 -v --dist=loadfile --make-reports=tests_torch_cuda_extensions_multi_gpu tests/deepspeed tests/extended\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: cat reports/tests_torch_cuda_extensions_multi_gpu_failures_short.txt\n\n      - name: Test durations\n        if: ${{ always() }}\n        run: cat reports/tests_torch_cuda_extensions_multi_gpu_durations.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_tests_torch_cuda_extensions_multi_gpu_test_reports\n          path: reports\n\n  send_results:\n    name: Send results to webhook\n    runs-on: ubuntu-latest\n    if: always()\n    needs: [\n        run_all_tests_torch_gpu,\n        run_all_tests_tf_gpu,\n        run_all_tests_torch_multi_gpu,\n        run_all_tests_tf_multi_gpu,\n        run_all_tests_torch_cuda_extensions_gpu,\n        run_all_tests_torch_cuda_extensions_multi_gpu\n    ]\n    steps:\n      - uses: actions/checkout@v2\n\n      - uses: actions/download-artifact@v2\n\n      - name: Send message to Slack\n        env:\n          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n\n\n        run: |\n          pip install slack_sdk\n          python utils/notification_service.py scheduled\n",
    "source": "reynolds9808/transformers_backdoor_attack",
    "path": ".github/workflows/self-scheduled.yml",
    "url": "https://github.com/reynolds9808/transformers_backdoor_attack/blob/e9536ea0046703f854fa0c05cfdcf179000bb1a2/.github/workflows/self-scheduled.yml",
    "retrieved_at": "2025-08-21T01:42:57.083030Z"
  },
  {
    "question": "Under what conditions does the `setup` job execute based on the release name?",
    "answer": "name: rAPId Release\n\non:\n  release:\n    types: [released]\n\njobs:\n  setup:\n    if: \"${{ startsWith(github.event.release.name, 'SDK: ') }}\"\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Log commit SHA\n        run: echo $GITHUB_SHA\n\n  sdk-release:\n    needs:\n      - setup\n    runs-on: self-hosted\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Populate .env with additional vars\n        run: |\n          echo TWINE_USERNAME=${{ secrets.TWINE_USERNAME }} >> .env\n          echo TWINE_PASSWORD=${{ secrets.TWINE_PASSWORD }} >> .env\n          echo TWINE_NON_INTERACTIVE=true >> .env\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n          cache: 'pip'\n\n      - name: Setup Python Environment\n        run: |\n          make sdk/setup\n\n      - name: SDK Release\n        run: make sdk/release\n",
    "source": "no10ds/rapid",
    "path": ".github/workflows/release_sdk.yml",
    "url": "https://github.com/no10ds/rapid/blob/c763da3d92b4371effa90f6df460c4d027b899c6/.github/workflows/release_sdk.yml",
    "retrieved_at": "2025-08-21T01:42:57.720317Z"
  },
  {
    "question": "What tests are performed to verify the SDL2 build for the Sony Playstation Portable?",
    "answer": "name: Build (Sony Playstation Portable)\n\non: [push, pull_request]\n\njobs:\n  psp:\n    runs-on: ubuntu-latest\n    container: pspdev/pspdev:latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Setup dependencies\n      run: |\n        apk update \n        apk add cmake gmp mpc1 mpfr4 make pkgconf\n    - name: Configure CMake\n      run: |\n        cmake -S . -B build \\\n          -DCMAKE_TOOLCHAIN_FILE=$PSPDEV/psp/share/pspdev.cmake \\\n          -DSDL_WERROR=ON \\\n          -DSDL_TESTS=ON \\\n          -DSDL_INSTALL_TESTS=ON \\\n          -DCMAKE_BUILD_TYPE=Release \\\n          -DCMAKE_INSTALL_PREFIX=prefix\n    - name: Build\n      run: cmake --build build --config Release\n    - name: Install\n      run: |\n        echo \"SDL2_DIR=$(pwd)/prefix\" >> $GITHUB_ENV\n        cmake --install build --config Release\n        ( cd prefix; find ) | LC_ALL=C sort -u\n    - name: Verify CMake configuration files\n      run: |\n        cmake -S cmake/test -B cmake_config_build \\\n          -DCMAKE_TOOLCHAIN_FILE=$PSPDEV/psp/share/pspdev.cmake \\\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }} \\\n          -DTEST_SHARED=FALSE \\\n          -DCMAKE_BUILD_TYPE=Release\n        cmake --build cmake_config_build --verbose\n    - name: Verify sdl2-config\n      run: |\n        export CC=psp-gcc\n        export PATH=${{ env.SDL2_DIR }}/bin:$PATH\n        export EXTRA_LDFLAGS=\"-L$PSPDEV/lib -L$PSPDEV/psp/lib -L$PSPDEV/psp/sdk/lib\"\n        cmake/test/test_sdlconfig.sh\n    - name: Verify sdl2.pc\n      run: |\n        export CC=psp-gcc\n        export PKG_CONFIG_PATH=${{ env.SDL2_DIR }}/lib/pkgconfig\n        export EXTRA_LDFLAGS=\"-L$PSPDEV/lib -L$PSPDEV/psp/lib -L$PSPDEV/psp/sdk/lib\"\n        cmake/test/test_pkgconfig.sh\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/psp.yaml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/psp.yaml",
    "retrieved_at": "2025-08-21T01:42:58.589925Z"
  },
  {
    "question": "What triggers this workflow, and what schedule does it follow?",
    "answer": "name: Doctests\n\non:\n  push:\n    branches:\n      - doctest*\n  repository_dispatch:\n  schedule:\n    - cron: \"0 0 * * *\"\n\n\nenv:\n  HF_HOME: /mnt/cache\n  TRANSFORMERS_IS_CI: yes\n  RUN_SLOW: yes\n  OMP_NUM_THREADS: 16\n  MKL_NUM_THREADS: 16\n  PYTEST_TIMEOUT: 600\n\njobs:\n  run_doctests:\n    runs-on: [self-hosted, docker-gpu-test, single-gpu]\n    container:\n      image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Launcher docker\n        uses: actions/checkout@v2\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: Install dependencies\n        run: |\n          apt -y update && apt install -y libsndfile1-dev\n          pip install --upgrade pip\n          pip install .[testing,torch-speech]\n\n      - name: Prepare files for doctests\n        run: |\n          python utils/prepare_for_doc_test.py src docs\n\n      - name: Run doctests\n        run: |\n          pytest --doctest-modules $(cat utils/documentation_tests.txt) -sv --doctest-continue-on-failure --doctest-glob=\"*.mdx\"\n\n      - name: Clean files after doctests\n        run: |\n          python utils/prepare_for_doc_test.py src docs --remove_new_line\n",
    "source": "datakloud/transformers-play",
    "path": ".github/workflows/doctests.yml",
    "url": "https://github.com/datakloud/transformers-play/blob/f380bf2b612e6030ef8bc8904b287d274f035e29/.github/workflows/doctests.yml",
    "retrieved_at": "2025-08-21T01:42:59.348340Z"
  },
  {
    "question": "Under what conditions will this workflow be triggered, specifically regarding the `renovate/**` branches?",
    "answer": "name: CI\n\non:\n  workflow_dispatch:\n  pull_request:\n  push:\n    branches:\n      - dev\n      - 'renovate/**'\n      - '!renovate/lock-file-maintenance'\n\njobs:\n  CI:\n    strategy:\n      fail-fast: false\n\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: install pnpm\n        uses: pnpm/action-setup@v2\n        with:\n          version: 7.x.x\n\n      - name: Setup Node\n        uses: actions/setup-node@v3\n        with:\n          node-version: 16\n          cache: pnpm\n\n      - name: install dependencies\n        run: pnpm install\n\n      - name: check formatting\n        run: pnpm format\n\n      - name: run linter\n        run: pnpm lint\n\n      #- name: run tests\n      #  run: pnpm test\n",
    "source": "Josem1801/tauri",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/Josem1801/tauri/blob/4b5b72d6abe9770f5482d75dea7befa786be36bb/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-21T01:42:59.974147Z"
  },
  {
    "question": "What actions are triggered when a pull request is opened, synchronized, or reopened?",
    "answer": "name: Pull Request\n\non:\n  pull_request:\n    types: [ opened, synchronize, reopened ]\n\npermissions:\n  contents: write\n  actions: read\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Validate Gradle Wrapper\n        uses: gradle/wrapper-validation-action@v2\n\n      - name: Setup JDK\n        uses: actions/setup-java@v4\n        with:\n          distribution: 'zulu'\n          java-version: 17\n\n      - name: Setup Gradle\n        uses: gradle/gradle-build-action@v3\n\n      - name: Check spotless\n        run: ./gradlew spotlessCheck\n\n      - name: Build App\n        run: ./gradlew :app:assembleDebug\n",
    "source": "igoracad/socialiteWidget",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/igoracad/socialiteWidget/blob/8069627a302bca2a52fff2bc6c5135a2558fc002/.github/workflows/build.yml",
    "retrieved_at": "2025-08-21T01:43:00.766236Z"
  },
  {
    "question": "What kind of evaluations (ESLint, Jest, store) are performed on the pull request, and in what order?",
    "answer": "on:\n  pull_request:\n    types: [opened, synchronize]\n    \njobs:\n  evaluator:\n    runs-on: self-hosted\n    name: Evaluator\n    services:\n      mysql:\n        image: mysql:8.0.34\n        env:\n          MYSQL_ROOT_PASSWORD: 'password'\n        ports:\n          - 3306:3306\n        options: --health-cmd=\"mysqladmin ping\" --health-interval=10s --health-timeout=5s --health-retries=3\n    steps:    \n      - name: Fetch project repository\n        uses: actions/checkout@v2\n\n      - name: Fetch Blocked Files Checkout action\n        uses: actions/checkout@v2\n        with:\n          repository: betrybe/blocked-files-checkout-action\n          ref: v2\n          token: ${{ secrets.GIT_HUB_PAT }}\n          path: .github/actions/blocked-files-checkout\n\n      - name: Fetch ESLint evaluator\n        uses: actions/checkout@v2\n        with:\n          repository: betrybe/eslint-linter-action\n          ref: v3.4\n          token: ${{ secrets.GIT_HUB_PAT }}\n          path: .github/actions/eslint-evaluator\n\n      - name: Fetch Jest evaluator\n        uses: actions/checkout@v2\n        with:\n          repository: betrybe/jest-evaluator-action\n          ref: v9.5\n          token: ${{ secrets.GIT_HUB_PAT }}\n          path: .github/actions/jest-evaluator\n\n      - name: Fetch Store evaluation\n        uses: actions/checkout@v2\n        with:\n          repository: betrybe/store-evaluation-action\n          ref: v8.0\n          token: ${{ secrets.GIT_HUB_PAT }}\n          path: .github/actions/store-evaluation\n\n      - name: Setup NodeJS\n        uses: actions/setup-node@v1.4.4\n        with:\n          node-version: '16'\n\n      - name: Restore protected files\n        uses: ./.github/actions/blocked-files-checkout\n        with:\n          restore_branch: 'main'\n\n      - name: Run ESLint evaluator\n        uses: ./.github/actions/eslint-evaluator\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          pr_number: ${{ github.event.pull_request.number }}\n\n      - name: Run Jest evaluation\n        id: evaluator\n        uses: ./.github/actions/jest-evaluator\n        with:\n          pr_author_username: ${{ github.event.pull_request.user.login }}\n        env:\n          MYSQL_USER: 'root'\n          MYSQL_PASSWORD: 'password'\n          MYSQL_HOSTNAME: 'mysql'\n          MYSQL_DATABASE: 'trybe_eval'\n          MYSQL_PORT: 3306\n\n      - name: Run Store evaluation\n        uses: ./.github/actions/store-evaluation\n        with:\n          evaluation-data: ${{ steps.evaluator.outputs.result }}\n          environment: production\n          token: ${{ secrets.GITHUB_TOKEN }}",
    "source": "eduzissimo/Project-Trybefy",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/eduzissimo/Project-Trybefy/blob/e05bf9721ab18303b4e3f78055474fcf6e7840be/.github/workflows/main.yml",
    "retrieved_at": "2025-08-21T01:43:01.521273Z"
  },
  {
    "question": "What pull request event triggers this workflow to add a reminder comment?",
    "answer": "name: Pull Request Opened\npermissions:\n  pull-requests: write\n\n# only trigger on pull request closed events\non:\n  pull_request_target:\n    types: [ opened ]\n\njobs:\n  pr_open_job:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/github-script@d7906e4ad0b1822421a7e6a35d5ca353c962f410 # v6.4.1\n        with:\n          script: |\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: \"Reminder for the PR assignee: If this is a user-visible change, please update the changelog as part of the PR.\"\n            })\n",
    "source": "augur-ai/mantis",
    "path": ".github/workflows/pr-opened.yml",
    "url": "https://github.com/augur-ai/mantis/blob/dce7be5b39e2498d5d4262b5bed15769e10df324/.github/workflows/pr-opened.yml",
    "retrieved_at": "2025-08-21T01:43:02.342533Z"
  },
  {
    "question": "What is the purpose of setting `fetch-depth: 0` in the checkout step?",
    "answer": "# Sample workflow for building and deploying a VitePress site to GitHub Pages\n#\nname: Deploy VitePress site to Pages\n\non:\n  # Runs on pushes targeting the `main` branch. Change this to `master` if you're\n  # using the `master` branch as the default branch.\n  push:\n    branches: [master]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\n# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\nconcurrency:\n  group: pages\n  cancel-in-progress: false\n\njobs:\n  # Build job\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0 # Not needed if lastUpdated is not enabled\n      # - uses: pnpm/action-setup@v3 # Uncomment this if you're using pnpm\n      # - uses: oven-sh/setup-bun@v1 # Uncomment this if you're using Bun\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20\n          cache: npm # or pnpm / yarn\n      - name: Setup Pages\n        uses: actions/configure-pages@v4\n      - name: Install dependencies\n        run: npm ci # or pnpm install / yarn install / bun install\n      - name: Build with VitePress\n        run: npm run docs:build # or pnpm docs:build / yarn docs:build / bun run docs:build\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: .vitepress/dist\n\n  # Deployment job\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    needs: build\n    runs-on: ubuntu-latest\n    name: Deploy\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\n",
    "source": "net-success/vitepress",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/net-success/vitepress/blob/4f1e6d3be5c9b20f40c481dad337b6d71342fe54/.github/workflows/main.yml",
    "retrieved_at": "2025-08-21T01:43:02.896962Z"
  },
  {
    "question": "Does this workflow build the project only when a pull request targets the `master` branch?",
    "answer": "name: Build on PR\n\non:\n  pull_request:\n    branches:\n      - master\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Use Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '20'\n      \n      - name: Install Dependencies\n        run: npm install\n        \n      - name: Run Build\n        run: npm run build\n",
    "source": "deepak-choure/turboWallet",
    "path": ".github/workflows/build.yaml",
    "url": "https://github.com/deepak-choure/turboWallet/blob/6b1ed27c2ceebaeb70261158d201335e8ef8d17e/.github/workflows/build.yaml",
    "retrieved_at": "2025-08-21T01:43:03.601111Z"
  },
  {
    "question": "What benchmarks are run and where are the results sent by this workflow?",
    "answer": "name: Run Benchmarks and upload results\n\non:\n  push:\n    branches:\n    - benchmark  # TODO: change it back to master once we really track the results. We commented this as speed.wasmer.io is failing\n\njobs:\n  run_benchmark:\n    name: Benchmark on ${{ matrix.build }}\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        build: [linux]\n        include:\n          - build: linux\n            os: ubuntu-latest\n    env:\n      SCCACHE_AZURE_BLOB_CONTAINER: wasmerstoragesccacheblob\n      SCCACHE_AZURE_CONNECTION_STRING: ${{ secrets.SCCACHE_AZURE_CONNECTION_STRING }}\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          profile: minimal\n          override: true\n      - name: Configure cargo data directory\n        # After this point, all cargo registry and crate data is stored in\n        # $GITHUB_WORKSPACE/.cargo_home. This allows us to cache only the files\n        # that are needed during the build process. Additionally, this works\n        # around a bug in the 'cache' action that causes directories outside of\n        # the workspace dir to be saved/restored incorrectly.\n        run: echo \"CARGO_HOME=$(pwd)/.cargo_home\" >> $GITHUB_ENV\n      - name: Cache\n        uses: actions/cache@master\n        with:\n          # Note: crates from the git repo always get rebuilt\n          # so we cache only those subdirectories of target/{debug|release} that\n          # contain the build output for crates that come from the registry.\n          path: |-\n            .cargo_home\n            target/*/.*\n            target/*/build\n            target/*/deps\n          key: ${{ matrix.os }}-${{ hashFiles('Cargo.lock') }}\n          restore-keys: |\n            ${{ matrix.os }}\n      - name: Install LLVM (Linux)\n        if: matrix.os == 'ubuntu-latest'\n        run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz -L -o llvm.tar.xz\n          mkdir -p /opt/llvm-10\n          tar xf llvm.tar.xz --strip-components=1 -C /opt/llvm-10\n          echo '/opt/llvm-10/bin' >> $GITHUB_PATH\n          echo 'name=LLVM_SYS_100_PREFIX=/opt/llvm-10' >> $GITHUB_ENV\n      - name: Install Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.8\n      - name: Install Python dependencies\n        run: |\n          pip install codespeed-client\n          pip install toml\n      - name: Run Benchmark\n        run: |\n          make bench\n          git clone https://github.com/wasmerio/wasmer-bench\n\n          python3 wasmer-bench/send_metrics.py\n\n",
    "source": "GeorgKreuzmayr/wasmer",
    "path": ".github/workflows/benchmark.yaml",
    "url": "https://github.com/GeorgKreuzmayr/wasmer/blob/14d8084c29f6c47e76a16fdee2083aa997d34482/.github/workflows/benchmark.yaml",
    "retrieved_at": "2025-08-22T01:43:27.681380Z"
  },
  {
    "question": "What types of tags trigger this workflow?",
    "answer": "name: \"Pre-release\"\n\non:\n  push:\n    tags:\n      - \"v[0-9]+.[0-9]+.[0-9]+-alpha.[0-9]+\"  # e.g. v0.37.0-alpha.1, v0.38.0-alpha.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-beta.[0-9]+\"   # e.g. v0.37.0-beta.1, v0.38.0-beta.10\n      - \"v[0-9]+.[0-9]+.[0-9]+-rc[0-9]+\"      # e.g. v0.37.0-rc1, v0.38.0-rc10\n\njobs:\n  prerelease:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      # Similar check to ./release-version.yml, but enforces this when pushing\n      # tags. The ./release-version.yml check can be bypassed and is mainly\n      # present for informational purposes.\n      - name: Check release version\n        run: |\n          # We strip the refs/tags/v prefix of the tag name.\n          TAG_VERSION=${GITHUB_REF#refs/tags/v}\n          # Get the version of the code, which has no \"v\" prefix.\n          CODE_VERSION=`go run ./cmd/cometbft/ version`\n          if [ \"$TAG_VERSION\" != \"$CODE_VERSION\" ]; then\n            echo \"\"\n            echo \"Tag version ${TAG_VERSION} does not match code version ${CODE_VERSION}\"\n            echo \"\"\n            echo \"Please either fix the release tag or the version of the software in version/version.go.\"\n            exit 1\n          fi\n\n      - name: Generate release notes\n        run: |\n          VERSION=\"${GITHUB_REF#refs/tags/}\"\n          CHANGELOG_URL=\"https://github.com/cometbft/cometbft/blob/${VERSION}/CHANGELOG.md\"\n          echo \"See the [CHANGELOG](${CHANGELOG_URL}) for changes available in this pre-release, but not yet officially released.\" > ../release_notes.md\n\n      - name: Release\n        uses: goreleaser/goreleaser-action@v4\n        with:\n          version: latest\n          args: release --clean --release-notes ../release_notes.md\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n  prerelease-success:\n    needs: prerelease\n    if: ${{ success() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack upon pre-release\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          RELEASE_URL: \"${{ github.server_url }}/${{ github.repository }}/releases/tag/${{ github.ref_name }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":sparkles: New CometBFT pre-release: <${{ env.RELEASE_URL }}|${{ github.ref_name }}>\"\n                  }\n                }\n              ]\n            }\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/pre-release.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/pre-release.yml",
    "retrieved_at": "2025-08-22T01:43:28.348782Z"
  },
  {
    "question": "What action is triggered when a push event occurs with a tag matching the pattern \"v[0-9]+.[0-9]+.[0-9]+\"?",
    "answer": "name: \"Release\"\n\non:\n  push:\n    tags:\n      - \"v[0-9]+.[0-9]+.[0-9]+\"  # Push events to matching v*, i.e. v1.0, v20.15.10\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      # Similar check to ./release-version.yml, but enforces this when pushing\n      # tags. The ./release-version.yml check can be bypassed and is mainly\n      # present for informational purposes.\n      - name: Check release version\n        run: |\n          # We strip the refs/tags/v prefix of the tag name.\n          TAG_VERSION=${GITHUB_REF#refs/tags/v}\n          # Get the version of the code, which has no \"v\" prefix.\n          CODE_VERSION=`go run ./cmd/cometbft/ version`\n          if [ \"$TAG_VERSION\" != \"$CODE_VERSION\" ]; then\n            echo \"\"\n            echo \"Tag version ${TAG_VERSION} does not match code version ${CODE_VERSION}\"\n            echo \"\"\n            echo \"Please either fix the release tag or the version of the software in version/version.go.\"\n            exit 1\n          fi\n\n      - name: Generate release notes\n        run: |\n          VERSION=\"${GITHUB_REF#refs/tags/}\"\n          VERSION_REF=\"${VERSION//[\\.]/}\"\n          CHANGELOG_URL=\"https://github.com/cometbft/cometbft/blob/${VERSION}/CHANGELOG.md#${VERSION_REF}\"\n          echo \"See the [CHANGELOG](${CHANGELOG_URL}) for this release.\" > ../release_notes.md\n\n      - name: Release\n        uses: goreleaser/goreleaser-action@v4\n        with:\n          version: latest\n          args: release --clean --release-notes ../release_notes.md\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n  release-success:\n    needs: release\n    if: ${{ success() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack upon release\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          RELEASE_URL: \"${{ github.server_url }}/${{ github.repository }}/releases/tag/${{ github.ref_name }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":rocket: New CometBFT release: <${{ env.RELEASE_URL }}|${{ github.ref_name }}>\"\n                  }\n                }\n              ]\n            }\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/release.yml",
    "retrieved_at": "2025-08-22T01:43:29.205070Z"
  },
  {
    "question": "Under what conditions are the SDK released to NPM and the `dev-frontend` image released to Docker Hub?",
    "answer": "name: Release SDK & UI\n\nenv:\n  CI: true\n  FORCE_COLOR: true\n\non:\n  push:\n    branches: [master, main]\n  pull_request:\n    branches: [master, main]\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v2\n\n      - uses: actions/setup-node@v2\n        with:\n          node-version: 12.x\n\n      - id: yarn-cache-dir-path\n        run: echo \"::set-output name=dir::$(yarn cache dir)\"\n\n      - uses: actions/cache@v1\n        with:\n          path: ${{ steps.yarn-cache-dir-path.outputs.dir }}\n          key: ${{ runner.os }}-yarn-${{ hashFiles('**/yarn.lock') }}\n          restore-keys: |\n            ${{ runner.os }}-yarn-\n\n      - run: yarn install --frozen-lockfile\n      - run: yarn build\n\n      - name: Test SDK & UI\n        run: yarn test\n\n      - name: Test SDK integration against live contracts\n        if: ${{ github.ref == 'refs/heads/master' }}\n        run: yarn test-live\n\n      - name: Release SDK on NPM\n        if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/master' }}\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: yarn release\n\n      - name: Login to Docker Hub\n        uses: azure/docker-login@v1\n        if: ${{ github.event_name == 'push' }}\n        with:\n          username: ${{ secrets.DOCKER_USERNAME }}\n          password: ${{ secrets.DOCKER_ACCESS_TOKEN }}\n\n      - name: Release dev-frontend on Docker Hub\n        if: ${{ github.event_name == 'push' }}\n        run: |\n          docker-compose build\n          docker-compose push\n        working-directory: ./packages/dev-frontend\n        env:\n          TAG: ${{ fromJSON('{ \"refs/heads/master\":\"latest\", \"refs/heads/main\":\"next\" }')[github.ref] }}\n",
    "source": "hcheng826/crangon",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/hcheng826/crangon/blob/2021a3fe84ac675d5a71e9238e1f1053237715a0/.github/workflows/release.yml",
    "retrieved_at": "2025-08-22T01:43:29.994671Z"
  },
  {
    "question": "What branch is checked out by the `actions/checkout@v3` action in the `e2e-long-test` job?",
    "answer": "# Weekly run of the E2E testnet using the long-running manifest on main\n\n# !! Relevant changes to this file should be propagated to the e2e-nightly-<V>x\n# files for the supported backport branches, when appropriate, modulo version\n# markers.\n\nname: e2e-long-main\non:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 3 * * 3'\n\njobs:\n  e2e-long-test:\n    runs-on: ubuntu-latest\n    timeout-minutes: 120\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker runner\n\n      - name: Run testnet\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/long.toml\n\n  e2e-long-fail:\n    needs: e2e-long-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ github.ref_name }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ github.ref_name }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Weekly long-run E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/e2e-long-37x.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/e2e-long-37x.yml",
    "retrieved_at": "2025-08-22T01:43:30.769673Z"
  },
  {
    "question": "Does this workflow lint protobuf files within the `proto` directory on both pull requests and pushes to the `v0.37.x` branch?",
    "answer": "name: Protobuf Lint\non:\n  pull_request:\n    paths:\n      - 'proto/**'\n  push:\n    branches:\n      - v0.37.x\n    paths:\n      - 'proto/**'\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    timeout-minutes: 5\n    steps:\n      - uses: actions/checkout@v3\n      - uses: bufbuild/buf-setup-action@v1.21.0\n      - uses: bufbuild/buf-lint-action@v1\n        with:\n          input: 'proto'\n",
    "source": "bnb-chain/greenfield-cometbft",
    "path": ".github/workflows/proto-lint.yml",
    "url": "https://github.com/bnb-chain/greenfield-cometbft/blob/1d6ca8f26cf781b5e2f0b09c6b1daf5b20c66751/.github/workflows/proto-lint.yml",
    "retrieved_at": "2025-08-22T01:43:31.571347Z"
  },
  {
    "question": "Does this workflow update and push the index on every push/pull request to the master branch?",
    "answer": "name: CI\n\non:\n  # Trigger the workflow on push or pull request,\n  # but only for the master branch\n  push:\n    branches:\n      - master\n  pull_request:\n    branches:\n      - master\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n      with:\n        ref: ${{ github.head_ref }}\n\n    - name: Run gen index\n      run: \"./index_gen.sh\"\n      shell: bash\n\n    - name: Add & Commit\n      uses: github-actions-x/commit@v2.4\n      with:\n        commit-message: 'Index updated'\n        name: '${{ secrets.GH_USER }}'\n        email: '${{ secrets.GH_EMAIL }}'\n        push-branch: ${{ github.head_ref }}\n        github-token: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Push\n      uses: ad-m/github-push-action@v0.5.0\n      with:\n        github_token: ${{ secrets.GH_TOKEN }}\n        branch: ${{ github.head_ref }}\n",
    "source": "kilitary/yara-malware-rules",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/kilitary/yara-malware-rules/blob/9bb7053697c09b3c24ad6a3e04a70c6000cdafb4/.github/workflows/main.yml",
    "retrieved_at": "2025-08-22T01:43:33.771040Z"
  },
  {
    "question": "What specific end-to-end tests are executed by this workflow?",
    "answer": "# Runs randomly generated E2E testnets nightly on the v0.37.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-37x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01', '02', '03', \"04\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"branch=`git branch --show-current`\" >> $GITHUB_OUTPUT\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner tests\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 5 -d networks/nightly/ -p\n\n      - name: Run ${{ matrix.p2p }} p2p testnets\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ needs.e2e-nightly-test.outputs.git-branch }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "zkMeLabs/moca-cometbft",
    "path": ".github/workflows/e2e-nightly-37x.yml",
    "url": "https://github.com/zkMeLabs/moca-cometbft/blob/04ec82d07cab97bbe98fe1feaf9fc5dbeaae36fb/.github/workflows/e2e-nightly-37x.yml",
    "retrieved_at": "2025-08-22T01:43:34.405995Z"
  },
  {
    "question": "What specific tests are executed by the `make tests` command in the \"Build\" step?",
    "answer": "# Runs randomly generated E2E testnets nightly on the v0.37.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-37x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01', '02', '03', \"04\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"branch=`git branch --show-current`\" >> $GITHUB_OUTPUT\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner tests\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 5 -d networks/nightly/ -p\n\n      - name: Run ${{ matrix.p2p }} p2p testnets\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ needs.e2e-nightly-test.outputs.git-branch }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "lightmos/lightmosbft",
    "path": ".github/workflows/e2e-nightly-37x.yml",
    "url": "https://github.com/lightmos/lightmosbft/blob/6f48b680f6d1bc32abf7f2ad91e84d3d53e7f96d/.github/workflows/e2e-nightly-37x.yml",
    "retrieved_at": "2025-08-22T01:43:35.214572Z"
  },
  {
    "question": "What specific tests are executed by the `e2e-nightly-test` job, and how are they defined?",
    "answer": "# Runs randomly generated E2E testnets nightly on the v0.37.x branch.\n\n# !! This file should be kept in sync with the e2e-nightly-main.yml file,\n# modulo changes to the version labels.\n\nname: e2e-nightly-37x\non:\n  schedule:\n    - cron: '0 2 * * *'\n\njobs:\n  e2e-nightly-test:\n    # Run parallel jobs for the listed testnet groups (must match the\n    # ./build/generator -g flag)\n    strategy:\n      fail-fast: false\n      matrix:\n        group: ['00', '01', '02', '03', \"04\"]\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - uses: actions/setup-go@v4\n        with:\n          go-version: '1.20'\n\n      - uses: actions/checkout@v3\n        with:\n          ref: 'v0.37.x'\n\n      - name: Capture git repo info\n        id: git-info\n        run: |\n          echo \"branch=`git branch --show-current`\" >> $GITHUB_OUTPUT\n\n      - name: Build\n        working-directory: test/e2e\n        # Run make jobs in parallel, since we can't run steps in parallel.\n        run: make -j2 docker generator runner tests\n\n      - name: Generate testnets\n        working-directory: test/e2e\n        # When changing -g, also change the matrix groups above\n        run: ./build/generator -g 5 -d networks/nightly/ -p\n\n      - name: Run ${{ matrix.p2p }} p2p testnets\n        working-directory: test/e2e\n        run: ./run-multiple.sh networks/nightly/*-group${{ matrix.group }}-*.toml\n\n    outputs:\n      git-branch: ${{ steps.git-info.outputs.branch }}\n\n  e2e-nightly-fail:\n    needs: e2e-nightly-test\n    if: ${{ failure() }}\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify Slack on failure\n        uses: slackapi/slack-github-action@v1.24.0\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK\n          BRANCH: ${{ needs.e2e-nightly-test.outputs.git-branch }}\n          RUN_URL: \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n          COMMITS_URL: \"${{ github.server_url }}/${{ github.repository }}/commits/${{ needs.e2e-nightly-test.outputs.git-branch }}\"\n        with:\n          payload: |\n            {\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \":skull: Nightly E2E tests for `${{ env.BRANCH }}` failed. See the <${{ env.RUN_URL }}|run details> and the <${{ env.COMMITS_URL }}|latest commits> possibly related to the failure.\"\n                  }\n                }\n              ]\n            }\n",
    "source": "SigmaGmbH/cometbft",
    "path": ".github/workflows/e2e-nightly-37x.yml",
    "url": "https://github.com/SigmaGmbH/cometbft/blob/b23ef56f8e6d8a7015a7f816a61f2e53b0b07b0d/.github/workflows/e2e-nightly-37x.yml",
    "retrieved_at": "2025-08-22T01:43:36.019417Z"
  },
  {
    "question": "What triggers this workflow, and what sequence of jobs will it execute upon triggering?",
    "answer": "name: Deploy Project\non: [push, workflow_dispatch]\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Get repo code\n        uses: actions/checkout@v3\n      - name: Install NodeJS\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: cd ./section-2-exercise && npm ci\n      - name: Run Lint\n        run: cd ./section-2-exercise && npm run lint\n  test:\n    needs: lint\n    runs-on: ubuntu-latest\n    steps:\n      - name: Get repo code\n        uses: actions/checkout@v3\n      - name: Install NodeJS\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: cd ./section-2-exercise && npm ci\n      - name: Run tests\n        run: cd ./section-2-exercise && npm run test\n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Get repo code\n        uses: actions/checkout@v3\n      - name: Install NodeJS\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: cd ./section-2-exercise && npm ci\n      - name: Build project\n        run: cd ./section-2-exercise && npm run build\n      - name: Deploy\n        run: \"echo 'Deploying...'\"\n\n\n\n",
    "source": "OrElharar/github-actions-workshop",
    "path": ".github/workflows/section-2-exercise-deployment.yml",
    "url": "https://github.com/OrElharar/github-actions-workshop/blob/bb9e6f94413bcafd9748806ea9c5caa2b861f11f/.github/workflows/section-2-exercise-deployment.yml",
    "retrieved_at": "2025-08-23T01:40:06.025472Z"
  },
  {
    "question": "What specific versioning scheme is implemented by the \"Bump version\" step in this workflow?",
    "answer": "on:\n  push:\n    branches:\n      - develop\n\nname: Publish to NPM\n\njobs:\n  publish:\n    name: Publish\n    runs-on: ubuntu-latest\n    environment: dev\n    steps:\n      - uses: actions/checkout@v3\n      - name: Use Node.js 18\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18.x'\n          always-auth: true\n      - name: Install Yarn\n        run: npm install -g yarn\n      - name: Bump version\n        id: version\n        run: |\n          CURRENT_VERSION=$(jq -r '.version' package.json)\n\n          BASE_VERSION=$(echo $CURRENT_VERSION | cut -d '-' -f 1)\n\n          NEW_BASE=$(echo $BASE_VERSION | awk -v OFS='.' -F. '{\n            $2 = $2 + 1;\n            $3 = 0;\n            print $0\n          }')\n\n          TIMESTAMP=$(date -u +\"%Y%m%d%H%M%S\")\n\n          COMMIT_SHA=$(git rev-parse --short HEAD)\n\n          NEW_VERSION=\"${NEW_BASE}-dev.${TIMESTAMP}.${COMMIT_SHA}\"\n          echo \"new_version=$NEW_VERSION\" >> $GITHUB_OUTPUT\n      - name: Update package.json version\n        run: |\n          jq \".version = \\\"${{ steps.version.outputs.new_version }}\\\"\" package.json > package.tmp\n          mv package.tmp package.json\n      - name: Install dependencies\n        run: yarn\n      - name: Run tests\n        run: yarn test\n      - name: Build\n        run: yarn build\n      - name: Setup .yarnrc.yml\n        run: |\n          yarn config set npmAuthToken $NPM_AUTH_TOKEN\n          yarn config set npmAlwaysAuth true\n        env:\n          NPM_AUTH_TOKEN: ${{ secrets.NPM_AUTH_TOKEN }}\n      - name: Publish\n        run: yarn npm publish --access public --tag dev\n",
    "source": "ton-org/sandbox",
    "path": ".github/workflows/publish-dev.yml",
    "url": "https://github.com/ton-org/sandbox/blob/24ad00977d3abb99ee027149acee1ab11a162bab/.github/workflows/publish-dev.yml",
    "retrieved_at": "2025-08-23T01:40:06.647322Z"
  },
  {
    "question": "What specific environment variables are set when running tests for the \"Latest klee-uclibc\" configuration in the Linux job?",
    "answer": "name: CI\n\non:\n  pull_request:\n    branches: master\n  push:\n    branches: master\n\n# Defaults for building KLEE\nenv:\n  BASE_IMAGE: ubuntu:bionic-20200807\n  REPOSITORY: klee\n  COVERAGE: 0\n  DISABLE_ASSERTIONS: 0\n  ENABLE_DOXYGEN: 0\n  ENABLE_OPTIMIZED: 1\n  ENABLE_DEBUG: 1\n  GTEST_VERSION: 1.7.0\n  KLEE_RUNTIME_BUILD: \"Debug+Asserts\"\n  LLVM_VERSION: 9\n  METASMT_VERSION: qf_abv\n  MINISAT_VERSION: \"master\"\n  REQUIRES_RTTI: 0\n  SANITIZER_BUILD:\n  SOLVERS: STP:Z3\n  STP_VERSION: 2.3.3\n  TCMALLOC_VERSION: 2.7\n  UCLIBC_VERSION: klee_uclibc_v1.2\n  USE_TCMALLOC: 1\n  USE_LIBCXX: 1\n  Z3_VERSION: 4.8.4\n\njobs:\n  Linux:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        name: [\n                \"LLVM 11\",\n                \"LLVM 10\",\n                \"LLVM 9\",\n                \"LLVM 8\",\n                \"LLVM 7\",\n                \"LLVM 6\",\n                \"LLVM 5\",\n                \"LLVM 4\",\n                \"LLVM 3.9\",\n                \"LLVM 3.8\",\n                \"ASan\",\n                \"UBSan\",\n                \"MSan\",\n                \"Z3 only\",\n                \"metaSMT STP\",\n                \"metaSMT Boolector\",\n                \"STP master\",\n                \"Latest klee-uclibc\",\n                \"Asserts enabled\",\n                \"No TCMalloc, optimised runtime\",\n            ]\n        include:\n          - name: \"LLVM 11\"\n            env:\n              LLVM_VERSION: 11\n          - name: \"LLVM 10\"\n            env:\n              LLVM_VERSION: 10\n          - name: \"LLVM 9\"\n            env:\n              LLVM_VERSION: 9\n          - name: \"LLVM 8\"\n            env:\n              LLVM_VERSION: 8\n          - name: \"LLVM 7\"\n            env:\n              LLVM_VERSION: 7\n          - name: \"LLVM 6\"\n            env:\n              LLVM_VERSION: 6\n          - name: \"LLVM 5\"\n            env:\n              LLVM_VERSION: 5\n          - name: \"LLVM 4\"\n            env:\n              LLVM_VERSION: 4\n          - name: \"LLVM 3.9\"\n            env:\n              LLVM_VERSION: 3.9\n          - name: \"LLVM 3.8\"\n            env:\n              LLVM_VERSION: 3.8\n              USE_LIBCXX: 0\n          # Sanitizer builds. Do unoptimized build otherwise the optimizer might remove problematic code\n          - name: \"ASan\"\n            env:\n              SANITIZER_BUILD: address\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n          - name: \"UBSan\"\n            env:\n              SANITIZER_BUILD: undefined\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n          - name: \"MSan\"\n            env:\n              SANITIZER_BUILD: memory\n              ENABLE_OPTIMIZED: 0\n              USE_TCMALLOC: 0\n              SOLVERS: STP\n            # Test just using Z3 only\n          - name: \"Z3 only\"\n            env:\n              SOLVERS: Z3\n          # Test just using metaSMT\n          - name: \"metaSMT STP\"\n            env:\n              SOLVERS: metaSMT\n              METASMT_DEFAULT: STP\n              REQUIRES_RTTI: 1\n          - name: \"metaSMT Boolector\"\n            env:\n              SOLVERS: metaSMT\n              METASMT_DEFAULT: BTOR\n              REQUIRES_RTTI: 1\n          # Test we can build against STP master\n          - name: \"STP master\"\n            env:\n              SOLVERS: STP\n              STP_VERSION: master\n          # Check we can build latest klee-uclibc branch\n          - name: \"Latest klee-uclibc\"\n            env:\n              UCLIBC_VERSION: klee_0_9_29\n          # Check at least one build with Asserts disabled.\n          - name: \"Asserts enabled\"\n            env:\n              SOLVERS: STP\n              DISABLE_ASSERTIONS: 1\n          # Check without TCMALLOC and with an optimised runtime library\n          - name: \"No TCMalloc, optimised runtime\"\n            env:\n              USE_TCMALLOC: 0\n              KLEE_RUNTIME_BUILD: \"Release+Debug+Asserts\"\n    steps:\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        env: ${{ matrix.env }}\n        run: scripts/build/build.sh klee --docker --create-final-image\n      - name: Run tests\n        run: scripts/build/run-tests.sh --run-docker --debug\n\n  macOS:\n    runs-on: macos-latest\n    env:\n      BASE: /tmp\n      SOLVERS: STP\n      UCLIBC_VERSION: 0\n      USE_TCMALLOC: 0\n      USE_LIBCXX: 0\n    steps:\n      - name: Install newer version of Bash\n        run: brew install bash\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        run: scripts/build/build.sh klee --debug --install-system-deps\n      - name: Run tests\n        run: scripts/build/run-tests.sh /tmp/klee_build* --debug\n\n  Docker:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout KLEE Code\n        uses: actions/checkout@v2\n      - name: Build Docker image\n        run: docker build .\n\n  Coverage:\n    runs-on: ubuntu-latest\n    env:\n      ENABLE_OPTIMIZED: 0\n      COVERAGE: 1\n    steps:\n      - name: Checkout KLEE source code\n        uses: actions/checkout@v2\n      - name: Build KLEE\n        run: scripts/build/build.sh klee --docker --create-final-image\n      - name: Run tests\n        run: scripts/build/run-tests.sh --coverage --upload-coverage --run-docker --debug\n",
    "source": "liuzikai/klc3",
    "path": ".github/workflows/build.yaml",
    "url": "https://github.com/liuzikai/klc3/blob/2d2773dfaf288bee6055c3b0693cb105afda2796/.github/workflows/build.yaml",
    "retrieved_at": "2025-08-23T01:40:08.892245Z"
  },
  {
    "question": "Under what conditions are the \"Build and push (for Push CI) in a daily basis\" steps executed, and for which jobs?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "Beomi/transformers-lmhead-logits",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/Beomi/transformers-lmhead-logits/blob/7a5ac2c84abfadd875eb1d2219bdeba621e28dbe/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-23T01:40:09.729238Z"
  },
  {
    "question": "Under what conditions are the Docker images rebuilt and pushed with the `-push-ci` tag?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "charon52hz/chz_transformers",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/charon52hz/chz_transformers/blob/779b26303dfb6e5218b1346f3a3c1ec87e8a82aa/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-23T01:40:10.538596Z"
  },
  {
    "question": "Under what conditions are the jobs triggered, and how does the `image_postfix` input affect the specific Docker images built and pushed?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "nelionel/T5_small_ADED",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/nelionel/T5_small_ADED/blob/fcf23af80ddd1023ce4be0dcb3615fff85c25108/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-23T01:40:11.451246Z"
  },
  {
    "question": "Under what conditions does the workflow build and push the `-push-ci` tagged Docker images?",
    "answer": "name: Build docker images (scheduled)\n\non:\n  push:\n    branches:\n      - build_ci_docker_image*\n  repository_dispatch:\n  workflow_call:\n    inputs:\n      image_postfix:\n        required: true\n        type: string\n  schedule:\n    - cron: \"17 0 * * *\"\n\nconcurrency:\n  group: docker-images-builds\n  cancel-in-progress: false\n\njobs:\n  latest-docker:\n    name: \"Latest PyTorch + TensorFlow [dev]\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-all-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-all-latest-gpu-push-ci\n\n  latest-torch-deepspeed-docker:\n    name: \"Latest PyTorch + DeepSpeed\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n\n  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      # Push CI images still need to be re-built daily\n      -\n        name: Build and push (for Push CI) in a daily basis\n        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n        if: inputs.image_postfix != '-push-ci'\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n\n  doc-builder:\n    name: \"Doc builder\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-doc-builder\n          push: true\n          tags: huggingface/transformers-doc-builder\n\n  latest-pytorch:\n    name: \"Latest PyTorch [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Cleanup disk\n        run: |\n          sudo ls -l /usr/local/lib/\n          sudo ls -l /usr/share/\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n          sudo rm -rf /usr/local/lib/android\n          sudo rm -rf /usr/share/dotnet\n          sudo du -sh /usr/local/lib/\n          sudo du -sh /usr/share/\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-pytorch-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-pytorch-gpu\n\n# Need to be fixed with the help from Guillaume.\n#  latest-pytorch-amd:\n#    name: \"Latest PyTorch (AMD) [dev]\"\n#    runs-on: [self-hosted, docker-gpu, amd-gpu, single-gpu, mi210]\n#    steps:\n#      - name: Set up Docker Buildx\n#        uses: docker/setup-buildx-action@v3\n#      - name: Check out code\n#        uses: actions/checkout@v3\n#      - name: Login to DockerHub\n#        uses: docker/login-action@v3\n#        with:\n#          username: ${{ secrets.DOCKERHUB_USERNAME }}\n#          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n#      - name: Build and push\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n#      # Push CI images still need to be re-built daily\n#      -\n#        name: Build and push (for Push CI) in a daily basis\n#        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n#        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n#        if: inputs.image_postfix != '-push-ci'\n#        uses: docker/build-push-action@v5\n#        with:\n#          context: ./docker/transformers-pytorch-amd-gpu\n#          build-args: |\n#            REF=main\n#          push: true\n#          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n\n  latest-tensorflow:\n    name: \"Latest TensorFlow [dev]\"\n    # Push CI doesn't need this image\n    if: inputs.image_postfix != '-push-ci'\n    runs-on: ubuntu-22.04\n    steps:\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Check out code\n        uses: actions/checkout@v3\n      -\n        name: Login to DockerHub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: ./docker/transformers-tensorflow-gpu\n          build-args: |\n            REF=main\n          push: true\n          tags: huggingface/transformers-tensorflow-gpu\n",
    "source": "tekiny/rag_ray_finetune",
    "path": ".github/workflows/build-docker-images.yml",
    "url": "https://github.com/tekiny/rag_ray_finetune/blob/a502b0d427e6ea217bb4d28b352297823385860a/.github/workflows/build-docker-images.yml",
    "retrieved_at": "2025-08-23T01:40:12.305513Z"
  },
  {
    "question": "How does the `metadata2gha` command generate the test matrices for unit and acceptance tests?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "scibian/puppet-module-arioch-keepalived",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/scibian/puppet-module-arioch-keepalived/blob/b74c85726bf9330f1243bd1f070d34c841aa953b/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-23T01:40:12.895486Z"
  },
  {
    "question": "How does `metadata2gha` determine the `puppet_unit_test_matrix`, `beaker_setfiles`, and `puppet_major_versions` outputs?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "SoftwareHeritage/puppet-puppet-archive",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/SoftwareHeritage/puppet-puppet-archive/blob/17315544ed355179d962caa6ff4ee83e8439b0ef/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-23T01:40:13.788312Z"
  },
  {
    "question": "How does the `metadata2gha` command in the `setup_matrix` job determine the test matrix configurations?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "jonsax/puppet-nginx",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/jonsax/puppet-nginx/blob/b704d632d50105256d46233b89f528a1a9bb9e7f/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-23T01:40:14.449146Z"
  },
  {
    "question": "How does the `metadata2gha` command generate the test matrix for unit and acceptance tests?",
    "answer": "name: CI\n\non: pull_request\n\njobs:\n  setup_matrix:\n    name: 'Setup Test Matrix'\n    runs-on: ubuntu-latest\n    outputs:\n      beaker_setfiles: ${{ steps.get-outputs.outputs.beaker_setfiles }}\n      puppet_major_versions: ${{ steps.get-outputs.outputs.puppet_major_versions }}\n      puppet_unit_test_matrix: ${{ steps.get-outputs.outputs.puppet_unit_test_matrix }}\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run rake validate\n        run: bundle exec rake validate\n      - name: Setup Test Matrix\n        id: get-outputs\n        run: bundle exec metadata2gha --use-fqdn --pidfile-workaround false\n\n  unit:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include: ${{fromJson(needs.setup_matrix.outputs.puppet_unit_test_matrix)}}\n    env:\n      BUNDLE_WITHOUT: development:system_tests:release\n      PUPPET_VERSION: \"~> ${{ matrix.puppet }}.0\"\n    name: Puppet ${{ matrix.puppet }} (Ruby ${{ matrix.ruby }})\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby }}\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake\n\n  acceptance:\n    needs: setup_matrix\n    runs-on: ubuntu-latest\n    env:\n      BUNDLE_WITHOUT: development:test:release\n    strategy:\n      fail-fast: false\n      matrix:\n        setfile: ${{fromJson(needs.setup_matrix.outputs.beaker_setfiles)}}\n        puppet: ${{fromJson(needs.setup_matrix.outputs.puppet_major_versions)}}\n    name: ${{ matrix.puppet.name }} - ${{ matrix.setfile.name }}\n    steps:\n      - name: Enable IPv6 on docker\n        run: |\n          echo '{\"ipv6\":true,\"fixed-cidr-v6\":\"2001:db8:1::/64\"}' | sudo tee /etc/docker/daemon.json\n          sudo service docker restart\n      - uses: actions/checkout@v2\n      - name: Setup ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: '2.7'\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake beaker\n        env:\n          BEAKER_PUPPET_COLLECTION: ${{ matrix.puppet.collection }}\n          BEAKER_setfile: ${{ matrix.setfile.value }}\n",
    "source": "Daemon-Solutions/puppet-rundeck",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/Daemon-Solutions/puppet-rundeck/blob/2feaa75b16f614bc14bfb10042321f9d232a2c7a/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-24T01:53:12.711273Z"
  },
  {
    "question": "What specific files or directories are excluded from triggering the workflow when changes are pushed or pulled?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "LokiXun/HumanPoseEstimation_TongjiSurvey",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/LokiXun/HumanPoseEstimation_TongjiSurvey/blob/838bef00caa26e8aba553fa7b2588ae69a0dc690/.github/workflows/build.yml",
    "retrieved_at": "2025-08-24T01:53:13.457299Z"
  },
  {
    "question": "Under what conditions will the workflow run, and which files trigger or prevent it?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "jiwei-dot/mmpose",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/jiwei-dot/mmpose/blob/94304d3038898a13198b7b5008352d22294a5b5e/.github/workflows/build.yml",
    "retrieved_at": "2025-08-24T01:53:14.270615Z"
  },
  {
    "question": "What specific conditions trigger the workflow to run, and which file changes are explicitly ignored?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "ReggieVW/mmpose",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/ReggieVW/mmpose/blob/566fac22b9ab2dc8231dd58a0b025ca08ebe2dee/.github/workflows/build.yml",
    "retrieved_at": "2025-08-24T01:53:15.149082Z"
  },
  {
    "question": "What specific files or directories are excluded from triggering the workflow on push and pull request events?",
    "answer": "# This workflow will install Python dependencies, run tests with a variety of Python versions\n# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n\nname: build\n\non:\n  push:\n    branches:\n      - master\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\n  pull_request:\n    paths-ignore:\n      - 'README.md'\n      - 'README_CN.md'\n      - 'docs/**'\n      - 'examples/**'\n      - '.dev_scripts/**'\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  build_cpu:\n    runs-on: ubuntu-18.04\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: pip install pip --upgrade\n      - name: Install onnx\n        run: pip install onnx\n      - name: Install PyTorch\n        run: pip install torch==${{matrix.torch}}+cpu torchvision==${{matrix.torchvision}}+cpu -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install MMCV\n        run: |\n          pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/${{matrix.torch_version}}/index.html\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Install other dependencies\n        run: |\n          pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n      - name: Build and install\n        run: rm -rf .eggs && pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n\n  build_cuda101:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.7]\n        torch: [1.5.0, 1.6.0, 1.7.0, 1.8.0]\n        include:\n          - torch: 1.5.0\n            torch_version: torch1.5\n            torchvision: 0.6.0\n          - torch: 1.6.0\n            torch_version: torch1.6\n            torchvision: 0.7.0\n          - torch: 1.7.0\n            torch_version: torch1.7\n            torchvision: 0.8.1\n          - torch: 1.8.0\n            torch_version: torch1.8\n            torchvision: 0.9.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get install -y libprotobuf-dev protobuf-compiler\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu101 torchvision==${{matrix.torchvision}}+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_cuda102:\n    runs-on: ubuntu-18.04\n    container:\n      image: pytorch/pytorch:1.9.0-cuda10.2-cudnn7-devel\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9]\n        torch: [1.9.0, 1.10.0]\n        include:\n          - torch: 1.9.0\n            torch_version: torch1.9\n            torchvision: 0.10.0\n          - torch: 1.10.0\n            torch_version: torch1.10\n            torchvision: 0.11.0\n\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Fetch GPG keys\n        run: |\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n          apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n      - name: Install system dependencies\n        run: |\n          apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libturbojpeg\n          apt-get clean\n          rm -rf /var/lib/apt/lists/*\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade\n      - name: Install dependencies for compiling onnx when python=3.9\n        run: python -m pip install protobuf && apt-get update && apt-get -y install libprotobuf-dev protobuf-compiler cmake\n        if: ${{matrix.python-version == '3.9'}}\n      - name: Install PyTorch\n        run: python -m pip install torch==${{matrix.torch}}+cu102 torchvision==${{matrix.torchvision}}+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/${{matrix.torch_version}}/index.html\n          python -m pip install -r requirements.txt\n          python -m pip install -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Build and install\n        run: |\n          rm -rf .eggs\n          python setup.py check -m -s\n          TORCH_CUDA_ARCH_LIST=7.0 python -m pip install -e .\n      - name: Run unittests and generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          files: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n\n  build_windows:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [windows-2022]\n        python-version: [3.8]\n        platform: [cpu]\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Upgrade pip\n        run: python -m pip install pip --upgrade --user\n      - name: Install PyTorch\n        # As a complement to Linux CI, we test on PyTorch LTS version\n        run: python -m pip install torch==1.8.2+${{ matrix.platform }} torchvision==0.9.2+${{ matrix.platform }} -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n      - name: Install MMCV\n        run: python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8/index.html --only-binary mmcv-full\n      - name: Install mmpose dependencies\n        run: |\n          python -V\n          python -m pip install xtcocotools\n          python -m pip install -r requirements/tests.txt -r requirements/optional.txt -r requirements/poseval.txt\n          python -m pip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n          python -c 'import mmcv; print(mmcv.__version__)'\n      - name: Show pip list\n        run: python -m pip list\n      - name: Build and install\n        run: python -m pip install -e .\n      - name: Run unittests\n        run: coverage run --branch --source mmpose -m pytest tests -sv\n      - name: Generate coverage report\n        run: |\n          coverage run --branch --omit=\"mmpose/apis/webcam/*\" --source mmpose -m pytest tests/\n          coverage xml\n          coverage report -m --omit=\"mmpose/apis/webcam/*\"\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v2\n        with:\n          file: ./coverage.xml\n          flags: unittests\n          env_vars: OS,PYTHON\n          name: codecov-umbrella\n          fail_ci_if_error: false\n",
    "source": "ritaank/pose-estimation-ui",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/ritaank/pose-estimation-ui/blob/d9faea69a3db7802d5813826c328ab87964fd20c/.github/workflows/build.yml",
    "retrieved_at": "2025-08-24T01:53:15.970846Z"
  },
  {
    "question": "Under what conditions will the `prepare` job be skipped?",
    "answer": "name: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  prepare:\n    runs-on: ubuntu-latest\n    if: \"! contains(github.event.head_commit.message, '[skip ci]')\"\n    steps:\n      - run: echo \"${{ github.event.head_commit.message }}\"\n  release:\n    needs: prepare\n    name: Release\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          persist-credentials: false\n      - name: Setup Node.js\n        uses: actions/setup-node@v1\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: npm ci\n      - name: Release\n        env:\n          GIT_AUTHOR_EMAIL: ${{ secrets.GIT_AUTHOR_EMAIL }}\n          GIT_AUTHOR_NAME: ${{ secrets.GIT_AUTHOR_NAME }}\n          GIT_COMMITTER_EMAIL: ${{ secrets.GIT_COMMITTER_EMAIL }}\n          GIT_COMMITTER_NAME: ${{ secrets.GIT_COMMITTER_NAME }}\n          GITHUB_TOKEN: ${{ secrets.PA_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n",
    "source": "americanexpress/lumberjack",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/americanexpress/lumberjack/blob/55a7e4915248382cbf9739cd79fcf904930c40cb/.github/workflows/release.yml",
    "retrieved_at": "2025-08-24T01:53:16.514595Z"
  },
  {
    "question": "Under what conditions will the `prepare` job be skipped based on the commit message?",
    "answer": "name: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  prepare:\n    runs-on: ubuntu-latest\n    if: \"! contains(github.event.head_commit.message, '[skip ci]')\"\n    steps:\n      - run: echo \"${{ github.event.head_commit.message }}\"\n  release:\n    needs: prepare\n    name: Release\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          persist-credentials: false\n      - name: Setup Node.js\n        uses: actions/setup-node@v1\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: npm ci\n      - name: Release\n        env:\n          GIT_AUTHOR_EMAIL: ${{ secrets.GIT_AUTHOR_EMAIL }}\n          GIT_AUTHOR_NAME: ${{ secrets.GIT_AUTHOR_NAME }}\n          GIT_COMMITTER_EMAIL: ${{ secrets.GIT_COMMITTER_EMAIL }}\n          GIT_COMMITTER_NAME: ${{ secrets.GIT_COMMITTER_NAME }}\n          GITHUB_TOKEN: ${{ secrets.PA_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n",
    "source": "americanexpress/amex-jest-preset-react",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/americanexpress/amex-jest-preset-react/blob/59b72225131d35209a3ea8184fd10dc47863d880/.github/workflows/release.yml",
    "retrieved_at": "2025-08-24T01:53:17.059263Z"
  },
  {
    "question": "Under what conditions will the `prepare` job be skipped?",
    "answer": "name: Release\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  prepare:\n    runs-on: ubuntu-latest\n    if: \"! contains(github.event.head_commit.message, '[skip ci]')\"\n    steps:\n      - run: echo \"${{ github.event.head_commit.message }}\"\n  release:\n    needs: prepare\n    name: Release\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          persist-credentials: false\n      - name: Setup Node.js\n        uses: actions/setup-node@v1\n        with:\n          node-version: 18\n      - name: Install dependencies\n        run: npm ci\n      - name: Release\n        env:\n          GIT_AUTHOR_EMAIL: ${{ secrets.GIT_AUTHOR_EMAIL }}\n          GIT_AUTHOR_NAME: ${{ secrets.GIT_AUTHOR_NAME }}\n          GIT_COMMITTER_EMAIL: ${{ secrets.GIT_COMMITTER_EMAIL }}\n          GIT_COMMITTER_NAME: ${{ secrets.GIT_COMMITTER_NAME }}\n          GITHUB_TOKEN: ${{ secrets.PA_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n",
    "source": "americanexpress/json-parse-context",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/americanexpress/json-parse-context/blob/ed84703711ad4ded75d00261a3f518c47d84daa6/.github/workflows/release.yml",
    "retrieved_at": "2025-08-24T01:53:17.678675Z"
  },
  {
    "question": "What is the purpose of building and deploying the application to CDN, FTP, and GitHub Pages upon the publishing of a release?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "HappyPot/democratic-management-",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/HappyPot/democratic-management-/blob/07aec203a3551c763908f8aecc819315150b5a6a/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-24T01:53:18.407889Z"
  },
  {
    "question": "What is the purpose of deploying the built application using three different methods (CDN, FTP, and GitHub Pages) when a release is published?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "singpay/d2dash",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/singpay/d2dash/blob/c2c6a8940e923421d24a0126b9dbd5c4993084fa/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-24T01:53:19.118407Z"
  },
  {
    "question": "What is the purpose of the different `VUE_APP_PUBLIC_PATH` values set in the `cdn`, `ftp`, and `gh-pages` jobs?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "swjgithub/d2-admin",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/swjgithub/d2-admin/blob/533616f64ea8dded70d6e92dbd96313f9efd2de1/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-25T01:48:06.999950Z"
  },
  {
    "question": "What is the purpose of deploying the built application to CDN, FTP, and GitHub Pages simultaneously upon a release?",
    "answer": "name: Deploy preview\n\non:\n  release:\n    types: [published]\n\njobs:\n\n  cdn:\n    name: CDN\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Setup qshell\n      uses: foxundermoon/setup-qshell@v1\n      env:\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      with:\n        qshell-version: '2.4.0'\n    - name: Test qshell\n      run: qshell version\n    - name: Login\n      run: qshell account ${{ secrets.AK }} ${{ secrets.SK }} GITHUB_ACTION\n    - name: CDN upload\n      run: |\n        qshell qupload2 \\\n        --src-dir=$GITHUB_WORKSPACE/dist \\\n        --bucket=d2-cdn \\\n        --key-prefix=${GITHUB_REPOSITORY//*\\//}/preview/ \\\n        --overwrite=true \\\n        --check-exists=true \\\n        --check-hash=true \\\n        --check-size=true \\\n        --rescan-local=true \\\n        --thread-count=32\n    - name: CDN refresh\n      run: |\n        echo \"https://cdn.d2.pub/${GITHUB_REPOSITORY//*\\//}/preview/\" > cdnrefresh.txt\n        qshell cdnrefresh --dirs -i ./cdnrefresh.txt\n  \n  ftp:\n    name: FTP\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/preview/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: SamKirkland/FTP-Deploy-Action@2.0.0\n      env:\n        FTP_SERVER: ${{ secrets.FTP_SERVER }}\n        FTP_USERNAME: ${{ secrets.FTP_USERNAME }}\n        FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}\n        METHOD: sftp\n        PORT: ${{ secrets.FTP_PORT }}\n        LOCAL_DIR: dist\n        REMOTE_DIR: /www/d2-admin/preview\n        ARGS: --delete --verbose --parallel=100\n  \n  gh-pages:\n    name: Github Pages\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - uses: bahmutov/npm-install@v1\n    - name: Set vue cli env\n      shell: bash\n      run: |\n        echo -e \"\\\n        VUE_APP_PUBLIC_PATH=/d2-admin/\\\n        \" > .env.preview.local\n        cat .env.preview.local | while read line\n        do\n          echo $line\n        done\n    - name: Build\n      run: yarn build:preview --report\n    - name: Deploy\n      uses: peaceiris/actions-gh-pages@v2\n      env:\n        PERSONAL_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n        PUBLISH_BRANCH: gh-pages\n        PUBLISH_DIR: ./dist\n      with:\n        forceOrphan: true",
    "source": "sftfjugg/d2-admin",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/sftfjugg/d2-admin/blob/eb0b002c6d9b431b5df5e9072c28693476287848/.github/workflows/deploy.yml",
    "retrieved_at": "2025-08-25T01:48:07.940286Z"
  },
  {
    "question": "What compliance checks are performed on the pull request by the `check_compliance` job?",
    "answer": "name: Compliance Checks\n\non: pull_request\n\njobs:\n  maintainer_check:\n    runs-on: ubuntu-latest\n    name: Check MAINTAINERS file\n    steps:\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n    - name: Run Maintainers Script\n      id: maintainer\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        python3 ./scripts/get_maintainer.py path CMakeLists.txt\n\n  check_compliance:\n    runs-on: ubuntu-latest\n    name: Run compliance checks on patch series (PR)\n    steps:\n    - name: Update PATH for west\n      run: |\n        echo \"$HOME/.local/bin\" >> $GITHUB_PATH\n\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n\n    - name: cache-pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-doc-pip\n\n    - name: Install python dependencies\n      run: |\n        pip3 install setuptools\n        pip3 install wheel\n        pip3 install python-magic junitparser==1.6.3 gitlint pylint pykwalify\n        pip3 install west\n\n    - name: west setup\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        git config --global user.email \"you@example.com\"\n        git config --global user.name \"Your Name\"\n        git remote -v\n        git rebase origin/${BASE_REF}\n        # debug\n        git log  --pretty=oneline | head -n 10\n        west init -l . || true\n        west update 2>&1 1> west.update.log || west update 2>&1 1> west.update2.log\n\n    - name: Run Compliance Tests\n      continue-on-error: true\n      id: compliance\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        export ZEPHYR_BASE=$PWD\n        # debug\n        ls -la\n        git log  --pretty=oneline | head -n 10\n        ./scripts/ci/check_compliance.py -m Devicetree -m Gitlint -m Identity -m Nits -m pylint -m checkpatch -m Kconfig -c origin/${BASE_REF}..\n\n    - name: upload-results\n      uses: actions/upload-artifact@master\n      continue-on-error: True\n      with:\n        name: compliance.xml\n        path: compliance.xml\n\n    - name: check-warns\n      run: |\n        if [[ ! -s \"compliance.xml\" ]]; then\n          exit 1;\n        fi\n\n        for file in Nits.txt checkpatch.txt Identity.txt Gitlint.txt pylint.txt Devicetree.txt Kconfig.txt; do\n          if [[ -s $file ]]; then\n            errors=$(cat $file)\n            errors=\"${errors//'%'/'%25'}\"\n            errors=\"${errors//$'\\n'/'%0A'}\"\n            errors=\"${errors//$'\\r'/'%0D'}\"\n            echo \"::error file=${file}::$errors\"\n            exit=1\n          fi\n        done\n\n        if [ \"${exit}\" == \"1\" ]; then\n          exit 1;\n        fi\n",
    "source": "jevinskie/js2232-zephyr",
    "path": ".github/workflows/compliance.yml",
    "url": "https://github.com/jevinskie/js2232-zephyr/blob/db40504c272734a519aa814a4f936455b527f5d1/.github/workflows/compliance.yml",
    "retrieved_at": "2025-08-25T01:48:08.679617Z"
  },
  {
    "question": "What compliance checks are performed on the pull request in the `check_compliance` job?",
    "answer": "name: Compliance Checks\n\non: pull_request\n\njobs:\n  maintainer_check:\n    runs-on: ubuntu-latest\n    name: Check MAINTAINERS file\n    steps:\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n    - name: Run Maintainers Script\n      id: maintainer\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        python3 ./scripts/get_maintainer.py path CMakeLists.txt\n\n  check_compliance:\n    runs-on: ubuntu-latest\n    name: Run compliance checks on patch series (PR)\n    steps:\n    - name: Update PATH for west\n      run: |\n        echo \"$HOME/.local/bin\" >> $GITHUB_PATH\n\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n\n    - name: cache-pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-doc-pip\n\n    - name: Install python dependencies\n      run: |\n        pip3 install setuptools\n        pip3 install wheel\n        pip3 install python-magic junitparser==1.6.3 gitlint pylint pykwalify\n        pip3 install west\n\n    - name: west setup\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        git config --global user.email \"you@example.com\"\n        git config --global user.name \"Your Name\"\n        git remote -v\n        git rebase origin/${BASE_REF}\n        # debug\n        git log  --pretty=oneline | head -n 10\n        west init -l . || true\n        west update 2>&1 1> west.update.log || west update 2>&1 1> west.update2.log\n\n    - name: Run Compliance Tests\n      continue-on-error: true\n      id: compliance\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        export ZEPHYR_BASE=$PWD\n        # debug\n        ls -la\n        git log  --pretty=oneline | head -n 10\n        ./scripts/ci/check_compliance.py -m Devicetree -m Gitlint -m Identity -m Nits -m pylint -m checkpatch -m Kconfig -c origin/${BASE_REF}..\n\n    - name: upload-results\n      uses: actions/upload-artifact@master\n      continue-on-error: True\n      with:\n        name: compliance.xml\n        path: compliance.xml\n\n    - name: check-warns\n      run: |\n        if [[ ! -s \"compliance.xml\" ]]; then\n          exit 1;\n        fi\n\n        for file in Nits.txt checkpatch.txt Identity.txt Gitlint.txt pylint.txt Devicetree.txt Kconfig.txt; do\n          if [[ -s $file ]]; then\n            errors=$(cat $file)\n            errors=\"${errors//'%'/'%25'}\"\n            errors=\"${errors//$'\\n'/'%0A'}\"\n            errors=\"${errors//$'\\r'/'%0D'}\"\n            echo \"::error file=${file}::$errors\"\n            exit=1\n          fi\n        done\n\n        if [ \"${exit}\" == \"1\" ]; then\n          exit 1;\n        fi\n",
    "source": "sstabellini/zephyr",
    "path": ".github/workflows/compliance.yml",
    "url": "https://github.com/sstabellini/zephyr/blob/7df99a12ab870b59e0e4af7b730d7b5d3fb43945/.github/workflows/compliance.yml",
    "retrieved_at": "2025-08-25T01:48:09.459096Z"
  },
  {
    "question": "How does the workflow handle compliance test failures and report errors within the pull request?",
    "answer": "name: Compliance Checks\n\non: pull_request\n\njobs:\n  maintainer_check:\n    runs-on: ubuntu-latest\n    name: Check MAINTAINERS file\n    steps:\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n    - name: Run Maintainers Script\n      id: maintainer\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        python3 ./scripts/get_maintainer.py path CMakeLists.txt\n\n  check_compliance:\n    runs-on: ubuntu-latest\n    name: Run compliance checks on patch series (PR)\n    steps:\n    - name: Update PATH for west\n      run: |\n        echo \"$HOME/.local/bin\" >> $GITHUB_PATH\n\n    - name: Checkout the code\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n        fetch-depth: 0\n\n    - name: cache-pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-doc-pip\n\n    - name: Install python dependencies\n      run: |\n        pip3 install setuptools\n        pip3 install wheel\n        pip3 install python-magic junitparser==1.6.3 gitlint pylint pykwalify\n        pip3 install west\n\n    - name: west setup\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        git config --global user.email \"you@example.com\"\n        git config --global user.name \"Your Name\"\n        git remote -v\n        git rebase origin/${BASE_REF}\n        # debug\n        git log  --pretty=oneline | head -n 10\n        west init -l . || true\n        west update 2>&1 1> west.update.log || west update 2>&1 1> west.update2.log\n\n    - name: Run Compliance Tests\n      continue-on-error: true\n      id: compliance\n      env:\n        BASE_REF: ${{ github.base_ref }}\n      run: |\n        export ZEPHYR_BASE=$PWD\n        # debug\n        ls -la\n        git log  --pretty=oneline | head -n 10\n        ./scripts/ci/check_compliance.py -m Devicetree -m Gitlint -m Identity -m Nits -m pylint -m checkpatch -m Kconfig -c origin/${BASE_REF}..\n\n    - name: upload-results\n      uses: actions/upload-artifact@master\n      continue-on-error: True\n      with:\n        name: compliance.xml\n        path: compliance.xml\n\n    - name: check-warns\n      run: |\n        if [[ ! -s \"compliance.xml\" ]]; then\n          exit 1;\n        fi\n\n        for file in Nits.txt checkpatch.txt Identity.txt Gitlint.txt pylint.txt Devicetree.txt Kconfig.txt; do\n          if [[ -s $file ]]; then\n            errors=$(cat $file)\n            errors=\"${errors//'%'/'%25'}\"\n            errors=\"${errors//$'\\n'/'%0A'}\"\n            errors=\"${errors//$'\\r'/'%0D'}\"\n            echo \"::error file=${file}::$errors\"\n            exit=1\n          fi\n        done\n\n        if [ \"${exit}\" == \"1\" ]; then\n          exit 1;\n        fi\n",
    "source": "lorenzomanica/sma-zephyr-threads",
    "path": ".github/workflows/compliance.yml",
    "url": "https://github.com/lorenzomanica/sma-zephyr-threads/blob/a76c81a00aa6936743f2d6f73be3e19e32856d62/.github/workflows/compliance.yml",
    "retrieved_at": "2025-08-25T01:48:10.400826Z"
  },
  {
    "question": "Under what conditions will this workflow execute given the defined branch exclusions?",
    "answer": "on:\n  push:\n    branches:\n      - '!trying'\n      - '!trying.tmp'\n      - '!staging'\n      - '!staging.tmp'\n\nname: Coverage\n\nenv:\n  RUST_BACKTRACE: 1\n  RUSTFLAGS: \"-Ccodegen-units=1 -Clink-dead-code -Coverflow-checks=off\"\n\njobs:\n  coverage:\n    name: Coverage\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          profile: minimal\n          override: true\n      - name: Install LLVM (Linux)\n        run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz -L -o llvm.tar.xz\n          mkdir -p /opt/llvm-10\n          tar xf llvm.tar.xz --strip-components=1 -C /opt/llvm-10\n          echo '/opt/llvm-10/bin' >> $GITHUB_PATH\n          echo 'LLVM_SYS_100_PREFIX=/opt/llvm-10' >> $GITHUB_ENV\n      - name: Generate Coverage Report\n        run: |\n          cargo install cargo-tarpaulin\n          cargo tarpaulin --forward --release -t120 --out Xml --ignore-tests --workspace --exclude wasmer-wasi-experimental-io-devices --exclude wasmer-c-api -- --skip traps:: --skip spec::linking --test-threads=1\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v1\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          file: ./cobertura.xml\n",
    "source": "jamesblacklock/wasmer-singlepass-v2",
    "path": ".github/workflows/coverage.yaml",
    "url": "https://github.com/jamesblacklock/wasmer-singlepass-v2/blob/b6e4ab8a6f9e4192425938cc9e6e070fc485a899/.github/workflows/coverage.yaml",
    "retrieved_at": "2025-08-25T01:48:11.114741Z"
  },
  {
    "question": "Under what conditions will this workflow run based on branch push events?",
    "answer": "on:\n  push:\n    branches:\n      - '!trying'\n      - '!trying.tmp'\n      - '!staging'\n      - '!staging.tmp'\n\nname: Coverage\n\nenv:\n  RUST_BACKTRACE: 1\n  RUSTFLAGS: \"-Ccodegen-units=1 -Clink-dead-code -Coverflow-checks=off\"\n\njobs:\n  coverage:\n    name: Coverage\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          profile: minimal\n          override: true\n      - name: Install LLVM (Linux)\n        run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz -L -o llvm.tar.xz\n          mkdir -p /opt/llvm-10\n          tar xf llvm.tar.xz --strip-components=1 -C /opt/llvm-10\n          echo '/opt/llvm-10/bin' >> $GITHUB_PATH\n          echo 'LLVM_SYS_100_PREFIX=/opt/llvm-10' >> $GITHUB_ENV\n      - name: Generate Coverage Report\n        run: |\n          cargo install cargo-tarpaulin\n          cargo tarpaulin --forward --release -t120 --out Xml --ignore-tests --workspace --exclude wasmer-wasi-experimental-io-devices --exclude wasmer-c-api -- --skip traps:: --skip spec::linking --test-threads=1\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v1\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          file: ./cobertura.xml\n",
    "source": "GeorgKreuzmayr/wasmer",
    "path": ".github/workflows/coverage.yaml",
    "url": "https://github.com/GeorgKreuzmayr/wasmer/blob/14d8084c29f6c47e76a16fdee2083aa997d34482/.github/workflows/coverage.yaml",
    "retrieved_at": "2025-08-25T01:48:11.800715Z"
  },
  {
    "question": "Under what conditions will this workflow automatically merge Dependabot pull requests?",
    "answer": "name: Dependabot Auto Merge\n\non:\n  pull_request_target:\n\njobs:\n  auto-merge:\n    timeout-minutes: 5\n    runs-on: ubuntu-latest\n    if: github.actor == 'dependabot[bot]'\n    steps:\n      - uses: actions/checkout@v2\n      - uses: ahmadnassri/action-dependabot-auto-merge@v2\n        with:\n          target: minor\n          github-token: ${{ secrets.DEP_AUTOMERGE }}\n",
    "source": "hash3liZer/khatta2",
    "path": ".github/workflows/auto-merge.yml",
    "url": "https://github.com/hash3liZer/khatta2/blob/e813a28b30b484d6fa75566837209a44c96d3f11/.github/workflows/auto-merge.yml",
    "retrieved_at": "2025-08-25T01:48:12.496227Z"
  },
  {
    "question": "What type of benchmarks are run on the Haystack framework by this workflow?",
    "answer": "name: Haystack 1.x Benchmarks\n\non:\n  workflow_dispatch:\n\npermissions:\n  id-token: write\n  contents: read\n\nenv:\n  AWS_REGION: eu-central-1\n\njobs:\n  deploy-runner:\n    runs-on: ubuntu-latest\n    outputs:\n      cml_runner_id: ${{ steps.deploy.outputs.cml_runner_id }}\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: iterative/setup-cml@v3\n\n      - name: AWS authentication\n        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}\n\n      - name: Launch EC2 instance and deploy runner\n        id: deploy\n        env:\n          repo_token: ${{ secrets.HAYSTACK_BOT_TOKEN }}\n        run: |\n          OUTPUT=$(cml runner launch \\\n          --cloud aws \\\n          --cloud-region ${{ env.AWS_REGION }} \\\n          --cloud-type=p3.2xlarge \\\n          --cloud-hdd-size=64 \\\n          --labels=cml 2>&1 | tee /dev/fd/2)\n          # Extract 'id' from the log and set it as an environment variable\n          ID_VALUE=$(echo \"$OUTPUT\" | jq -r '.message? | fromjson? | select(.id != null) | .id // empty')\n          echo \"cml_runner_id=$ID_VALUE\" >> \"$GITHUB_OUTPUT\"\n\n  run-reader-benchmarks:\n    needs: deploy-runner\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/reader/*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-reader\n          path: test/benchmarks/out/\n\n  run-elasticsearch-benchmarks:\n    needs:\n      - deploy-runner\n      - run-reader-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      elasticsearch:\n        image: elasticsearch:7.17.6\n        env:\n          discovery.type: \"single-node\"\n        ports:\n          - 9201:9200\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,elasticsearch,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-elasticsearch-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-elasticsearch\n          path: test/benchmarks/out/\n\n  run-weaviate-benchmarks:\n    needs:\n      - deploy-runner\n      - run-elasticsearch-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      weaviate:\n        image: semitechnologies/weaviate:1.17.2\n        env:\n          AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: \"true\"\n          PERSISTENCE_DATA_PATH: \"/var/lib/weaviate\"\n        ports:\n          - 8080:8080\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,weaviate,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-weaviate-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-weaviate\n          path: test/benchmarks/out/\n\n  run-opensearch-benchmarks:\n    needs:\n      - deploy-runner\n      - run-weaviate-benchmarks\n    runs-on: [self-hosted, cml]\n    container:\n      image: docker://iterativeai/cml:0-dvc2-base1-gpu\n      options: --gpus all\n    services:\n      opensearch:\n        image: opensearchproject/opensearch:1.3.5\n        env:\n          discovery.type: \"single-node\"\n          OPENSEARCH_JAVA_OPTS: \"-Xms4096m -Xmx4096m\"\n        ports:\n          - 9200:9200\n    timeout-minutes: 2880\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: v1.x\n\n      - name: Install Haystack + Datadog requirements\n        run: |\n          pip install .[metrics,opensearch,benchmarks,inference]\n          pip install -r test/benchmarks/datadog/requirements.txt\n\n      - name: Run benchmarks\n        working-directory: test/benchmarks\n        run: |\n          mkdir +p out\n          for f in ./configs/**/*-opensearch-*.yml; do\n            name=\"${f%.*}\"\n            echo \"=== Running benchmarks for $name ===\";\n            config_name=\"$(basename \"$name\")\"\n            python run.py --output \"out/$config_name.json\" \"$f\";\n            echo \"=== Benchmarks done for $name (or failed) ===\";\n          done\n\n      - name: Send Benchmark results to Datadog\n        working-directory: test/benchmarks\n        run: |\n          python datadog/send_metrics.py out/ ${{ secrets.CORE_DATADOG_API_KEY }} https://api.datadoghq.eu\n\n      - name: Archive benchmark results\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results-opensearch\n          path: test/benchmarks/out/\n\n  terminate-runner:\n    if: always()\n    needs:\n      - deploy-runner\n      - run-opensearch-benchmarks\n    runs-on: ubuntu-latest\n    steps:\n      - name: AWS authentication\n        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}\n\n      - name: Terminate EC2 instance\n        env:\n          CML_RUNNER_ID: ${{needs.deploy-runner.outputs.cml_runner_id}}\n        run: |\n          # Get the instance ID using its Name tag and terminate the instance\n          INSTANCE_ID=$(aws ec2 describe-instances --filters \"Name=tag:Name,Values=${{ env.CML_RUNNER_ID }}\" --query \"Reservations[*].Instances[*].[InstanceId]\" --output text)\n          aws ec2 terminate-instances --instance-ids \"$INSTANCE_ID\"\n",
    "source": "jkinda/haystack",
    "path": ".github/workflows/benchmarks.yml",
    "url": "https://github.com/jkinda/haystack/blob/25d333bed327cc40d536051ec3c4db26bbb1b147/.github/workflows/benchmarks.yml",
    "retrieved_at": "2025-08-25T01:48:13.279502Z"
  },
  {
    "question": "What triggers this workflow to run, and how frequently does it run by default?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "iGrubesic/expert-disco",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/iGrubesic/expert-disco/blob/d25f48756b2e76d29a08ac4e264c6c57bdac8e66/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-25T01:48:14.052351Z"
  },
  {
    "question": "Under what conditions (branches, events) does this workflow trigger?",
    "answer": "name: grpc Tests\n\n# START OF COMMON SECTION\non:\n  push:\n    branches: [ 'master', 'main', 'release/**' ]\n  pull_request:\n    branches: [ '*' ]\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n# END OF COMMON SECTION\n\njobs:\n  build_wolfssl:\n    name: Build wolfSSL\n    if: github.repository_owner == 'wolfssl'\n    # Just to keep it the same as the testing target\n    runs-on: ubuntu-22.04\n    # This should be a safe limit for the tests to run.\n    timeout-minutes: 10\n    steps:\n      - name: Build wolfSSL\n        uses: wolfSSL/actions-build-autotools-project@v1\n        with:\n          path: wolfssl\n          configure: --enable-all 'CPPFLAGS=-DWOLFSSL_RSA_KEY_CHECK -DHAVE_EX_DATA_CLEANUP_HOOKS'\n          install: true\n\n      - name: tar build-dir\n        run: tar -zcf build-dir.tgz build-dir\n\n      - name: Upload built lib\n        uses: actions/upload-artifact@v4\n        with:\n          name: wolf-install-grpc\n          path: build-dir.tgz\n          retention-days: 5\n\n  grpc_check:\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - ref: v1.60.0\n            tests: >-\n              bad_ssl_alpn_test bad_ssl_cert_test client_ssl_test\n              crl_ssl_transport_security_test server_ssl_test\n              ssl_transport_security_test ssl_transport_security_utils_test\n              test_core_security_ssl_credentials_test test_cpp_end2end_ssl_credentials_test\n              h2_ssl_cert_test h2_ssl_session_reuse_test\n    name: ${{ matrix.ref }}\n    if: github.repository_owner == 'wolfssl'\n    runs-on: ubuntu-22.04\n    # This should be a safe limit for the tests to run.\n    timeout-minutes: 30\n    needs: build_wolfssl\n    steps:\n      - name: Confirm IPv4 and IPv6 support\n        run: |\n          ip addr list lo | grep 'inet '\n          ip addr list lo | grep 'inet6 '\n\n      - name: Install prereqs\n        run:\n          sudo apt-get install build-essential autoconf libtool pkg-config cmake clang libc++-dev\n\n      - name: Download lib\n        uses: actions/download-artifact@v4\n        with:\n          name: wolf-install-grpc\n\n      - name: untar build-dir\n        run: tar -xf build-dir.tgz\n\n      - name: Checkout OSP\n        uses: actions/checkout@v4\n        with:\n          repository: wolfssl/osp\n          path: osp\n\n      - name: Checkout grpc\n        uses: actions/checkout@v4\n        with:\n          repository: grpc/grpc\n          path: grpc\n          ref: ${{ matrix.ref }}\n\n      - name: Build grpc\n        working-directory: ./grpc\n        run: |\n          patch -p1 < ../osp/grpc/grpc-${{ matrix.ref }}.patch\n          git submodule update --init\n          mkdir cmake/build\n          cd cmake/build\n          cmake -DgRPC_BUILD_TESTS=ON -DgRPC_SSL_PROVIDER=wolfssl \\\n            -DWOLFSSL_INSTALL_DIR=$GITHUB_WORKSPACE/build-dir ../..\n          make -j $(nproc) ${{ matrix.tests }}\n\n      - name: Run grpc tests\n        working-directory: ./grpc\n        run: |\n          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/build-dir/lib:$LD_LIBRARY_PATH\n          ./tools/run_tests/start_port_server.py\n          for t in ${{ matrix.tests }} ; do\n            ./cmake/build/$t\n          done\n",
    "source": "deepaksirone/wolfssl_bellerophon",
    "path": ".github/workflows/grpc.yml",
    "url": "https://github.com/deepaksirone/wolfssl_bellerophon/blob/50121495276c3c39dc5e525f07f6ebb5ae36cbc0/.github/workflows/grpc.yml",
    "retrieved_at": "2025-08-26T01:44:26.124139Z"
  },
  {
    "question": "What code quality and security checks are performed by this workflow on pull requests?",
    "answer": "name: Linters\n\non:\n  pull_request: { }\n\njobs:\n\n  linters:\n    name: linters\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.10'\n\n      - name: Install and Run Pre-commit\n        uses: pre-commit/action@v2.0.3\n\n      - name: Download Semgrep rules\n        run: git clone --depth 1 https://github.com/frappe/semgrep-rules.git frappe-semgrep-rules\n\n      - name: Download semgrep\n        run: pip install semgrep==0.97.0\n\n      - name: Run Semgrep rules\n        run: semgrep ci --config ./frappe-semgrep-rules/rules --config r/python.lang.correctness\n",
    "source": "CloudPlinthOne/cloudidp",
    "path": ".github/workflows/linters.yml",
    "url": "https://github.com/CloudPlinthOne/cloudidp/blob/dc4e40a2bf0dbb9fd3991528f6f96ad5676ced31/.github/workflows/linters.yml",
    "retrieved_at": "2025-08-26T01:44:26.935741Z"
  },
  {
    "question": "For every push or pull request to any branch, which Node.js versions are used to run the `npm test` command?",
    "answer": "name: 'CI'\n\non:\n  push:\n    branches: '**'\n  pull_request:\n    branches: '**'\n\npermissions:\n  contents: read\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        node-version: [12.x, 14.x, 16.x, 18.x]\n\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          persist-credentials: false\n      - name: Setup node\n        uses: actions/setup-node@v3\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: npm\n      - run: npm install\n      - run: npm test\n",
    "source": "X-oss-byte/Axios",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/X-oss-byte/Axios/blob/383089c74466b204992650a885b310e96872bfaf/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-26T01:44:27.749891Z"
  },
  {
    "question": "Under what conditions will the \"Post-processing\" job be executed?",
    "answer": "# Copyright (c) 2022, NVIDIA CORPORATION.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nname: Blossom-CI\non:\n  issue_comment:\n    types: [created]\n  workflow_dispatch:\n      inputs:\n          platform:\n            description: 'runs-on argument'\n            required: false\n          args:\n            description: 'argument'\n            required: false\n\npermissions:\n  actions: write\n  checks: write\n  contents: write\n  issues: write\n  pull-requests: write\n  repository-projects: write\n  statuses: write\n\njobs:\n  Authorization:\n    name: Authorization\n    runs-on: blossom\n    outputs:\n      args: ${{ env.args }}\n\n    # This job only runs for pull request comments\n    if: |\n      github.event.comment.body == '/build' &&\n      (\n        github.actor == 'wendell-hom' ||\n        github.actor == 'wyli' ||\n        github.actor == 'Nic-Ma' ||\n        github.actor == 'yiheng-wang-nv' ||\n        github.actor == 'KumoLiu'\n      )\n    steps:\n      - name: Check if comment is issued by authorized person\n        run: blossom-ci\n        env:\n          OPERATION: 'AUTH'\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          REPO_KEY_DATA: ${{ secrets.BLOSSOM_KEY }}\n\n  Vulnerability-scan:\n    name: Vulnerability scan\n    needs: [Authorization]\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n        with:\n          repository: ${{ fromJson(needs.Authorization.outputs.args).repo }}\n          ref: ${{ fromJson(needs.Authorization.outputs.args).ref }}\n          lfs: 'true'\n\n      # add blackduck properties https://synopsys.atlassian.net/wiki/spaces/INTDOCS/pages/631308372/Methods+for+Configuring+Analysis#Using-a-configuration-file\n      - name: Setup blackduck properties\n        run: |\n             echo detect.excluded.detector.types=PIP >> application.properties\n\n      - name: Run blossom action\n        uses: NVIDIA/blossom-action@main\n        env:\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          REPO_KEY_DATA: ${{ secrets.BLOSSOM_KEY }}\n        with:\n          args1: ${{ fromJson(needs.Authorization.outputs.args).args1 }}\n          args2: ${{ fromJson(needs.Authorization.outputs.args).args2 }}\n          args3: ${{ fromJson(needs.Authorization.outputs.args).args3 }}\n\n  Job-trigger:\n    name: Start ci job\n    needs: [Vulnerability-scan]\n    runs-on: blossom\n    steps:\n      - name: Start ci job\n        run: blossom-ci\n        env:\n          OPERATION: 'START-CI-JOB'\n          CI_SERVER: ${{ secrets.CI_SERVER }}\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n  Post-processing:\n    name: Post processing\n    runs-on: blossom\n    if : github.event_name == 'workflow_dispatch'\n    steps:\n      - name: Start post processing\n        run: blossom-ci\n        env:\n          OPERATION: 'POST-PROCESSING'\n          CI_SERVER: ${{ secrets.CI_SERVER }}\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "source": "Project-MONAI/model-zoo",
    "path": ".github/workflows/blossom-ci.yml",
    "url": "https://github.com/Project-MONAI/model-zoo/blob/e5c6da35788c54fcbadcef00884b87d2b10b14bc/.github/workflows/blossom-ci.yml",
    "retrieved_at": "2025-08-26T01:44:28.658131Z"
  },
  {
    "question": "What repository data is captured and stored by this workflow's `github-repo-stats` job?",
    "answer": "name: \"Repo Stats\"\n\non:\n  schedule:\n    # Run this once per day, towards the end of the day for keeping the most\n    # recent data point most meaningful (hours are interpreted in UTC).\n    - cron: \"0 23 * * *\"\n  workflow_dispatch:\n    # Allow for running this manually.\n\n    # Declare default permissions as read only.\npermissions: read-all\n\njobs:\n  snapshot:\n    name: github-repo-stats\n    runs-on: ubuntu-latest\n    steps:\n      - name: run-ghrs\n        # Use latest release.\n        uses: jgehrcke/github-repo-stats@306db38ad131cab2aa5f2cd3062bf6f8aa78c1aa # v1.4.2\n        with:\n          databranch: github-repo-stats\n          ghtoken: ${{ secrets.KEPTN_BOT_TOKEN }}\n",
    "source": "keptn/lifecycle-toolkit",
    "path": ".github/workflows/github-repo-stats.yml",
    "url": "https://github.com/keptn/lifecycle-toolkit/blob/40e195d78dd1886166e17d2e9c69b75a249fc384/.github/workflows/github-repo-stats.yml",
    "retrieved_at": "2025-08-26T01:44:29.562305Z"
  },
  {
    "question": "What i18n-related checks are performed by this workflow when triggered by a push or pull request to the main branch?",
    "answer": "name: Check i18n\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  RAILS_ENV: test\n\njobs:\n  check-i18n:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libicu-dev libidn11-dev libprotobuf-dev protobuf-compiler\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: '2.7'\n        bundler-cache: true\n    - name: Check locale file normalization\n      run: bundle exec i18n-tasks check-normalized\n    - name: Check for unused strings\n      run: bundle exec i18n-tasks unused -l en\n    - name: Check for wrong string interpolations\n      run: bundle exec i18n-tasks check-consistent-interpolations\n    - name: Check that all required locale files exist\n      run: bundle exec rake repo:check_locales_files\n",
    "source": "justjosias/truth-social",
    "path": ".github/workflows/check-i18n.yml",
    "url": "https://github.com/justjosias/truth-social/blob/70a5176b97247151340f83359142936dedb99e5b/.github/workflows/check-i18n.yml",
    "retrieved_at": "2025-08-26T01:44:31.879606Z"
  },
  {
    "question": "What i18n checks are performed by this workflow on push and pull requests to the main branch?",
    "answer": "name: Check i18n\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  RAILS_ENV: test\n\njobs:\n  check-i18n:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libicu-dev libidn11-dev libprotobuf-dev protobuf-compiler\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: '2.7'\n        bundler-cache: true\n    - name: Check locale file normalization\n      run: bundle exec i18n-tasks check-normalized\n    - name: Check for unused strings\n      run: bundle exec i18n-tasks unused -l en\n    - name: Check for wrong string interpolations\n      run: bundle exec i18n-tasks check-consistent-interpolations\n    - name: Check that all required locale files exist\n      run: bundle exec rake repo:check_locales_files\n",
    "source": "tugtug12121/e",
    "path": ".github/workflows/check-i18n.yml",
    "url": "https://github.com/tugtug12121/e/blob/70a5176b97247151340f83359142936dedb99e5b/.github/workflows/check-i18n.yml",
    "retrieved_at": "2025-08-26T01:44:32.670357Z"
  },
  {
    "question": "What specific i18n-related checks are performed by this workflow?",
    "answer": "name: Check i18n\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  RAILS_ENV: test\n\njobs:\n  check-i18n:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Install system dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libicu-dev libidn11-dev libprotobuf-dev protobuf-compiler\n    - name: Set up Ruby\n      uses: ruby/setup-ruby@v1\n      with:\n        ruby-version: '2.7'\n        bundler-cache: true\n    - name: Check locale file normalization\n      run: bundle exec i18n-tasks check-normalized\n    - name: Check for unused strings\n      run: bundle exec i18n-tasks unused -l en\n    - name: Check for wrong string interpolations\n      run: bundle exec i18n-tasks check-consistent-interpolations\n    - name: Check that all required locale files exist\n      run: bundle exec rake repo:check_locales_files\n",
    "source": "ftrbndd/truth",
    "path": ".github/workflows/check-i18n.yml",
    "url": "https://github.com/ftrbndd/truth/blob/6a6d9e5843e1da243f7b2bf901951b9f569931e3/.github/workflows/check-i18n.yml",
    "retrieved_at": "2025-08-26T01:44:33.440729Z"
  },
  {
    "question": "What triggers this workflow to run, and how frequently does it run on a schedule?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "wowdesarrollo/status-pages",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/wowdesarrollo/status-pages/blob/e2845769a7ed7ff7abcc43bcdfaaea23a6b79acf/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-26T01:44:34.446225Z"
  },
  {
    "question": "What triggers this workflow to run and check the status of the configured endpoints?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "bishrant/uptime-monitor",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/bishrant/uptime-monitor/blob/e86e3db03ccc7f5528957c7fc6ad3907765d075f/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-26T01:44:35.397866Z"
  },
  {
    "question": "What triggers the deployment of the 'get-quote' Google Cloud Function?",
    "answer": "name: Deploy to Google Cloud Functions\n\non:\n  push:\n    branches:\n      - main  # or your default branch\n  workflow_dispatch:  # allows manual triggers\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment: cicd\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '>=22.0.0'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build\n        run: npm run build\n\n      - id: 'auth'\n        uses: google-github-actions/auth@v2\n        with:\n          credentials_json: '${{ secrets.GCP_SA_KEY }}'\n          project_id: ${{ vars.GCP_PROJECT_ID }}\n\n      - name: Setup Google Cloud SDK\n        uses: google-github-actions/setup-gcloud@v2\n        with:\n          project_id: ${{ vars.GCP_PROJECT_ID }}\n\n      - name: Deploy to Cloud Run Functions\n        run: |\n          gcloud functions deploy get-quote \\\n            --entry-point=getQuote \\\n            --trigger-http \\\n            --runtime nodejs22 \\\n            --region=us-west2 \\\n            --allow-unauthenticated\n",
    "source": "astriaorg/swap-routing-api",
    "path": ".github/workflows/deploy.yaml",
    "url": "https://github.com/astriaorg/swap-routing-api/blob/02f091963fd571ec771ef5aba70cacd4377ae27f/.github/workflows/deploy.yaml",
    "retrieved_at": "2025-08-27T01:41:07.868224Z"
  },
  {
    "question": "What actions does this workflow perform on a scheduled basis or when issues are opened/reopened?",
    "answer": "name: CompatHelper\n\non:\n  schedule:\n    - cron: '00 * * * *'\n  issues:\n    types: [opened, reopened]\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        julia-version: [1.2.0]\n        julia-arch: [x86]\n        os: [ubuntu-latest]\n    steps:\n      - uses: julia-actions/setup-julia@latest\n        with:\n          version: ${{ matrix.julia-version }}\n      - name: Pkg.add(\"CompatHelper\")\n        run: julia -e 'using Pkg; Pkg.add(\"CompatHelper\")'\n      - name: CompatHelper.main()\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: julia -e 'using CompatHelper; CompatHelper.main()'\n",
    "source": "dillondaudert/UMAP.jl",
    "path": ".github/workflows/CompatHelper.yml",
    "url": "https://github.com/dillondaudert/UMAP.jl/blob/b738d5e0d8b1cffbb4d9f419c232aa79af5098d0/.github/workflows/CompatHelper.yml",
    "retrieved_at": "2025-08-27T01:41:08.527579Z"
  },
  {
    "question": "What specific coding standards or formatting rules are enforced by this workflow?",
    "answer": "extends: relaxed\nrules:\n  colons:\n    max-spaces-after: -1\n  line-length:\n    max: 120\n  empty-lines:\n    max-end: 2\n",
    "source": "stanford-rc/www.sherlock.stanford.edu",
    "path": ".github/workflows/config/yamllint.yml",
    "url": "https://github.com/stanford-rc/www.sherlock.stanford.edu/blob/639ef94a923bc1c213de316ab533ec7167e7f696/.github/workflows/config/yamllint.yml",
    "retrieved_at": "2025-08-27T01:41:09.424598Z"
  },
  {
    "question": "What triggers this workflow, and how often does it run automatically?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Uptime CI\non:\n  schedule:\n    - cron: \"*/5 * * * *\"\n  repository_dispatch:\n    types: [uptime]\n  workflow_dispatch:\njobs:\n  release:\n    name: Check status\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Check endpoint status\n        uses: upptime/uptime-monitor@v1.26.4\n        with:\n          command: \"update\"\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n          SECRETS_CONTEXT: ${{ toJson(secrets) }}\n",
    "source": "obigtech/status",
    "path": ".github/workflows/uptime.yml",
    "url": "https://github.com/obigtech/status/blob/ab1ffcb848304f9ae26349c8a16e9ca25960088d/.github/workflows/uptime.yml",
    "retrieved_at": "2025-08-27T01:41:11.589028Z"
  },
  {
    "question": "What triggers this workflow to run, and how often does the scheduled trigger activate it?",
    "answer": "name: Self-hosted runner (scheduled)\n\non:\n  repository_dispatch:\n  schedule:\n    - cron: \"0 2 * * *\"\n\nenv:\n  HF_HOME: /mnt/cache\n  TRANSFORMERS_IS_CI: yes\n  OMP_NUM_THREADS: 8\n  MKL_NUM_THREADS: 8\n  RUN_SLOW: yes\n  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n  TF_FORCE_GPU_ALLOW_GROWTH: true\n  RUN_PT_TF_CROSS_TESTS: 1\n\njobs:\n  setup:\n    name: Setup\n    strategy:\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: |\n          git fetch && git checkout ${{ github.sha }}\n\n      - name: Cleanup\n        working-directory: /transformers\n        run: |\n          rm -rf tests/__pycache__\n          rm -rf reports\n\n      - id: set-matrix\n        name: Identify models to test\n        working-directory: /transformers/tests\n        run: |\n          echo \"::set-output name=matrix::$(python3 -c 'import os; x = list(filter(os.path.isdir, os.listdir(os.getcwd()))); x.sort(); print(x)')\"\n\n      - name: NVIDIA-SMI\n        run: |\n          nvidia-smi\n\n      - name: GPU visibility\n        working-directory: /transformers\n        run: |\n          utils/print_env_pt.py\n          TF_CPP_MIN_LOG_LEVEL=3 python3 -c \"import tensorflow as tf; print('TF GPUs available:', bool(tf.config.list_physical_devices('GPU')))\"\n          TF_CPP_MIN_LOG_LEVEL=3 python3 -c \"import tensorflow as tf; print('Number of TF GPUs available:', len(tf.config.list_physical_devices('GPU')))\"\n\n  run_tests_gpu:\n    name: Model tests\n    strategy:\n      fail-fast: false\n      matrix:\n        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Echo folder ${{ matrix.folders }}\n        run: echo \"${{ matrix.folders }}\"\n\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all non-slow tests on GPU\n        working-directory: /transformers\n        run: python3 -m pytest -v --make-reports=${{ matrix.machines }}_tests_gpu_${{ matrix.folders }} tests/${{ matrix.folders }}\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/${{ matrix.machines }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_all_tests_gpu_${{ matrix.folders }}_test_reports\n          path: /transformers/reports/${{ matrix.machines }}_tests_gpu_${{ matrix.folders }}\n\n  run_examples_gpu:\n    name: Examples directory\n    runs-on: [self-hosted, single-gpu-docker]\n    container:\n      image: huggingface/transformers-all-latest-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run examples tests on GPU\n        working-directory: /transformers\n        run: |\n          pip install -r examples/pytorch/_tests_requirements.txt\n          python3 -m pytest -v --make-reports=examples_gpu examples/pytorch\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/examples_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: run_examples_gpu\n          path: /transformers/reports/examples_gpu\n\n  run_pipelines_torch_gpu:\n    name: PyTorch pipelines\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-pytorch-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all pipeline tests on GPU\n        working-directory: /transformers\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python3 -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=${{ matrix.machines }}_tests_torch_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /transformers/reports/${{ matrix.machines }}_tests_torch_pipeline_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_torch_pipeline_gpu\n          path: /transformers/reports/${{ matrix.machines }}_tests_torch_pipeline_gpu\n\n  run_pipelines_tf_gpu:\n    name: TensorFlow pipelines\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    container:\n      image: huggingface/transformers-tensorflow-gpu\n      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    needs: setup\n    steps:\n      - name: Update clone\n        working-directory: /transformers\n        run: |\n          git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all pipeline tests on GPU\n        working-directory: /transformers\n        env:\n          RUN_PIPELINE_TESTS: yes\n        run: |\n          python3 -m pytest -n 1 -v --dist=loadfile -m is_pipeline_test --make-reports=${{ matrix.machines }}_tests_tf_pipeline_gpu tests\n\n      - name: Failure short reports\n        if: ${{ always() }}\n        run: |\n          cat /transformers/reports/${{ matrix.machines }}_tests_tf_pipeline_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_tf_pipeline_gpu\n          path: /transformers/reports/${{ matrix.machines }}_tests_tf_pipeline_gpu\n\n  run_all_tests_torch_cuda_extensions_gpu:\n    name: Torch CUDA extension tests\n    strategy:\n      fail-fast: false\n      matrix:\n        machines: [multi-gpu-docker, single-gpu-docker]\n    runs-on: ${{ matrix.machines }}\n    needs: setup\n    container:\n      image: huggingface/transformers-pytorch-deepspeed-latest-gpu\n      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n    steps:\n      - name: Update clone\n        working-directory: /workspace/transformers\n        run: git fetch && git checkout ${{ github.sha }}\n\n      - name: Run all tests on GPU\n        working-directory: /workspace/transformers\n        run: |\n          python -m pytest -v --make-reports=${{ matrix.machines }}_tests_torch_cuda_extensions_gpu tests/deepspeed tests/extended\n\n      - name: Failure short reports\n        if: ${{ failure() }}\n        continue-on-error: true\n        run: cat /workspace/transformers/reports/${{ matrix.machines }}_tests_torch_cuda_extensions_gpu/failures_short.txt\n\n      - name: Test suite reports artifacts\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: ${{ matrix.machines }}_run_tests_torch_cuda_extensions_gpu_test_reports\n          path: /workspace/transformers/reports/${{ matrix.machines }}_tests_torch_cuda_extensions_gpu\n\n\n  send_results:\n    name: Send results to webhook\n    runs-on: ubuntu-latest\n    if: always()\n    needs: [setup, run_tests_gpu, run_examples_gpu, run_pipelines_tf_gpu, run_pipelines_torch_gpu, run_all_tests_torch_cuda_extensions_gpu]\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/download-artifact@v2\n      - name: Send message to Slack\n        env:\n          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n        run: |\n          pip install slack_sdk\n          python utils/notification_service.py \"${{ needs.setup.outputs.matrix }}\"\n",
    "source": "piercelamb/transformers_fork",
    "path": ".github/workflows/self-scheduled.yml",
    "url": "https://github.com/piercelamb/transformers_fork/blob/692f2ea4a07039628322a38e86a38c928c5f7e33/.github/workflows/self-scheduled.yml",
    "retrieved_at": "2025-08-27T01:41:12.474921Z"
  },
  {
    "question": "What triggers this workflow to run besides the scheduled cron job?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Updates CI\non:\n  schedule:\n    - cron: \"0 3 * * *\"\n  repository_dispatch:\n    types: [updates]\n  workflow_dispatch:\njobs:\n  release:\n    name: Deploy updates\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Update code\n        uses: upptime/updates@master\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n",
    "source": "iGrubesic/expert-disco",
    "path": ".github/workflows/updates.yml",
    "url": "https://github.com/iGrubesic/expert-disco/blob/d25f48756b2e76d29a08ac4e264c6c57bdac8e66/.github/workflows/updates.yml",
    "retrieved_at": "2025-08-27T01:41:13.117241Z"
  },
  {
    "question": "What triggers this workflow to run, and what action does it perform?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Updates CI\non:\n  schedule:\n    - cron: \"0 3 * * *\"\n  repository_dispatch:\n    types: [updates]\n  workflow_dispatch:\njobs:\n  release:\n    name: Deploy updates\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Update code\n        uses: upptime/updates@master\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n",
    "source": "wowdesarrollo/status-pages",
    "path": ".github/workflows/updates.yml",
    "url": "https://github.com/wowdesarrollo/status-pages/blob/e2845769a7ed7ff7abcc43bcdfaaea23a6b79acf/.github/workflows/updates.yml",
    "retrieved_at": "2025-08-27T01:41:13.837667Z"
  },
  {
    "question": "What triggers this workflow to run besides the scheduled cron job?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Updates CI\non:\n  schedule:\n    - cron: \"0 3 * * *\"\n  repository_dispatch:\n    types: [updates]\n  workflow_dispatch:\njobs:\n  release:\n    name: Deploy updates\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Update code\n        uses: upptime/updates@master\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n",
    "source": "bishrant/uptime-monitor",
    "path": ".github/workflows/updates.yml",
    "url": "https://github.com/bishrant/uptime-monitor/blob/e86e3db03ccc7f5528957c7fc6ad3907765d075f/.github/workflows/updates.yml",
    "retrieved_at": "2025-08-27T01:41:14.605484Z"
  },
  {
    "question": "What triggers this workflow to run and deploy updates?",
    "answer": "# This file was generated by upptime/uptime-monitor@v1.26.4\n#\n# ===============================\n# Do not edit this file directly!\n# ===============================\n#\n# Your changes will be overwritten when the template updates (daily)\n# Instead, change your .upptimerc.yml configuration: https://upptime.js.org/docs\n\nname: Updates CI\non:\n  schedule:\n    - cron: \"0 3 * * *\"\n  repository_dispatch:\n    types: [updates]\n  workflow_dispatch:\njobs:\n  release:\n    name: Deploy updates\n    runs-on: ubuntu-18.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.3\n        with:\n          ref: ${{ github.head_ref }}\n          token: ${{ secrets.GH_PAT }}\n      - name: Update code\n        uses: upptime/updates@master\n        env:\n          GH_PAT: ${{ secrets.GH_PAT }}\n",
    "source": "obigtech/status",
    "path": ".github/workflows/updates.yml",
    "url": "https://github.com/obigtech/status/blob/ab1ffcb848304f9ae26349c8a16e9ca25960088d/.github/workflows/updates.yml",
    "retrieved_at": "2025-08-27T01:41:15.280216Z"
  },
  {
    "question": "Under what conditions does this workflow trigger, and what steps does it execute when triggered?",
    "answer": "name: Analytics App Tests\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'apps/analytics/**'\n      - '.github/workflows/**'\n  pull_request:\n    branches:\n      - main\n    paths:\n      - 'apps/analytics/**'\n      - '.github/workflows/**'\n\njobs:\n  run-smoke-test:\n    name: Smoke test\n    runs-on: ubuntu-latest\n    env:\n      TURBO_TOKEN: ${{ secrets.TURBO_TOKEN }}\n      TURBO_TEAM: ${{ secrets.TURBO_TEAM }}\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Install Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: 18\n\n      - uses: pnpm/action-setup@v2\n        name: Install pnpm\n        id: pnpm-install\n        with:\n          version: 8\n          run_install: false\n\n      - name: Get pnpm store directory\n        id: pnpm-cache\n        shell: bash\n        run: |\n          echo \"STORE_PATH=$(pnpm store path)\" >> $GITHUB_OUTPUT\n\n      - uses: actions/cache@v3\n        name: Setup pnpm cache\n        with:\n          path: |\n            ~/.cache/Cypress\n            ${{ steps.pnpm-cache.outputs.STORE_PATH }}\n          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}\n          restore-keys: |\n            ${{ runner.os }}-pnpm-store-\n\n      - name: Install dependencies\n        run: pnpm install\n\n      - name: Setup environment variables\n        run: |\n          touch apps/analytics/.env\n          echo DATABASE_URL=${{ secrets.DATABASE_URL }} >> apps/analytics/.env\n          echo HASH_SALT=${{ secrets.HASH_SALT }} >> apps/analytics/.env\n          echo MAXMIND_LICENSE_KEY=${{ secrets.MAXMIND_LICENSE_KEY }} >> apps/analytics/.env\n\n      - name: Build Prisma client\n        run: cd apps/analytics && pnpm run build-postgresql-client\n\n      - name: Build app\n        run: pnpm run build:analytics\n",
    "source": "hash3liZer/khatta2",
    "path": ".github/workflows/analytics-app-testing.yml",
    "url": "https://github.com/hash3liZer/khatta2/blob/e813a28b30b484d6fa75566837209a44c96d3f11/.github/workflows/analytics-app-testing.yml",
    "retrieved_at": "2025-08-27T01:41:16.081269Z"
  },
  {
    "question": "Under what conditions will the `unix` job execute integration tests on the Go client?",
    "answer": "# This workflow is just for checking whether modifications works for the Go client.\n\nname: Go Client\n\non:\n  push:\n    branches:\n      - master\n      - 'rel/*'\n    paths-ignore:\n      - 'docs/**'\n  pull_request:\n    branches:\n      - master\n      - 'rel/*'\n    paths-ignore:\n      - 'docs/**'\n  # allow manually run the action:\n  workflow_dispatch:\n\nenv:\n  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3\n\njobs:\n  unix:\n    strategy:\n      fail-fast: false\n      max-parallel: 20\n      matrix:\n        java: [ 11 ]\n        os: [ ubuntu-latest ]\n    runs-on: ${{ matrix.os}}\n\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          token: ${{secrets.GITHUB_TOKEN}}\n          submodules: recursive\n      - name: Set up JDK ${{ matrix.java }}\n        uses: actions/setup-java@v1\n        with:\n          java-version: ${{ matrix.java }}\n      - name: Cache Maven packages\n        uses: actions/cache@v2\n        with:\n          path: ~/.m2\n          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}\n          restore-keys: ${{ runner.os }}-m2-\n      - name: Compile IoTDB Server\n        run: mvn -B package -Dmaven.test.skip=true  -am -pl server\n      - name: Integration test\n        shell: bash\n        run: |\n          cd client-go && make e2e_test_for_parent_git_repo e2e_test_clean_for_parent_git_repo",
    "source": "rohankumardubey/Iotdb",
    "path": ".github/workflows/client-go.yml",
    "url": "https://github.com/rohankumardubey/Iotdb/blob/860c8de52ced88b4440f10bc0b07cd8a9c7441bd/.github/workflows/client-go.yml",
    "retrieved_at": "2025-08-28T01:40:54.709403Z"
  },
  {
    "question": "Does this workflow run Clippy on all targets and features for both push and pull requests against the `main` branch?",
    "answer": "name: Clippy\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - dev\n\njobs:\n  clippy:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y webkit2gtk-4.0\n\n      - name: Install clippy with stable toolchain\n        uses: dtolnay/rust-toolchain@stable\n        with:\n          components: clippy\n\n      - uses: Swatinem/rust-cache@v2\n\n      - run: cargo clippy --manifest-path=Cargo.toml --all-targets --all-features -- -D warnings\n",
    "source": "deid84/tauri-plugin-serialport",
    "path": ".github/workflows/clippy.yml",
    "url": "https://github.com/deid84/tauri-plugin-serialport/blob/f92e8895bf34b057de71a7a35417da408d2f9ee4/.github/workflows/clippy.yml",
    "retrieved_at": "2025-08-28T01:40:57.121790Z"
  },
  {
    "question": "Does this workflow run clippy on every push to main and every pull request targeting main or dev?",
    "answer": "name: Clippy\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - dev\n\njobs:\n  clippy:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y webkit2gtk-4.0\n\n      - name: Install clippy with stable toolchain\n        uses: dtolnay/rust-toolchain@stable\n        with:\n          components: clippy\n\n      - uses: Swatinem/rust-cache@v2\n\n      - run: cargo clippy --manifest-path=Cargo.toml --all-targets --all-features -- -D warnings\n",
    "source": "kessdev/tauri-plugin-rusqlite",
    "path": ".github/workflows/clippy.yml",
    "url": "https://github.com/kessdev/tauri-plugin-rusqlite/blob/ac28949bde650524adc9f5936321af4d54f557da/.github/workflows/clippy.yml",
    "retrieved_at": "2025-08-28T01:40:58.129976Z"
  },
  {
    "question": "Does this workflow run clippy on every push to `main` and pull request targeting `main` or `dev` branches?",
    "answer": "name: Clippy\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - dev\n\njobs:\n  clippy:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y webkit2gtk-4.0\n\n      - name: Install clippy with stable toolchain\n        uses: dtolnay/rust-toolchain@stable\n        with:\n          components: clippy\n\n      - uses: Swatinem/rust-cache@v2\n\n      - run: cargo clippy --manifest-path=Cargo.toml --all-targets --all-features -- -D warnings\n",
    "source": "Altaks/Tauri-Remote-Caching-Plugin",
    "path": ".github/workflows/clippy.yml",
    "url": "https://github.com/Altaks/Tauri-Remote-Caching-Plugin/blob/2f60c22bf08a2160392a88ac22c1e9e2c23b2a33/.github/workflows/clippy.yml",
    "retrieved_at": "2025-08-28T01:40:59.230928Z"
  },
  {
    "question": "Does this workflow run Clippy on every push to main and pull request targeting main or dev?",
    "answer": "name: Clippy\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - dev\n\njobs:\n  clippy:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y webkit2gtk-4.0\n\n      - name: Install clippy with stable toolchain\n        uses: dtolnay/rust-toolchain@stable\n        with:\n          components: clippy\n\n      - uses: Swatinem/rust-cache@v2\n\n      - run: cargo clippy --manifest-path=Cargo.toml --all-targets --all-features -- -D warnings\n",
    "source": "ryantaylor/tauri-plugin-cohdb",
    "path": ".github/workflows/clippy.yml",
    "url": "https://github.com/ryantaylor/tauri-plugin-cohdb/blob/f8fc522bfea56a7c90f001ef52f791675d7bb7d2/.github/workflows/clippy.yml",
    "retrieved_at": "2025-08-28T01:41:00.414184Z"
  },
  {
    "question": "What is the purpose of the \"Build and Deploy Collection\" step using the `galaxy-role-import-action`?",
    "answer": "---\nname: release\n\non:\n  push:\n    tags: [\"*\"]\n\npermissions:\n  id-token: write\n  contents: write\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out code\n        uses: actions/checkout@v2\n\n      - name: version\n        run: echo \"version=$(/usr/bin/basename ${{ github.ref }})\" >> $GITHUB_OUTPUT\n        id: version\n\n      - name: release\n        uses: actions/create-release@v1\n        id: create_release\n        with:\n          draft: false\n          prerelease: false\n          release_name: ${{ steps.version.outputs.version }}\n          tag_name: ${{ github.ref }}\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n\n      - name: Package\n        uses: a7ul/tar-action@v1.1.2\n        with:\n          files: .\n          command: c\n          outPath: ${{ steps.version.outputs.version }}.tar.gz\n\n      - name: Sign release with Sigstore\n        continue-on-error: true\n        uses: sigstore/gh-action-sigstore-python@v0.0.9\n        with:\n          inputs: ${{ steps.version.outputs.version }}.tar.gz\n          release-signing-artifacts: true\n          upload-signing-artifacts: true\n\n      - name: upload signed asset\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz\n          asset_content_type: application/gzip\n\n      - name: upload sigstore certificate\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz.crt\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz.crt\n          asset_content_type: application/x-x509-ca-cert\n\n      - name: upload sigstore signature\n        continue-on-error: true\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ${{ steps.version.outputs.version }}.tar.gz.sig\n          asset_name: ${{ steps.version.outputs.version }}.tar.gz.sig\n          asset_content_type: application/octet-stream\n\n      - name: Build and Deploy Collection\n        uses: 0x022b/galaxy-role-import-action@1.0.0\n        with:\n          galaxy_api_key: \"${{ secrets.ANSIBLE_GALAXY_TOKEN }}\"\n",
    "source": "chrisvanmeer/ansible-role-nftables",
    "path": ".github/workflows/release.yaml",
    "url": "https://github.com/chrisvanmeer/ansible-role-nftables/blob/c4da513d08a24df4ae042b85a10d9d3f7a27ca8a/.github/workflows/release.yaml",
    "retrieved_at": "2025-08-28T01:41:01.359774Z"
  },
  {
    "question": "Under what conditions, besides manual dispatch, will this workflow execute?",
    "answer": "name: 'build and deploy Speckle functions'\non:\n  workflow_dispatch:\n  push:\n    tags:\n      - '*'\n\njobs:\n  publish-automate-function-version: # make sure the action works on a clean machine without building\n    env:\n      FUNCTION_SCHEMA_FILE_NAME: functionSchema.json\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.1.1\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install and configure Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: 1.3.2\n          virtualenvs-create: false\n          virtualenvs-in-project: false\n          installer-parallel: true\n      - name: Restore dependencies\n        run: poetry install --no-root\n      - name: Extract functionInputSchema\n        id: extract_schema\n        run: |\n          python main.py generate_schema ${HOME}/${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n      - name: Speckle Automate Function - Build and Publish\n        uses: specklesystems/speckle-automate-github-composite-action@0.8.0\n        with:\n          speckle_automate_url: ${{ env.SPECKLE_AUTOMATE_URL || 'https://automate.speckle.dev' }} \n          speckle_token: ${{ secrets.SPECKLE_FUNCTION_TOKEN }}\n          speckle_function_id: ${{ secrets.SPECKLE_FUNCTION_ID }}\n          speckle_function_input_schema_file_path: ${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n          speckle_function_command: 'python -u main.py run'\n",
    "source": "specklesystems/Demo-function",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/specklesystems/Demo-function/blob/ee524d2b2408f7d789799484b84638b983bc0342/.github/workflows/main.yml",
    "retrieved_at": "2025-08-28T01:41:02.129487Z"
  },
  {
    "question": "What triggers this workflow, and what specific tags will initiate a run?",
    "answer": "name: 'build and deploy Speckle functions'\non:\n  workflow_dispatch:\n  push:\n    tags:\n      - '*'\n\njobs:\n  publish-automate-function-version: # make sure the action works on a clean machine without building\n    env:\n      FUNCTION_SCHEMA_FILE_NAME: functionSchema.json\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.1.1\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install and configure Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: 1.3.2\n          virtualenvs-create: false\n          virtualenvs-in-project: false\n          installer-parallel: true\n      - name: Restore dependencies\n        run: poetry install --no-root\n      - name: Extract functionInputSchema\n        id: extract_schema\n        run: |\n          python main.py generate_schema ${HOME}/${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n      - name: Speckle Automate Function - Build and Publish\n        uses: specklesystems/speckle-automate-github-composite-action@0.8.0\n        with:\n          speckle_automate_url: ${{ env.SPECKLE_AUTOMATE_URL || 'https://automate.speckle.dev' }} \n          speckle_token: ${{ secrets.SPECKLE_FUNCTION_TOKEN }}\n          speckle_function_id: ${{ secrets.SPECKLE_FUNCTION_ID }}\n          speckle_function_input_schema_file_path: ${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n          speckle_function_command: 'python -u main.py run'\n",
    "source": "specklesystems/SHL-PW-Demo",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/specklesystems/SHL-PW-Demo/blob/abffa658bab320f3fcfe18729a319339bcc61037/.github/workflows/main.yml",
    "retrieved_at": "2025-08-28T01:41:02.998519Z"
  },
  {
    "question": "What triggers this workflow to build and deploy Speckle functions?",
    "answer": "name: 'build and deploy Speckle functions'\non:\n  workflow_dispatch:\n  push:\n    tags:\n      - '*'\n\njobs:\n  publish-automate-function-version: # make sure the action works on a clean machine without building\n    env:\n      FUNCTION_SCHEMA_FILE_NAME: functionSchema.json\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.1.1\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install and configure Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: 1.3.2\n          virtualenvs-create: false\n          virtualenvs-in-project: false\n          installer-parallel: true\n      - name: Restore dependencies\n        run: poetry install --no-root\n      - name: Extract functionInputSchema\n        id: extract_schema\n        run: |\n          python main.py generate_schema ${HOME}/${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n      - name: Speckle Automate Function - Build and Publish\n        uses: specklesystems/speckle-automate-github-composite-action@0.8.0\n        with:\n          speckle_automate_url: ${{ env.SPECKLE_AUTOMATE_URL || 'https://automate.speckle.dev' }} \n          speckle_token: ${{ secrets.SPECKLE_FUNCTION_TOKEN }}\n          speckle_function_id: ${{ secrets.SPECKLE_FUNCTION_ID }}\n          speckle_function_input_schema_file_path: ${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n          speckle_function_command: 'python -u main.py run'\n",
    "source": "jsdbroughton/LINK-me-up",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/jsdbroughton/LINK-me-up/blob/1e2f41b7acf586d9472648f31613333fa5d01df2/.github/workflows/main.yml",
    "retrieved_at": "2025-08-28T01:41:03.752018Z"
  },
  {
    "question": "What triggers this workflow to run, besides manual dispatch?",
    "answer": "name: 'build and deploy Speckle functions'\non:\n  workflow_dispatch:\n  push:\n    tags:\n      - '*'\n\njobs:\n  publish-automate-function-version: # make sure the action works on a clean machine without building\n    env:\n      FUNCTION_SCHEMA_FILE_NAME: functionSchema.json\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.1.1\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install and configure Poetry\n        uses: snok/install-poetry@v1\n        with:\n          version: 1.3.2\n          virtualenvs-create: false\n          virtualenvs-in-project: false\n          installer-parallel: true\n      - name: Restore dependencies\n        run: poetry install --no-root\n      - name: Extract functionInputSchema\n        id: extract_schema\n        run: |\n          python main.py generate_schema ${HOME}/${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n      - name: Speckle Automate Function - Build and Publish\n        uses: specklesystems/speckle-automate-github-composite-action@0.8.0\n        with:\n          speckle_automate_url: ${{ env.SPECKLE_AUTOMATE_URL || 'https://automate.speckle.dev' }} \n          speckle_token: ${{ secrets.SPECKLE_FUNCTION_TOKEN }}\n          speckle_function_id: ${{ secrets.SPECKLE_FUNCTION_ID }}\n          speckle_function_input_schema_file_path: ${{ env.FUNCTION_SCHEMA_FILE_NAME }}\n          speckle_function_command: 'python -u main.py run'\n",
    "source": "gjedlicska/automate_e2e_test",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/gjedlicska/automate_e2e_test/blob/c1b9a7fd5f798f1f6354f887cbbacf3498a620cd/.github/workflows/main.yml",
    "retrieved_at": "2025-08-28T01:41:04.492712Z"
  },
  {
    "question": "What specific tests are executed and uploaded as artifacts by this workflow?",
    "answer": "name: Build (Sony Playstation 2)\n\non: [push, pull_request]\n\njobs:\n  ps2:\n    runs-on: ubuntu-latest\n    container: ps2dev/ps2dev:latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Setup dependencies\n      run: |\n        apk update \n        apk add cmake gmp mpc1 mpfr4 ninja pkgconf make git\n\n    # To be removed once ps2_drivers is part of PS2DEV\n    - name: Install ps2_drivers lib\n      run: |\n        git clone https://github.com/fjtrujy/ps2_drivers.git\n        cd ps2_drivers\n        make -j $(getconf _NPROCESSORS_ONLN) clean\n        make -j $(getconf _NPROCESSORS_ONLN)\n        make -j $(getconf _NPROCESSORS_ONLN) install\n\n    - name: Configure (CMake)\n      run: |\n        cmake -S . -B build -G Ninja \\\n          -DCMAKE_TOOLCHAIN_FILE=$PS2DEV/ps2sdk/ps2dev.cmake \\\n          -DSDL_WERROR=ON \\\n          -DSDL_TESTS=ON \\\n          -DCMAKE_INSTALL_PREFIX=cmake_prefix \\\n          -DCMAKE_BUILD_TYPE=Release\n    - name: Build\n      run: cmake --build build --config Release --verbose --parallel\n    - name: Install (CMake)\n      run: |\n        set -eu\n        cmake --install build/ --config Release\n        echo \"SDL2_DIR=$(pwd)/cmake_prefix\" >> $GITHUB_ENV\n        ( cd cmake_prefix; find ) | LC_ALL=C sort -u\n\n    - name: Verify CMake configuration files\n      run: |\n        cmake -S cmake/test -B cmake_config_build -G Ninja \\\n          -DCMAKE_TOOLCHAIN_FILE=$PS2DEV/ps2sdk/ps2dev.cmake \\\n          -DTEST_SHARED=FALSE \\\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }} \\\n          -DCMAKE_BUILD_TYPE=Release\n        cmake --build cmake_config_build --verbose\n    - name: Verify sdl2-config\n      run: |\n        export CC=mips64r5900el-ps2-elf-gcc\n        export PATH=${{ env.SDL2_DIR }}/bin:$PATH\n        export EXTRA_LDFLAGS=\"-L$PS2DEV/ps2sdk/ee/lib -L$PS2DEV/gsKit/lib -L$PS2DEV/ps2sdk/ports/lib\"\n        cmake/test/test_sdlconfig.sh\n    - name: Verify sdl2.pc\n      run: |\n        export CC=mips64r5900el-ps2-elf-gcc\n        export EXTRA_LDFLAGS=\"-L$PS2DEV/ps2sdk/ee/lib -L$PS2DEV/gsKit/lib -L$PS2DEV/ps2sdk/ports/lib\"\n        export PKG_CONFIG_PATH=${{ env.SDL2_DIR }}/lib/pkgconfig\n        cmake/test/test_pkgconfig.sh\n    \n    - name: Get short SHA\n      id: slug\n      run: echo \"::set-output name=sha8::$(echo ${GITHUB_SHA} | cut -c1-8)\"\n\n    - name: Upload artifacts\n      if: ${{ success() }}\n      uses: actions/upload-artifact@v3\n      with:\n        name: tests-${{ steps.slug.outputs.sha8 }}\n        path: |\n          build/test\n",
    "source": "JohnnyonFlame/SDL-dumbbuffers",
    "path": ".github/workflows/ps2.yaml",
    "url": "https://github.com/JohnnyonFlame/SDL-dumbbuffers/blob/64547c0842431003f58c571595d7486e0f0c9440/.github/workflows/ps2.yaml",
    "retrieved_at": "2025-08-29T01:40:32.685866Z"
  },
  {
    "question": "What tests are built and run by this workflow to verify the SDL2 library for the PS2?",
    "answer": "name: Build (Sony Playstation 2)\n\non: [push, pull_request]\n\njobs:\n  ps2:\n    runs-on: ubuntu-latest\n    container: ps2dev/ps2dev:latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Setup dependencies\n      run: |\n        apk update \n        apk add cmake gmp mpc1 mpfr4 ninja pkgconf make git\n\n    # To be removed once ps2_drivers is part of PS2DEV\n    - name: Install ps2_drivers lib\n      run: |\n        git clone https://github.com/fjtrujy/ps2_drivers.git\n        cd ps2_drivers\n        make -j $(getconf _NPROCESSORS_ONLN) clean\n        make -j $(getconf _NPROCESSORS_ONLN)\n        make -j $(getconf _NPROCESSORS_ONLN) install\n\n    - name: Configure (CMake)\n      run: |\n        cmake -S . -B build -G Ninja \\\n          -DCMAKE_TOOLCHAIN_FILE=$PS2DEV/ps2sdk/ps2dev.cmake \\\n          -DSDL_WERROR=ON \\\n          -DSDL_TESTS=ON \\\n          -DCMAKE_INSTALL_PREFIX=cmake_prefix \\\n          -DCMAKE_BUILD_TYPE=Release\n    - name: Build\n      run: cmake --build build --config Release --verbose --parallel\n    - name: Install (CMake)\n      run: |\n        set -eu\n        cmake --install build/ --config Release\n        echo \"SDL2_DIR=$(pwd)/cmake_prefix\" >> $GITHUB_ENV\n        ( cd cmake_prefix; find ) | LC_ALL=C sort -u\n\n    - name: Verify CMake configuration files\n      run: |\n        cmake -S cmake/test -B cmake_config_build -G Ninja \\\n          -DCMAKE_TOOLCHAIN_FILE=$PS2DEV/ps2sdk/ps2dev.cmake \\\n          -DTEST_SHARED=FALSE \\\n          -DCMAKE_PREFIX_PATH=${{ env.SDL2_DIR }} \\\n          -DCMAKE_BUILD_TYPE=Release\n        cmake --build cmake_config_build --verbose\n    - name: Verify sdl2-config\n      run: |\n        export CC=mips64r5900el-ps2-elf-gcc\n        export PATH=${{ env.SDL2_DIR }}/bin:$PATH\n        export EXTRA_LDFLAGS=\"-L$PS2DEV/ps2sdk/ee/lib -L$PS2DEV/gsKit/lib -L$PS2DEV/ps2sdk/ports/lib\"\n        cmake/test/test_sdlconfig.sh\n    - name: Verify sdl2.pc\n      run: |\n        export CC=mips64r5900el-ps2-elf-gcc\n        export EXTRA_LDFLAGS=\"-L$PS2DEV/ps2sdk/ee/lib -L$PS2DEV/gsKit/lib -L$PS2DEV/ps2sdk/ports/lib\"\n        export PKG_CONFIG_PATH=${{ env.SDL2_DIR }}/lib/pkgconfig\n        cmake/test/test_pkgconfig.sh\n    \n    - name: Get short SHA\n      id: slug\n      run: echo \"::set-output name=sha8::$(echo ${GITHUB_SHA} | cut -c1-8)\"\n\n    - name: Upload artifacts\n      if: ${{ success() }}\n      uses: actions/upload-artifact@v3\n      with:\n        name: tests-${{ steps.slug.outputs.sha8 }}\n        path: |\n          build/test\n",
    "source": "ultralight-ux/SDL",
    "path": ".github/workflows/ps2.yaml",
    "url": "https://github.com/ultralight-ux/SDL/blob/0fb4721d90dc10e72f24dcbd0f0e0aa2eff6749f/.github/workflows/ps2.yaml",
    "retrieved_at": "2025-08-29T01:40:34.945332Z"
  },
  {
    "question": "What pull request events trigger this workflow to run linting checks?",
    "answer": "name: Lint\non:\n  pull_request_target:\n    branches:\n      - main\njobs:\n  lint:\n    strategy:\n      matrix:\n        node: [\"14.x\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v2\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n          fetch-depth: 2\n\n      - name: Use Node.js 14.x\n        uses: actions/setup-node@v2\n        with:\n          node-version: ${{ matrix.node }}\n          # cache: \"yarn\"\n          # cache-dependency-path: yarn.lock\n\n      - name: Install deps\n        if: steps.yarn-cache.outputs.cache-hit != 'true'\n        run: yarn\n\n      - name: Lint\n        run: yarn lint:report\n        continue-on-error: true\n\n      - name: Merge lint reports\n        run: jq -s '[.[]]|flatten' lint-results/*.json &> lint-results/eslint_report.json\n\n      - name: Annotate Code Linting Results\n        uses: ataylorme/eslint-annotate-action@1.2.0\n        with:\n          repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          report-json: \"lint-results/eslint_report.json\"\n\n      - name: Upload ESLint report\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: lint-results\n          path: lint-results\n",
    "source": "TerribleDev/cal",
    "path": ".github/workflows/lint.yml",
    "url": "https://github.com/TerribleDev/cal/blob/f2988870d5516d6f4ede2a54f6c670e7c27f41f3/.github/workflows/lint.yml",
    "retrieved_at": "2025-08-29T01:40:35.591615Z"
  },
  {
    "question": "How does this workflow use pull_request_target to trigger linting on code from external contributors?",
    "answer": "name: Lint\non:\n  pull_request_target:\n    branches:\n      - main\njobs:\n  lint:\n    strategy:\n      matrix:\n        node: [\"14.x\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v2\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n          fetch-depth: 2\n\n      - name: Use Node.js 14.x\n        uses: actions/setup-node@v2\n        with:\n          node-version: ${{ matrix.node }}\n          # cache: \"yarn\"\n          # cache-dependency-path: yarn.lock\n\n      - name: Install deps\n        if: steps.yarn-cache.outputs.cache-hit != 'true'\n        run: yarn\n\n      - name: Lint\n        run: yarn lint:report\n        continue-on-error: true\n\n      - name: Merge lint reports\n        run: jq -s '[.[]]|flatten' lint-results/*.json &> lint-results/eslint_report.json\n\n      - name: Annotate Code Linting Results\n        uses: ataylorme/eslint-annotate-action@1.2.0\n        with:\n          repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          report-json: \"lint-results/eslint_report.json\"\n\n      - name: Upload ESLint report\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: lint-results\n          path: lint-results\n",
    "source": "undefine-org/cal.com",
    "path": ".github/workflows/lint.yml",
    "url": "https://github.com/undefine-org/cal.com/blob/66aeadffbb370363166eadc502592f06a60e496f/.github/workflows/lint.yml",
    "retrieved_at": "2025-08-29T01:40:36.295989Z"
  },
  {
    "question": "Does this workflow run linting checks and report them on pull requests targeting the main branch?",
    "answer": "name: Lint\non:\n  pull_request_target:\n    branches:\n      - main\njobs:\n  lint:\n    strategy:\n      matrix:\n        node: [\"14.x\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v2\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n          fetch-depth: 2\n\n      - name: Use Node.js 14.x\n        uses: actions/setup-node@v2\n        with:\n          node-version: ${{ matrix.node }}\n          # cache: \"yarn\"\n          # cache-dependency-path: yarn.lock\n\n      - name: Install deps\n        if: steps.yarn-cache.outputs.cache-hit != 'true'\n        run: yarn\n\n      - name: Lint\n        run: yarn lint:report\n        continue-on-error: true\n\n      - name: Merge lint reports\n        run: jq -s '[.[]]|flatten' lint-results/*.json &> lint-results/eslint_report.json\n\n      - name: Annotate Code Linting Results\n        uses: ataylorme/eslint-annotate-action@1.2.0\n        with:\n          repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          report-json: \"lint-results/eslint_report.json\"\n\n      - name: Upload ESLint report\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: lint-results\n          path: lint-results\n",
    "source": "topperge/calendso",
    "path": ".github/workflows/lint.yml",
    "url": "https://github.com/topperge/calendso/blob/2c4a891a89c6866c84eed147a1d52c6a0efbe371/.github/workflows/lint.yml",
    "retrieved_at": "2025-08-29T01:40:36.944633Z"
  },
  {
    "question": "How does this workflow annotate pull requests with ESLint results from the `lint-results/eslint_report.json` file?",
    "answer": "name: Lint\non:\n  pull_request_target:\n    branches:\n      - main\njobs:\n  lint:\n    strategy:\n      matrix:\n        node: [\"14.x\"]\n        os: [ubuntu-latest]\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v2\n        with:\n          ref: ${{ github.event.pull_request.head.sha }}\n          fetch-depth: 2\n\n      - name: Use Node.js 14.x\n        uses: actions/setup-node@v2\n        with:\n          node-version: ${{ matrix.node }}\n          # cache: \"yarn\"\n          # cache-dependency-path: yarn.lock\n\n      - name: Install deps\n        if: steps.yarn-cache.outputs.cache-hit != 'true'\n        run: yarn\n\n      - name: Lint\n        run: yarn lint:report\n        continue-on-error: true\n\n      - name: Merge lint reports\n        run: jq -s '[.[]]|flatten' lint-results/*.json &> lint-results/eslint_report.json\n\n      - name: Annotate Code Linting Results\n        uses: ataylorme/eslint-annotate-action@1.2.0\n        with:\n          repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          report-json: \"lint-results/eslint_report.json\"\n\n      - name: Upload ESLint report\n        if: ${{ always() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: lint-results\n          path: lint-results\n",
    "source": "beefmcr/wevocalender",
    "path": ".github/workflows/lint.yml",
    "url": "https://github.com/beefmcr/wevocalender/blob/746643bf8e7eda0396e67c3581f5f6487b27b965/.github/workflows/lint.yml",
    "retrieved_at": "2025-08-29T01:40:37.607690Z"
  },
  {
    "question": "What specific changes to files under `csrc/`, `demo/csrc/`, or `CMakeLists.txt` trigger this workflow?",
    "answer": "name: backend-coreml\n\non:\n  push:\n    paths:\n      - \"csrc/**\"\n      - \"demo/csrc/**\"\n      - \"CMakeLists.txt\"\n\n  pull_request:\n    paths:\n      - \"csrc/**\"\n      - \"demo/csrc/**\"\n      - \"CMakeLists.txt\"\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\nenv:\n  DEVELOPER_DIR: /Applications/Xcode_13.4.1.app/Contents/Developer\npermissions:\n  contents: read\n\njobs:\n  build_macos_arm64:\n    runs-on: macos-12\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v3\n        with:\n          submodules: 'recursive'\n      - name: install opencv\n        run: |\n          wget https://github.com/irexyc/mmdeploy-ci-resource/releases/download/opencv/opencv-osx-arm64-4.6.0.tar.gz\n          mkdir $GITHUB_WORKSPACE/opencv-install\n          tar xf opencv-osx-arm64-4.6.0.tar.gz -C $GITHUB_WORKSPACE/opencv-install\n      - name: install libtorch\n        run: |\n          wget https://github.com/irexyc/mmdeploy-ci-resource/releases/download/libtorch/libtorch-osx-arm64-1.8.0.tar.gz\n          mkdir $GITHUB_WORKSPACE/libtorch-install\n          tar xf libtorch-osx-arm64-1.8.0.tar.gz -C $GITHUB_WORKSPACE/libtorch-install\n      - name: build\n        run: |\n          mkdir build && cd build\n          cmake .. -DCMAKE_OSX_ARCHITECTURES=\"arm64\" \\\n            -DCMAKE_SYSTEM_PROCESSOR=\"arm64\" \\\n            -DMMDEPLOY_BUILD_SDK=ON \\\n            -DMMDEPLOY_TARGET_DEVICES=\"cpu\" \\\n            -DMMDEPLOY_CODEBASES=all \\\n            -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4 \\\n            -DTorch_DIR=$GITHUB_WORKSPACE/libtorch-install/share/cmake/Torch \\\n            -DMMDEPLOY_TARGET_BACKENDS=\"coreml\" \\\n            -DMMDEPLOY_BUILD_EXAMPLES=ON \\\n            -DMMDEPLOY_SHARED_LIBS=OFF\n          cmake --build . -j 3\n          cmake --build . --target install\n      - name: build-shared\n        run: |\n          mkdir build-shared && cd build-shared\n          cmake .. -DCMAKE_OSX_ARCHITECTURES=\"arm64\" \\\n            -DCMAKE_SYSTEM_PROCESSOR=\"arm64\" \\\n            -DMMDEPLOY_BUILD_SDK=ON \\\n            -DMMDEPLOY_TARGET_DEVICES=\"cpu\" \\\n            -DMMDEPLOY_CODEBASES=all \\\n            -DOpenCV_DIR=$GITHUB_WORKSPACE/opencv-install/lib/cmake/opencv4 \\\n            -DTorch_DIR=$GITHUB_WORKSPACE/libtorch-install/share/cmake/Torch \\\n            -DMMDEPLOY_TARGET_BACKENDS=\"coreml\" \\\n            -DMMDEPLOY_BUILD_EXAMPLES=ON \\\n            -DMMDEPLOY_SHARED_LIBS=ON\n          cmake --build . -j 3\n          cmake --build . --target install\n",
    "source": "drilistbox/mmdeploy",
    "path": ".github/workflows/backend-coreml.yml",
    "url": "https://github.com/drilistbox/mmdeploy/blob/23b3d9034b15ae6d33ae2fe164eeb583f3d0b6b4/.github/workflows/backend-coreml.yml",
    "retrieved_at": "2025-08-29T01:40:38.440616Z"
  },
  {
    "question": "Under what conditions will the code coverage be uploaded to Codecov?",
    "answer": "# This is a basic workflow to help you get started with Actions\n\nname: CI\n\n# Controls when the action will run.\non:\n  # Triggers the workflow on push or pull request events but only for the master branch\n  push:\n    branches: [master]\n  pull_request:\n    branches: [master]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n  build:\n    # The type of runner that the job will run on\n    runs-on: macos-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        channel: [stable, beta, dev]\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      - name: Flutter action\n        uses: subosito/flutter-action@v1\n        with:\n          channel: ${{ matrix.channel }}\n\n      - name: Run Tests\n        run: |\n          flutter pub get\n          flutter format --dry-run --set-exit-if-changed .\n          flutter analyze --no-pub\n          flutter test --no-pub --coverage\n      - name: Upload coverage to Codecov\n        if: ${{ matrix.channel == 'stable' }}\n        uses: codecov/codecov-action@v1\n        with:\n          file: coverage/lcov.info\n",
    "source": "nlfiedler/choose_input_chips",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/nlfiedler/choose_input_chips/blob/570bfc32cc6bb25641e90a6c11e419d497602d9e/.github/workflows/main.yml",
    "retrieved_at": "2025-08-29T01:40:39.151416Z"
  },
  {
    "question": "Under what conditions is code coverage data uploaded to Codecov?",
    "answer": "# This is a basic workflow to help you get started with Actions\n\nname: CI\n\n# Controls when the action will run.\non:\n  # Triggers the workflow on push or pull request events but only for the master branch\n  push:\n    branches: [master]\n  pull_request:\n    branches: [master]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n  build:\n    # The type of runner that the job will run on\n    runs-on: macos-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        channel: [stable, beta, dev]\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      - name: Flutter action\n        uses: subosito/flutter-action@v1\n        with:\n          channel: ${{ matrix.channel }}\n\n      - name: Run Tests\n        run: |\n          flutter pub get\n          flutter format --dry-run --set-exit-if-changed .\n          flutter analyze --no-pub\n          flutter test --no-pub --coverage\n      - name: Upload coverage to Codecov\n        if: ${{ matrix.channel == 'stable' }}\n        uses: codecov/codecov-action@v1\n        with:\n          file: coverage/lcov.info\n",
    "source": "jackz314/flutter_chips_input",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/jackz314/flutter_chips_input/blob/fde20b6b61da8c167b18af7d7aa66b9ccae75a23/.github/workflows/main.yml",
    "retrieved_at": "2025-08-29T01:40:39.799522Z"
  },
  {
    "question": "Under what conditions does the \"CLA Assistant\" step execute, and what actions does it perform when triggered?",
    "answer": "name: \"CLA Assistant\"\non:\n  issue_comment:\n    types: [created]\n  pull_request_target:\n    types: [opened,closed,synchronize]\n\njobs:\n  CLAssistant:\n    runs-on: ubuntu-latest\n    steps:\n      - name: \"CLA Assistant\"\n        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'\n        # Beta Release\n        uses: cla-assistant/github-action@v2.1.3-beta\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          # the below token should have repo scope and must be manually added by you in the repository's secret\n          PERSONAL_ACCESS_TOKEN : ${{ secrets.PERSONAL_ACCESS_TOKEN }}\n        with:\n          path-to-signatures: 'cla.json'\n          path-to-document: 'https://github.com/0xPolygon/polygon-edge/blob/develop/CLA.md'\n          branch: 'cla-signatures'\n          allowlist: dependabot[bot],dependabot-preview[bot]\n",
    "source": "NEXTEP-CXS/Nextep",
    "path": ".github/workflows/cla.yml",
    "url": "https://github.com/NEXTEP-CXS/Nextep/blob/bde78172a8cc012c06c39daffdcd3ff5ef294ead/.github/workflows/cla.yml",
    "retrieved_at": "2025-08-29T01:40:40.463783Z"
  },
  {
    "question": "Under what conditions will the `lint` job run, considering both `pull_request` and `push` triggers and their associated path filters?",
    "answer": "name: lint\n\non:\n  pull_request:\n    paths:\n      - '**'\n\n  push:\n    branches:\n      - develop\n    paths:\n      - '**'\n      - '!.github/**'\n      - '.github/workflows/lint.yaml'\n      - '!docker/**'\n      - '!docs/**'\n      - '!contrib/**'\n  workflow_dispatch:\n\npermissions: {}\n\n# When a PR is updated, cancel the jobs from the previous version. Merges\n# do not define head_ref, so use run_id to never cancel those jobs.\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n  cancel-in-progress: true\n\njobs:\n  lint:\n    timeout-minutes: 15\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.12\"]\n        lintcommand:\n          - \"pylint -j 2 datacube\"\n          - \"mypy datacube examples integration_tests tests\"\n    name: Linting\n    steps:\n      - name: checkout git\n        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n        with:\n          fetch-depth: 0\n          persist-credentials: false\n      - name: Install uv\n        uses: astral-sh/setup-uv@4959332f0f014c5280e7eac8b70c90cb574c9f9b # v6.6.0\n      - name: run linter\n        run: |\n          uv run ${LINT_COMMAND}\n        env:\n          LINT_COMMAND: ${{matrix.lintcommand}}\n          UV_PYTHON: ${{ matrix.python-version }}\n          UV_PYTHON_PREFERENCE: managed\n",
    "source": "opendatacube/datacube-core",
    "path": ".github/workflows/lint.yaml",
    "url": "https://github.com/opendatacube/datacube-core/blob/fa888a11cde4a2e32041280ac79875e1e013a540/.github/workflows/lint.yaml",
    "retrieved_at": "2025-08-30T01:35:34.470017Z"
  },
  {
    "question": "What happens when no files are found during the artifact upload in the `buildRuleSet` job?",
    "answer": "name: Continuous Integration\n\non:\n  push:\n  pull_request:\n    branches:\n      - main\n\nenv:\n  VERSION: \"1.0.2\"\n\njobs:\n  testRules:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Run guard rules tests\n      shell: bash\n      run: |\n        curl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/aws-cloudformation/cloudformation-guard/main/install-guard.sh | sh\n        export PATH=${PATH}:~/.guard/bin\n        cfn-guard test -d ./rules/\n    ## If test fails run step to pull out only failed tests\n    - name: Display Failed Rules Only\n      if: ${{ failure() }}\n      shell: bash\n      run: |\n        curl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/aws-cloudformation/cloudformation-guard/main/install-guard.sh | sh\n        export PATH=${PATH}:~/.guard/bin\n        cfn-guard test -d ./rules/ | grep \"FAIL Rules:\" -B 2 -A 1\n  buildRuleSet:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repo\n        uses: actions/checkout@v2\n      - run: |\n          chmod +x ./mappings/build.py\n          python3 ./mappings/build.py -r $VERSION\n        shell: bash\n      - uses: actions/upload-artifact@v3\n        with:\n          name: ruleset-build\n          path: |\n            docker/output/\n            mappings/rule_set_guard_rules_registry_all_rules.json\n          if-no-files-found: error\n",
    "source": "aws-cloudformation/aws-guard-rules-registry",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/aws-cloudformation/aws-guard-rules-registry/blob/7f7340c26ae5d5e8874651dbffeb12e0e9f505b6/.github/workflows/ci.yml",
    "retrieved_at": "2025-08-30T01:35:35.369255Z"
  },
  {
    "question": "For each push or pull request, what GHC versions are used to build the project?",
    "answer": "name: Cabal CI\n\non:\n  push:\n    branches:\n      - master\n  pull_request:\n\njobs:\n  build:\n    name: cabal ${{ matrix.ghc }}\n    runs-on: ubuntu-16.04\n    strategy:\n      matrix:\n        ghc: [\"8.10.1\", \"8.8.1\", \"8.6.5\", \"8.6.4\", \"8.6.3\", \"8.6.2\"]\n        cabal: [\"3.0\"]\n\n    steps:\n    - uses: actions/checkout@v1\n    - uses: actions/setup-haskell@v1\n      name: Setup Haskell\n      with:\n        ghc-version: ${{ matrix.ghc }}\n        cabal-version: ${{ matrix.cabal }}\n\n    - uses: actions/cache@v1\n      name: Cache ~/.cabal/packages\n      with:\n        path: ~/.cabal/packages\n        key: cabal-packages-${{ matrix.ghc }}\n\n    - uses: actions/cache@v1\n      name: Cache ~/.cabal/store\n      with:\n        path: ~/.cabal/store\n        key: cabal-store-${{ matrix.ghc }}\n\n    - uses: actions/cache@v1\n      name: Cache dist-newstyle\n      with:\n        path: dist-newstyle\n        key: dist-newstyle-${{ matrix.ghc }}\n\n    - name: Install dependencies\n      run: |\n        cabal update\n    - name: Build\n      run: |\n        cabal new-build\n",
    "source": "sdiehl/pairing",
    "path": ".github/workflows/cabal.yml",
    "url": "https://github.com/sdiehl/pairing/blob/fa41b722d9f260bd00be0b250ce7cc5324f26a09/.github/workflows/cabal.yml",
    "retrieved_at": "2025-08-30T01:35:36.293912Z"
  },
  {
    "question": "Which branches trigger this workflow on push and pull requests, and which files are ignored on pull requests?",
    "answer": "name: Flatpak CI\n\non:\n  push:\n    branches:\n    - main\n    - flatpak-1.0.x\n    - flatpak-1.2.x\n    - flatpak-1.4.x\n    - flatpak-1.6.x\n    - flatpak-1.8.x\n    - flatpak-1.10.x\n    - flatpak-1.12.x\n    - flatpak-1.14.x\n  pull_request:\n    paths-ignore:\n    - README.md\n    - CONTRIBUTING.md\n    - NEWS\n    - COPYING\n    - CODE_OF_CONDUCT.md\n    - uncrustify.cfg\n    - uncrustify.sh\n    branches:\n    - main\n    - flatpak-1.0.x\n    - flatpak-1.2.x\n    - flatpak-1.4.x\n    - flatpak-1.6.x\n    - flatpak-1.8.x\n    - flatpak-1.10.x\n    - flatpak-1.12.x\n    - flatpak-1.14.x\n\npermissions:\n  contents: read\n\njobs:\n  check:\n    name: Build with gcc and test\n    runs-on: ubuntu-22.04\n    steps:\n    - name: Install Dependencies\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y libglib2.0-dev attr automake gettext autopoint bison  dbus gtk-doc-tools \\\n        libfuse3-dev ostree libostree-dev libarchive-dev libzstd-dev libcap-dev libattr1-dev libdw-dev libelf-dev python3-pyparsing \\\n        libjson-glib-dev shared-mime-info desktop-file-utils libpolkit-agent-1-dev libpolkit-gobject-1-dev \\\n        libseccomp-dev libsoup2.4-dev libcurl4-openssl-dev libsystemd-dev libxml2-utils libgpgme11-dev gobject-introspection \\\n        libgirepository1.0-dev libappstream-dev libdconf-dev clang socat meson libdbus-1-dev e2fslibs-dev bubblewrap xdg-dbus-proxy \\\n        python3-pip meson ninja-build libyaml-dev libstemmer-dev gperf itstool libmalcontent-0-dev\n        # One of the tests wants this\n        sudo mkdir /tmp/flatpak-com.example.App-OwnedByRoot\n    - name: Check out flatpak\n      uses: actions/checkout@v3\n      with:\n        submodules: true\n    - name: Build appstream dependency # (We need at least 0.15.3 for the g_once fix)\n      run: |\n        sudo pip3 install 'meson~=0.62'\n        git clone --branch v0.15.4 --depth 1 --no-tags https://github.com/ximion/appstream.git ./appstream\n        pushd ./appstream\n        meson setup --prefix=/usr _build\n        ninja -C _build\n        sudo ninja -C _build install\n        popd\n    - name: Create logs dir\n      run: mkdir test-logs\n    - name: autogen.sh\n      run: NOCONFIGURE=1 ./autogen.sh\n    - name: configure\n      # We don't do gtk-doc or GObject-Introspection here, because they can\n      # clash with AddressSanitizer. Instead, the clang build enables those.\n      run: |\n        mkdir _build\n        pushd _build\n        ../configure  --enable-internal-checks --enable-asan --disable-introspection --with-curl --with-system-bubblewrap --with-system-dbus-proxy\n        popd\n      env:\n        CFLAGS: -O2 -Wp,-D_FORTIFY_SOURCE=2\n    - name: Build flatpak\n      run: make -C _build -j $(getconf _NPROCESSORS_ONLN)\n    - name: Run tests\n      run: make -C _build check -j $(getconf _NPROCESSORS_ONLN)\n      env:\n        ASAN_OPTIONS: detect_leaks=0 # Right now we're not fully clean, but this gets us use-after-free etc\n    - name: Collect overall test logs on failure\n      if: failure()\n      run: mv _build/test-suite.log test-logs/ || true\n    - name: Collect individual test logs on cancel\n      if: failure() || cancelled()\n      run: mv _build/tests/*.log test-logs/ || true\n    - name: Upload test logs\n      uses: actions/upload-artifact@v3\n      if: failure() || cancelled()\n      with:\n        name: test logs\n        path: test-logs\n\n  # This is similar to the above, but runs on an older OS with some different configuration:\n  # * Soup instead of curl\n  # * Use built in bubblewrap instead of external\n  # * Use built in xdg-dbus-proxy instead of external\n  # * Disable malcontent build-dependency\n  check-alt2:\n    name: Build with gcc and test (older)\n    runs-on: ubuntu-20.04\n    steps:\n    - name: Install Dependencies\n      run: |\n        sudo add-apt-repository ppa:flatpak/stable\n        sudo add-apt-repository 'deb https://download.mono-project.com/repo/ubuntu stable-bionic main' # Needed for updates to work\n        sudo apt-get update\n        sudo apt-get install -y libglib2.0-dev attr automake gettext autopoint bison  dbus gtk-doc-tools \\\n        libfuse-dev ostree libostree-dev libarchive-dev libzstd-dev libcap-dev libattr1-dev libdw-dev libelf-dev python3-pyparsing \\\n        libjson-glib-dev shared-mime-info desktop-file-utils libpolkit-agent-1-dev libpolkit-gobject-1-dev \\\n        libseccomp-dev libsoup2.4-dev libcurl4-openssl-dev libsystemd-dev libxml2-utils libgpgme11-dev gobject-introspection \\\n        libgirepository1.0-dev libappstream-dev libdconf-dev clang socat meson libdbus-1-dev e2fslibs-dev\n        # One of the tests wants this\n        sudo mkdir /tmp/flatpak-com.example.App-OwnedByRoot\n    - name: Check out flatpak\n      uses: actions/checkout@v3\n      with:\n        submodules: true\n    - name: Create logs dir\n      run: mkdir test-logs\n    - name: autogen.sh\n      run: NOCONFIGURE=1 ./autogen.sh\n    - name: configure\n      # We don't do gtk-doc or GObject-Introspection here, because they can\n      # clash with AddressSanitizer. Instead, the clang build enables those.\n      run: |\n        mkdir _build\n        pushd _build\n        ../configure  --enable-internal-checks --enable-asan --disable-introspection --without-curl\n        popd\n      env:\n        CFLAGS: -O2 -Wp,-D_FORTIFY_SOURCE=2\n    - name: Build flatpak\n      run: make -C _build -j $(getconf _NPROCESSORS_ONLN)\n    # We build with Ubuntu 18.04's GLib to prove that we can, but there's a\n    # race condition that makes it fail tests, so upgrade to a version from\n    # a PPA before running the tests: see\n    # https://github.com/flatpak/flatpak/pull/3121,\n    # https://gitlab.gnome.org/GNOME/glib/-/issues/1014\n    - name: Upgrade GLib before running tests\n      run: |\n        sudo apt-get install -y libglib2.0-dev\n    - name: Run tests\n      run: make -C _build check -j $(getconf _NPROCESSORS_ONLN)\n      env:\n        ASAN_OPTIONS: detect_leaks=0 # Right now we're not fully clean, but this gets us use-after-free etc\n    - name: Collect overall test logs on failure\n      if: failure()\n      run: mv _build/test-suite.log test-logs/ || true\n    - name: Collect individual test logs on cancel\n      if: failure() || cancelled()\n      run: mv _build/tests/*.log test-logs/ || true\n    - name: Upload test logs\n      uses: actions/upload-artifact@v3\n      if: failure() || cancelled()\n      with:\n        name: test logs\n        path: test-logs\n\n  clang:\n    permissions:\n      security-events: write # for codeql\n    name: Build with clang and analyze\n    runs-on: ubuntu-20.04\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'cpp', 'python' ]\n        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python' ]\n        # Learn more:\n        # https://docs.github.com/en/free-pro-team@latest/github/finding-security-vulnerabilities-and-errors-in-your-code/configuring-code-scanning#changing-the-languages-that-are-analyzed\n    steps:\n    # Initializes the CodeQL tools for scanning.\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v2\n      with:\n        languages: ${{ matrix.language }}\n        # If you wish to specify custom queries, you can do so here or in a config file.\n        # By default, queries listed here will override any specified in a config file.\n        # Prefix the list here with \"+\" to use these queries and those in the config file.\n        # queries: ./path/to/local/query, your-org/your-repo/queries@main\n    - name: Install Dependencies\n      run: |\n        sudo add-apt-repository ppa:flatpak/stable\n        sudo add-apt-repository 'deb https://download.mono-project.com/repo/ubuntu stable-bionic main' # Needed for updates to work\n        sudo apt-get update\n        sudo apt-get install -y libglib2.0-dev attr automake gettext autopoint bison  dbus gtk-doc-tools \\\n        libfuse-dev ostree libostree-dev libarchive-dev libzstd-dev libcap-dev libattr1-dev libdw-dev libelf-dev python3-pyparsing \\\n        libjson-glib-dev shared-mime-info desktop-file-utils libpolkit-agent-1-dev libpolkit-gobject-1-dev \\\n        libseccomp-dev libsoup2.4-dev libcurl4-openssl-dev libsystemd-dev libxml2-utils libgpgme11-dev gobject-introspection \\\n        libgirepository1.0-dev libappstream-dev libdconf-dev clang e2fslibs-dev\n    - name: Check out flatpak\n      uses: actions/checkout@v3\n      with:\n        submodules: true\n    - name: configure\n      run: ./autogen.sh\n      env:\n        CC: clang\n        CFLAGS: -Werror=unused-variable\n    - name: Build flatpak\n      run: make -j $(getconf _NPROCESSORS_ONLN)\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v2\n\n  valgrind:\n    name: Run tests in valgrind\n    needs: check # Don't run expensive test if main check fails\n    runs-on: ubuntu-22.04 # Might as well test with a different one too\n    if: ${{ false }} # Currently Valgrind takes too long and always fails\n    steps:\n    - name: Install Dependencies\n      run: |\n        sudo add-apt-repository ppa:flatpak/stable\n        sudo apt-get update\n        sudo add-apt-repository 'deb https://download.mono-project.com/repo/ubuntu stable-focal main' # Needed for updates to work\n        sudo apt-get install -y libglib2.0-dev attr automake gettext autopoint bison  dbus gtk-doc-tools \\\n        libfuse-dev ostree libostree-dev libarchive-dev libzstd-dev libcap-dev libattr1-dev libdw-dev libelf-dev python3-pyparsing \\\n        libjson-glib-dev shared-mime-info desktop-file-utils libpolkit-agent-1-dev libpolkit-gobject-1-dev \\\n        libseccomp-dev libsoup2.4-dev libcurl4-openssl-dev libsystemd-dev libxml2-utils libgpgme11-dev gobject-introspection \\\n        libgirepository1.0-dev libappstream-dev libdconf-dev clang socat meson libdbus-1-dev \\\n        valgrind e2fslibs-dev\n    - name: Check out flatpak\n      uses: actions/checkout@v3\n      with:\n        submodules: true\n    - name: Create logs dir\n      run: mkdir test-logs\n    - name: autogen.sh\n      run: NOCONFIGURE=1 ./autogen.sh\n    - name: configure\n      run: |\n        mkdir _build\n        pushd _build\n        ../configure --enable-gtk-doc --enable-gtk-doc-html --enable-introspection\n        popd\n      env:\n        CFLAGS: -O2\n    - name: Build flatpak\n      run: make -C _build -j $(getconf _NPROCESSORS_ONLN)\n    - name: Distcheck\n      run: make -C _build distcheck\n    - name: Run tests under valgrind\n      run: make -C _build check\n      env:\n        FLATPAK_TESTS_VALGRIND: true\n    - name: Collect overall test logs on failure\n      if: failure()\n      run: mv _build/test-suite.log test-logs/ || true\n    - name: Collect individual test logs on cancel\n      if: failure() || cancelled()\n      run: mv _build/tests/*.log test-logs/ || true\n    - name: Upload test logs\n      uses: actions/upload-artifact@v3\n      if: failure() || cancelled()\n      with:\n        name: test logs\n        path: test-logs\n",
    "source": "endlessm/flatpak",
    "path": ".github/workflows/check.yml",
    "url": "https://github.com/endlessm/flatpak/blob/787caf96aaf2b233020a776397a2584ee079af44/.github/workflows/check.yml",
    "retrieved_at": "2025-08-30T01:35:37.317505Z"
  },
  {
    "question": "What specific Deno version within the v1.x range will be used for linting and testing?",
    "answer": "# This workflow uses actions that are not certified by GitHub.\n# They are provided by a third-party and are governed by\n# separate terms of service, privacy policy, and support\n# documentation.\n\n# This workflow will install Deno then run `deno lint` and `deno test`.\n# For more information see: https://github.com/denoland/setup-deno\n\nname: Deno\n\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\n\npermissions:\n  contents: read\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Setup repo\n        uses: actions/checkout@v4\n\n      - name: Setup Deno\n        # uses: denoland/setup-deno@v1\n        uses: denoland/setup-deno@61fe2df320078202e33d7d5ad347e7dcfa0e8f31  # v1.1.2\n        with:\n          deno-version: v1.x\n\n      # Uncomment this step to verify the use of 'deno fmt' on each commit.\n      # - name: Verify formatting\n      #   run: deno fmt --check\n\n      - name: Run linter\n        run: deno lint\n\n      - name: Run tests\n        run: deno test -A\n",
    "source": "Keasanda/keasanda-portfolio",
    "path": ".github/workflows/deno2.yml",
    "url": "https://github.com/Keasanda/keasanda-portfolio/blob/ff233ac4c7c0a4246e01c1a196c61259b21a9dc2/.github/workflows/deno2.yml",
    "retrieved_at": "2025-08-30T01:35:38.255469Z"
  },
  {
    "question": "What triggers this workflow to synchronize translations with Crowdin?",
    "answer": "name: Crowdin Action\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  synchronize-with-crowdin:\n    runs-on: ubuntu-latest\n\n    steps:\n\n    - name: Checkout\n      uses: actions/checkout@v2\n\n    - name: crowdin action\n      uses: crowdin/github-action@1.4.2\n      with:\n        upload_translations: true\n        download_translations: true\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        CROWDIN_PROJECT_ID: ${{ secrets.CROWDIN_PROJECT_ID }}\n        CROWDIN_PERSONAL_TOKEN: ${{ secrets.CROWDIN_PERSONAL_TOKEN }}\n",
    "source": "sagivo/calendso-2",
    "path": ".github/workflows/crowdin.yml",
    "url": "https://github.com/sagivo/calendso-2/blob/0d50d278bbb734598e112f507334596544a44955/.github/workflows/crowdin.yml",
    "retrieved_at": "2025-08-30T01:35:39.164089Z"
  },
  {
    "question": "What specific Android application is built and published by this workflow?",
    "answer": "\n# This is a basic workflow to help you get started with Actions\n\nname: Build\n\n# Controls when the action will run. \non:\n  # Triggers the workflow on push or pull request events but only for the master branch\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n\n  buildAndroid:\n    name: buildAndroid\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@main\n\n      - name: Setup Android NDK\n        uses: nttld/setup-ndk@main\n        id: setup-ndk\n        with:\n          ndk-version: r15c\n\n      - name: Setup Java JDK\n        uses: actions/setup-java@main\n        with:\n          distribution: 'zulu'\n          java-version: 11\n\n      - name: Setup Android SDK\n        uses: android-actions/setup-android@main\n\n      - name: Setup Haxe\n        uses: krdlab/setup-haxe@v1.2.0\n        with:\n          haxe-version: 4.2.0\n\n      - name: Install Haxelib\n        run: |\n          haxelib setup ~/haxelib\n          haxelib install hxcpp 4.2.1 > /dev/null\n          haxelib install lime 7.9.0\n          haxelib install openfl 9.1.0\n          haxelib --never install flixel 4.11.0\n          haxelib run lime setup flixel\n          haxelib install flixel-tools\n          haxelib install flixel-ui\n          haxelib install flixel-addons 2.11.0\n          haxelib install tjson\n          haxelib install hxjsonast\n          haxelib install hscript\n          haxelib git hxCodec https://github.com/SPLCoding/hxCodec-but-it-works-xd.git\n          haxelib git linc_luajit https://github.com/Sirox228/linc_luajit\n          haxelib git extension-androidtools https://github.com/MaysLastPlay77/extension-androidtools\n          haxelib install hxcpp-debug-server\n          haxelib list\n      - name: Create Version Tag\n        run: echo \"${{github.run_id}}\" > VERSION\n\n      - name: Setup Lime\n        run: |\n          haxelib run lime setup -alias -y\n          haxelib run lime config ANDROID_SDK $ANDROID_HOME\n          haxelib run lime config ANDROID_NDK_ROOT $ANDROID_NDK_HOME\n          haxelib run lime config JAVA_HOME $JAVA_HOME\n          haxelib run lime config ANDROID_SETUP true\n          haxelib set lime 7.9.0\n          haxelib set openfl 9.1.0\n          haxelib set flixel 4.11.0\n          haxelib set flixel-addons 2.11.0\n          haxelib set hxcpp 4.2.1\n        env:\n          ANDROID_NDK_HOME: ${{ steps.setup-ndk.outputs.ndk-path }}\n\n      - name: Compile\n        run: haxelib run lime build android -D NO_PRECOMPILED_HEADERS --app-version=\"4.0.0-${{ github.run_id}}\"\n\n      - name: Publish Artifact\n        uses: actions/upload-artifact@main\n        with:\n          name: buildAndroid\n          path: export/release/android/bin/app/build/outputs/apk/debug\n",
    "source": "NighCyan/FNF-TG-Engine",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/NighCyan/FNF-TG-Engine/blob/84be9d5daa633cad25344c76d2fc40117cb63b5b/.github/workflows/main.yml",
    "retrieved_at": "2025-08-30T01:35:40.037037Z"
  },
  {
    "question": "What security vulnerabilities does `zizmor` analyze in the repository when triggered by a push to `main` or a pull request?",
    "answer": "name: GitHub Actions Security Analysis with zizmor\n\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n\npermissions: {}\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}\n  cancel-in-progress: true\n\njobs:\n  zizmor:\n    name: zizmor latest via uv\n    runs-on: ubuntu-latest\n\n    permissions:\n      security-events: write\n      contents: read\n      actions: read\n\n    steps:\n      - name: Harden Runner\n        uses: step-security/harden-runner@ec9f2d5744a09debf3a187a3f4f675c53b671911 # v2.13.0\n        with:\n          disable-sudo: true\n          egress-policy: block\n          allowed-endpoints: >\n            api.github.com:443\n            release-assets.githubusercontent.com:443\n            files.pythonhosted.org:443\n            github.com:443\n            objects.githubusercontent.com:443\n            pypi.org:443\n\n      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n        with:\n          persist-credentials: false\n\n      - uses: astral-sh/setup-uv@d9e0f98d3fc6adb07d1e3d37f3043649ddad06a1 # v6.5.0\n\n      - run: uvx zizmor --persona pedantic --format sarif . > results.sarif\n        env:\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n      - uses: github/codeql-action/upload-sarif@3c3833e0f8c1c83d449a7478aa59c036a9165498 # v3.29.5\n        with:\n          sarif_file: results.sarif\n          category: zizmor\n",
    "source": "halostatue/diff-lcs",
    "path": ".github/workflows/zizmor.yml",
    "url": "https://github.com/halostatue/diff-lcs/blob/d8423754bfbd62619d784dde71a23e718e08c15d/.github/workflows/zizmor.yml",
    "retrieved_at": "2025-08-30T01:35:41.136954Z"
  },
  {
    "question": "What ref is checked out if the `branch` input is not provided during a `workflow_dispatch` event?",
    "answer": "name: Lint Python\n\non:\n  workflow_call:\n  workflow_dispatch:\n    inputs:\n      branch:\n        description: \"(Optional) Branch to checkout\"\n        required: false\n        type: string\nenv:\n  POETRY_VERSION: \"1.8.2\"\n\n\njobs:\n  lint:\n    name: Run Mypy\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version:\n          - \"3.13\"\n          - \"3.12\"\n          - \"3.11\"\n          - \"3.10\"\n    steps:\n      - name: Check out the code at a specific ref\n        uses: actions/checkout@v4\n        with:\n          ref: ${{ inputs.branch || github.ref }}\n          persist-credentials: true\n      - name: \"Setup Environment\"\n        uses: ./.github/actions/setup-uv\n      - name: Install the project\n        run: uv sync --dev\n      - name: Run Mypy\n        run: |\n          uv run mypy --namespace-packages -p \"langflow\"\n        env:\n          GITHUB_TOKEN: ${{ secrets.github_token }}\n      - name: Minimize uv cache\n        run: uv cache prune --ci\n",
    "source": "nawadkar/langflow-auth0",
    "path": ".github/workflows/lint-py.yml",
    "url": "https://github.com/nawadkar/langflow-auth0/blob/9a9be60857a4317b71ea8ac26d01afe247b3f224/.github/workflows/lint-py.yml",
    "retrieved_at": "2025-08-30T01:35:42.063675Z"
  },
  {
    "question": "Under what conditions is a GitHub release created, and what artifacts are included?",
    "answer": "name: Langflow Release\nrun-name: Langflow Release by @${{ github.actor }}\n\non:\n  workflow_dispatch:\n    inputs:\n      release_package_base:\n        description: \"Release Langflow Base\"\n        required: true\n        type: boolean\n        default: false\n      release_package_main:\n        description: \"Release Langflow\"\n        required: true\n        type: boolean\n        default: false\n      build_docker_base:\n        description: \"Build Docker Image for Langflow Base\"\n        required: true\n        type: boolean\n        default: false\n      build_docker_main:\n        description: \"Build Docker Image for Langflow\"\n        required: true\n        type: boolean\n        default: false\n      build_docker_ep:\n        description: \"Build Docker Image for Langflow with Entrypoint\"\n        required: false\n        type: boolean\n        default: false\n      pre_release:\n        description: \"Pre-release\"\n        required: false\n        type: boolean\n        default: false\n      create_release:\n        description: \"Whether to create a gh release\"\n        required: false\n        type: boolean\n        default: true\n\n\njobs:\n  ci:\n    if: ${{ github.event.inputs.release_package_base == 'true' || github.event.inputs.release_package_main == 'true' }}\n    name: CI\n    uses: ./.github/workflows/ci.yml\n    with:\n      python-versions: \"['3.10', '3.11', '3.12']\"\n      frontend-tests-folder: \"tests\"\n      release: true\n\n  release-base:\n    name: Release Langflow Base\n    needs: [ci]\n    if: inputs.release_package_base == true\n    runs-on: ubuntu-latest\n    outputs:\n      version: ${{ steps.check-version.outputs.version }}\n      skipped: ${{ steps.check-version.outputs.skipped }}\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Setup Environment\n        uses: ./.github/actions/setup-uv\n      - name: Install the project\n        run: uv sync --dev\n      - name: Check Version\n        id: check-version\n        run: |\n          version=$(uv tree | grep 'langflow-base' | awk '{print $3}' | sed 's/^v//')\n          last_released_version=$(curl -s \"https://pypi.org/pypi/langflow-base/json\" | jq -r '.releases | keys | .[]' | sort -V | tail -n 1)\n          if [ \"$version\" = \"$last_released_version\" ]; then\n            echo \"Version $version is already released. Skipping release.\"\n            echo skipped=true >> $GITHUB_OUTPUT\n            exit 0\n          else\n            echo version=$version >> $GITHUB_OUTPUT\n            echo skipped=false >> $GITHUB_OUTPUT\n          fi\n      - name: Build project for distribution\n        if: steps.check-version.outputs.skipped == 'false'\n        run: make build base=true args=\"--wheel\"\n      - name: Test CLI\n        if: steps.check-version.outputs.skipped == 'false'\n        run: |\n          # TODO: Unsure why the whl is not built in src/backend/base/dist\n          mkdir src/backend/base/dist\n          mv dist/*.whl src/backend/base/dist\n          uv pip install src/backend/base/dist/*.whl\n          uv run python -m langflow run --host 127.0.0.1 --port 7860 --backend-only &\n          SERVER_PID=$!\n          # Wait for the server to start\n          timeout 120 bash -c 'until curl -f http://127.0.0.1:7860/api/v1/auto_login; do sleep 2; done' || (echo \"Server did not start in time\" && kill $SERVER_PID && exit 1)\n          # Terminate the server\n          kill $SERVER_PID || (echo \"Failed to terminate the server\" && exit 1)\n          sleep 20 # give the server some time to terminate\n          # Check if the server is still running\n          if kill -0 $SERVER_PID 2>/dev/null; then\n            echo \"Failed to terminate the server\"\n            exit 0\n          else\n            echo \"Server terminated successfully\"\n          fi\n      - name: Publish to PyPI\n        if: steps.check-version.outputs.skipped == 'false'\n        env:\n          UV_PUBLISH_TOKEN: ${{ secrets.PYPI_API_TOKEN }}\n        run: |\n          make publish base=true\n      - name: Upload Artifact\n        if: steps.check-version.outputs.skipped == 'false'\n        uses: actions/upload-artifact@v4\n        with:\n          name: dist-base\n          path: src/backend/base/dist\n\n  release-main:\n    name: Release Langflow Main\n    if: inputs.release_package_main == true\n    needs: [release-base]\n    runs-on: ubuntu-latest\n    outputs:\n      version: ${{ steps.check-version.outputs.version }}\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Setup Environment\n        uses: ./.github/actions/setup-uv\n      - name: Install the project\n        run: uv sync --dev\n\n      # If pre-release is true, we need to check if  [\"a\", \"b\", \"rc\", \"dev\", \"post\"] is in the version string\n      # if the version string is incorrect, we need to exit the workflow\n      - name: Check if pre-release\n        if: inputs.pre_release == 'true'\n        run: |\n          version=$(uv tree | grep 'langflow' | grep -v 'langflow-base' | awk '{print $2}' | sed 's/^v//')\n          if [[ \"${version}\" =~ ^([0-9]+\\.)?([0-9]+\\.)?[0-9]+((a|b|rc|dev|post)([0-9]+))$ ]]; then\n            echo \"Pre-release version detected. Continuing with the release.\"\n          else\n            echo \"Invalid pre-release version detected. Exiting the workflow.\"\n            exit 1\n          fi\n      - name: Check Version\n        id: check-version\n        run: |\n          version=$(uv tree | grep 'langflow' | grep -v 'langflow-base' | awk '{print $2}' | sed 's/^v//')\n          last_released_version=$(curl -s \"https://pypi.org/pypi/langflow/json\" | jq -r '.releases | keys | .[]' | sort -V | tail -n 1)\n          if [ \"$version\" = \"$last_released_version\" ]; then\n            echo \"Version $version is already released. Skipping release.\"\n            exit 1\n          else\n            echo version=$version >> $GITHUB_OUTPUT\n          fi\n      - name: Wait for PyPI Propagation\n        if: needs.release-base.outputs.skipped == 'false'\n        run: sleep 300 # wait for 5 minutes to ensure PyPI propagation\n\n      - name: Build project for distribution\n        run: make build main=true args=\"--no-sources --wheel\"\n      - name: Test CLI\n        run: |\n          uv pip install dist/*.whl\n          uv run python -m langflow run --host 127.0.0.1 --port 7860 --backend-only &\n          SERVER_PID=$!\n          # Wait for the server to start\n          timeout 120 bash -c 'until curl -f http://127.0.0.1:7860/health_check; do sleep 2; done' || (echo \"Server did not start in time\" && kill $SERVER_PID && exit 1)\n          # Terminate the server\n          kill $SERVER_PID || (echo \"Failed to terminate the server\" && exit 1)\n          sleep 20 # give the server some time to terminate\n          # Check if the server is still running\n          if kill -0 $SERVER_PID 2>/dev/null; then\n            echo \"Failed to terminate the server\"\n            exit 0\n          else\n            echo \"Server terminated successfully\"\n          fi\n      - name: Publish to PyPI\n        env:\n          UV_PUBLISH_TOKEN: ${{ secrets.PYPI_API_TOKEN }}\n        run: |\n          make publish main=true\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: dist-main\n          path: dist\n\n  call_docker_build_base:\n    name: Call Docker Build Workflow for Langflow Base\n    if: inputs.build_docker_base == true\n    needs: [release-base, release-main]\n    uses: ./.github/workflows/docker-build.yml\n    with:\n      base_version: ${{ needs.release-base.outputs.version }}\n      main_version: ${{ needs.release-main.outputs.version }}\n      release_type: base\n      pre_release: ${{ inputs.pre_release }}\n    secrets: inherit\n\n  call_docker_build_main:\n    name: Call Docker Build Workflow for Langflow\n    if: inputs.build_docker_main == true\n    needs: [release-main]\n    uses: ./.github/workflows/docker-build.yml\n    with:\n      main_version: ${{ needs.release-main.outputs.version }}\n      release_type: main\n      pre_release: ${{ inputs.pre_release }}\n    secrets: inherit\n\n  call_docker_build_main_ep:\n    name: Call Docker Build Workflow for Langflow with Entrypoint\n    if: inputs.build_docker_ep == true\n    needs: [release-main]\n    uses: ./.github/workflows/docker-build.yml\n    with:\n      main_version: ${{ needs.release-main.outputs.version }}\n      release_type: main-ep\n      pre_release: False\n    secrets: inherit\n\n  create_release:\n    name: Create Release\n    runs-on: ubuntu-latest\n    needs: release-main\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: dist-main\n          path: dist\n      - name: Create Release\n        uses: ncipollo/release-action@v1\n        with:\n          artifacts: \"dist/*\"\n          token: ${{ secrets.GITHUB_TOKEN }}\n          draft: false\n          generateReleaseNotes: true\n          prerelease: ${{ inputs.pre_release }}\n          tag: ${{ needs.release-main.outputs.version }}\n          commit: ${{ github.ref }}\n",
    "source": "GenuineArt/langflow-ai",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/GenuineArt/langflow-ai/blob/e47639af93b6ed8b940196d65b826eca0b316f24/.github/workflows/release.yml",
    "retrieved_at": "2025-08-30T01:35:43.079447Z"
  },
  {
    "question": "Does this workflow run tests on every push and pull request?",
    "answer": "name: Run Tests\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - uses: actions/setup-go@v2\n      with:\n        go-version: '^1.16.3'\n    - uses: supercharge/redis-github-action@1.2.0\n      with:\n        redis-version: 6\n    - run: go test -v -race ./\n",
    "source": "microsoft/redplex",
    "path": ".github/workflows/validate.yml",
    "url": "https://github.com/microsoft/redplex/blob/248ac9a6adfc13bb2da2404bea767dde69dc0272/.github/workflows/validate.yml",
    "retrieved_at": "2025-08-31T01:45:25.313292Z"
  },
  {
    "question": "How does the `auto_assign.yml` file configure the assignment of the Project Lead as a reviewer for pull requests?",
    "answer": "# This file is related to assigning the Project Lead for the PRs\n\nname: 'Add Project Lead as Reviewer'\non: pull_request\n\njobs:\n  add-reviews:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: kentaro-m/auto-assign-action@v1.1.2\n        with:\n          configuration-path: \".github/auto_assign.yml\"\n",
    "source": "KamalDGRT/yii2-portfolio",
    "path": ".github/workflows/assign_pr.yml",
    "url": "https://github.com/KamalDGRT/yii2-portfolio/blob/5671dcf57fcc4d3451520d36f155dc8263ffc35f/.github/workflows/assign_pr.yml",
    "retrieved_at": "2025-08-31T01:45:26.262798Z"
  },
  {
    "question": "What specific tests are executed by `run_tests.py` in the `scripts/west_commands/` directory?",
    "answer": "# Copyright (c) 2020 Linaro Limited.\n# SPDX-License-Identifier: Apache-2.0\n\nname: Zephyr West Command Tests\n\non:\n  push:\n    paths:\n    - 'scripts/west-commands.yml'\n    - 'scripts/west_commands/**'\n    - '.github/workflows/west_cmds.yml'\n  pull_request:\n    paths:\n    - 'scripts/west-commands.yml'\n    - 'scripts/west_commands/**'\n    - '.github/workflows/west_cmds.yml'\n\njobs:\n  west-commnads:\n    name: West Command Tests\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8]\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        exclude:\n          - os: macos-latest\n            python-version: 3.6\n          - os: windows-latest\n            python-version: 3.6\n    steps:\n    - name: checkout\n      uses: actions/checkout@v2\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v1\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: cache-pip-linux\n      if: startsWith(runner.os, 'Linux')\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip\n        key: ${{ runner.os }}-pip-${{ matrix.python-version }}\n        restore-keys: |\n          ${{ runner.os }}-pip-${{ matrix.python-version }}\n    - name: cache-pip-mac\n      if: startsWith(runner.os, 'macOS')\n      uses: actions/cache@v1\n      with:\n        path: ~/Library/Caches/pip\n        # Trailing '-' was just to get a different cache name\n        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-\n        restore-keys: |\n          ${{ runner.os }}-pip-${{ matrix.python-version }}-\n    - name: cache-pip-win\n      if: startsWith(runner.os, 'Windows')\n      uses: actions/cache@v1\n      with:\n        path: ~\\AppData\\Local\\pip\\Cache\n        key: ${{ runner.os }}-pip-${{ matrix.python-version }}\n        restore-keys: |\n          ${{ runner.os }}-pip-${{ matrix.python-version }}\n    - name: install pytest\n      run: |\n        pip3 install wheel\n        pip3 install pytest west pyelftools canopen progress mypy intelhex psutil\n    - name: run pytest-win\n      if: runner.os == 'Windows'\n      run: |\n        python ./scripts/west_commands/run_tests.py\n    - name: run pytest-mac-linux\n      if: runner.os != 'Windows'\n      run: |\n        ./scripts/west_commands/run_tests.py\n",
    "source": "GPE-Sistemas/zephyr-ncs-gpe",
    "path": ".github/workflows/west_cmds.yml",
    "url": "https://github.com/GPE-Sistemas/zephyr-ncs-gpe/blob/fe0c5d10e02de3083a4044aa51a97144e749d144/.github/workflows/west_cmds.yml",
    "retrieved_at": "2025-08-31T01:45:27.145855Z"
  },
  {
    "question": "What repository characteristics are being validated by the repolinter using the specified ruleset?",
    "answer": "# SPDX-License-Identifier: Apache-2.0\n# Hyperledger Repolinter Action\n\nname: Repolinter\n\non:\n  workflow_dispatch:\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    container: ghcr.io/todogroup/repolinter:v0.10.1\n    steps:\n      - name: Checkout Code\n        uses: actions/checkout@v4\n      - name: Lint Repo\n        continue-on-error: true\n        run: bundle exec /app/bin/repolinter.js --rulesetUrl https://raw.githubusercontent.com/hyperledger-labs/hyperledger-community-management-tools/master/repo_structure/repolint.json --format markdown | tee /repolinter-report.md\n      - name: Save repolinter-report file\n        uses: actions/upload-artifact@v3\n        with:\n          name: repolinter-report\n          path: /repolinter-report.md\n",
    "source": "NJITBlockchainLab/bifold",
    "path": ".github/workflows/repolinter.yml",
    "url": "https://github.com/NJITBlockchainLab/bifold/blob/c8d91286782825cbb4c32f80305b443b37b46168/.github/workflows/repolinter.yml",
    "retrieved_at": "2025-08-31T01:45:27.900132Z"
  },
  {
    "question": "Which integration tests are allowed to fail, and why are they configured as such?",
    "answer": "name: integration\non: [push, pull_request]\n\njobs:\n  integration-tests:\n    runs-on: ubuntu-latest\n    name: ${{ matrix.integration }}\n    strategy:\n      # https://help.github.com/en/actions/getting-started-with-github-actions/about-github-actions#usage-limits\n      # There's a limit of 60 concurrent jobs across all repos in the rust-lang organization.\n      # In order to prevent overusing too much of that 60 limit, we throttle the\n      # number of rustfmt jobs that will run concurrently.\n      max-parallel: 4\n      fail-fast: false\n      matrix:\n        integration: [\n          bitflags,\n          error-chain,\n          log,\n          mdbook,\n          packed_simd,\n          rust-semverver,\n          tempdir,\n          futures-rs,\n          rust-clippy,\n          failure,\n        ]\n        include:\n          # Allowed Failures\n          # Actions doesn't yet support explicitly marking matrix legs as allowed failures\n          # https://github.community/t5/GitHub-Actions/continue-on-error-allow-failure-UI-indication/td-p/37033\n          # https://github.community/t5/GitHub-Actions/Why-a-matrix-step-will-be-canceled-if-another-one-failed/td-p/30920\n          # Instead, leverage `continue-on-error`\n          # https://help.github.com/en/actions/automating-your-workflow-with-github-actions/workflow-syntax-for-github-actions#jobsjob_idstepscontinue-on-error\n          #\n          # Failing due to breaking changes in rustfmt 2.0 where empty\n          # match blocks have trailing commas removed\n          # https://github.com/rust-lang/rustfmt/pull/4226\n          - integration: chalk\n            allow-failure: true\n          - integration: crater\n            allow-failure: true\n          - integration: glob\n            allow-failure: true\n          - integration: stdsimd\n            allow-failure: true\n          # Using old rustfmt configuration option\n          - integration: rand\n            allow-failure: true\n          # Keep this as an allowed failure as it's fragile to breaking changes of rustc.\n          - integration: rust-clippy\n            allow-failure: true\n          # Using old rustfmt configuration option\n          - integration: packed_simd\n            allow-failure: true\n          # calebcartwright (2019-12-24)\n          # Keeping this as an allowed failure since it was flagged as such in the TravisCI config, even though\n          # it appears to have been passing for quite some time.\n          # Original comment was: temporal build failure due to breaking changes in the nightly compiler\n          - integration: rust-semverver\n            allow-failure: true\n          # Can be moved back to include section after https://github.com/rust-lang-nursery/failure/pull/298 is merged\n          - integration: failure\n            allow-failure: true\n\n    steps:\n    - name: checkout\n      uses: actions/checkout@v2\n\n      # Run build\n    - name: setup\n      uses: actions-rs/toolchain@v1\n      with:\n        toolchain: nightly-x86_64-unknown-linux-gnu\n        target: x86_64-unknown-linux-gnu\n        override: true\n        profile: minimal\n        default: true\n    - name: run integration tests\n      env:\n        INTEGRATION: ${{ matrix.integration }}\n        TARGET: x86_64-unknown-linux-gnu\n      run: ./ci/integration.sh\n      continue-on-error: ${{ matrix.allow-failure == true }}\n",
    "source": "BoredApe8461/rustfmt",
    "path": ".github/workflows/integration.yml",
    "url": "https://github.com/BoredApe8461/rustfmt/blob/fe11316f3c152fd7622b1978be1d8a9be93d03d0/.github/workflows/integration.yml",
    "retrieved_at": "2025-08-31T01:45:28.903446Z"
  },
  {
    "question": "What CSL files and locales are being updated and where are they being copied to within the repository?",
    "answer": "name: Refresh Citation Style Language Files\n\non:\n  schedule:\n    # run on 1st and 15th of each month\n    - cron: '1 2 1,15 * *'\n  workflow_dispatch:\n\njobs:\n  publish:\n    name: Refresh Citation Style Language Files\n    runs-on: ubuntu-latest\n    if: github.repository == 'JabRef/jabref'\n    steps:\n      - name: Checkout source\n        uses: actions/checkout@v2\n        with:\n          ref: main\n          fetch-depth: 0\n      - name: Initialize git\n        run: |\n          git checkout main\n          git config --local core.editor /usr/bin/cat\n          git config user.name \"github actions\"\n          git config user.email \"jabrefmail+webfeedback@gmail.com\"\n      - name: Add csl-styles remote\n        run: git remote add -f csl-styles https://github.com/citation-style-language/styles.git\n      - name: Update csl-styles\n        run: |\n          git subtree pull --prefix buildres/csl/csl-styles csl-styles master --squash || true\n          cp buildres/csl/csl-styles/acm-siggraph.csl src/main/resources/csl-styles/\n          cp buildres/csl/csl-styles/ieee.csl src/main/resources/csl-styles/\n          cp buildres/csl/csl-styles/turabian-author-date.csl src/main/resources/csl-styles/\n          git add .\n          git commit -m\"Refresh example styles\" || true\n      - name: Add csl-locales remote\n        run: git remote add -f csl-locales https://github.com/citation-style-language/locales.git\n      - name: Update csl-locales\n        run: |\n          git subtree pull --prefix buildres/csl/csl-locales csl-locales master --squash || true\n          cp buildres/csl/csl-locales/locales.json src/main/resources/csl-locales/\n          cp buildres/csl/csl-locales/locales-en-US.xml src/main/resources/csl-locales/\n          git add .\n          git commit -m\"Refresh example styles\" || true\n      - uses: peter-evans/create-pull-request@v3\n        with:\n          token: ${{ secrets.GH_TOKEN_UPDATE_GRADLE_WRAPPER }}\n          branch: refresh-csl\n          commit-message: Update CSL styles\n          title: \"[Bot] Update CSL styles\"\n          labels: dependencies\n",
    "source": "tjfernandes/SE2122_57464_58763_57677_58125_63764",
    "path": ".github/workflows/refresh-csl-subtrees.yml",
    "url": "https://github.com/tjfernandes/SE2122_57464_58763_57677_58125_63764/blob/545d42658484a4315751ecac830c3da4f194fc25/.github/workflows/refresh-csl-subtrees.yml",
    "retrieved_at": "2025-08-31T01:45:29.896981Z"
  },
  {
    "question": "Under what conditions are the `ubuntu`, `mac`, `windows`, `i386`, and `valgrind` jobs executed based on the `github.ref_name`?",
    "answer": "name: build\non: [push, pull_request]\njobs:\n  ubuntu:\n    runs-on: ${{ matrix.os }}\n    if: ${{ !startsWith(github.ref_name, 'mac') && !startsWith(github.ref_name, 'windows') }}\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - postgres: 18\n            os: ubuntu-24.04\n          - postgres: 17\n            os: ubuntu-24.04\n          - postgres: 16\n            os: ubuntu-22.04\n          - postgres: 15\n            os: ubuntu-22.04\n          - postgres: 14\n            os: ubuntu-20.04\n          - postgres: 13\n            os: ubuntu-20.04\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ankane/setup-postgres@v1\n        with:\n          postgres-version: ${{ matrix.postgres }}\n          dev-files: true\n      - run: make\n        env:\n          PG_CFLAGS: -DUSE_ASSERT_CHECKING -Wall -Wextra -Werror -Wno-unused-parameter -Wno-sign-compare\n      - run: |\n          export PG_CONFIG=`which pg_config`\n          sudo --preserve-env=PG_CONFIG make install\n      - run: make installcheck\n      - if: ${{ failure() }}\n        run: cat regression.diffs\n      - run: |\n          sudo apt-get update\n          sudo apt-get install libipc-run-perl\n      - run: make prove_installcheck\n  mac:\n    runs-on: ${{ matrix.os }}\n    if: ${{ !startsWith(github.ref_name, 'windows') }}\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - postgres: 16\n            os: macos-14\n          - postgres: 14\n            os: macos-13\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ankane/setup-postgres@v1\n        with:\n          postgres-version: ${{ matrix.postgres }}\n      - run: make\n        env:\n          PG_CFLAGS: -DUSE_ASSERT_CHECKING -Wall -Wextra -Werror -Wno-unused-parameter\n      - run: make install\n      - run: make installcheck\n      - if: ${{ failure() }}\n        run: cat regression.diffs\n      # Homebrew Postgres does not enable TAP tests, so need to download\n      - run: |\n          brew install cpanm\n          cpanm --notest IPC::Run\n          wget -q https://github.com/postgres/postgres/archive/refs/tags/$TAG.tar.gz\n          tar xf $TAG.tar.gz\n          mv postgres-$TAG postgres\n        env:\n          TAG: ${{ matrix.postgres == 16 && 'REL_16_2' || 'REL_14_11' }}\n      - run: make prove_installcheck PROVE_FLAGS=\"-I ./postgres/src/test/perl -I ./test/perl\"\n        env:\n          PERL5LIB: /Users/runner/perl5/lib/perl5\n      - run: make clean && $(brew --prefix llvm@15)/bin/scan-build --status-bugs make\n        env:\n          PG_CFLAGS: -DUSE_ASSERT_CHECKING\n  windows:\n    runs-on: windows-latest\n    if: ${{ !startsWith(github.ref_name, 'mac') }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ankane/setup-postgres@v1\n        with:\n          postgres-version: 14\n      - run: |\n          call \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Auxiliary\\Build\\vcvars64.bat\" && ^\n          nmake /NOLOGO /F Makefile.win && ^\n          nmake /NOLOGO /F Makefile.win install && ^\n          nmake /NOLOGO /F Makefile.win installcheck && ^\n          nmake /NOLOGO /F Makefile.win clean && ^\n          nmake /NOLOGO /F Makefile.win uninstall\n        shell: cmd\n      - if: ${{ failure() }}\n        run: cat regression.diffs\n  i386:\n    if: ${{ !startsWith(github.ref_name, 'mac') && !startsWith(github.ref_name, 'windows') }}\n    runs-on: ubuntu-latest\n    container:\n      image: debian:12\n      options: --platform linux/386\n    steps:\n      - run: apt-get update && apt-get install -y build-essential git libipc-run-perl postgresql-15 postgresql-server-dev-15 sudo\n      - run: service postgresql start\n      - run: |\n          git clone https://github.com/${{ github.repository }}.git pgvector\n          cd pgvector\n          git fetch origin ${{ github.ref }}\n          git reset --hard FETCH_HEAD\n          make\n          make install\n          chown -R postgres .\n          sudo -u postgres make installcheck\n          sudo -u postgres make prove_installcheck\n        env:\n          PG_CFLAGS: -DUSE_ASSERT_CHECKING -Wall -Wextra -Werror -Wno-unused-parameter -Wno-sign-compare\n      - if: ${{ failure() }}\n        run: cat pgvector/regression.diffs\n  valgrind:\n    if: ${{ !startsWith(github.ref_name, 'mac') && !startsWith(github.ref_name, 'windows') }}\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: ankane/setup-postgres-valgrind@v1\n        with:\n          postgres-version: 16\n          check-ub: yes\n      - run: make OPTFLAGS=\"\"\n      - run: sudo --preserve-env=PG_CONFIG make install\n      - run: make installcheck\n",
    "source": "manuhg/pgvector",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/manuhg/pgvector/blob/bcafa08e25c5f476afbc3f73ac3d31c486b3687b/.github/workflows/build.yml",
    "retrieved_at": "2025-08-31T01:45:30.897099Z"
  },
  {
    "question": "Under what conditions does the Chromatic job run, considering pull request reviews and labels?",
    "answer": "name: Chromatic\n\non:\n  workflow_dispatch:\n  pull_request_review:\n    types: [submitted]\n    branches:\n      - 'master'\n    paths:\n      - packages/design-system/**\n      - .github/workflows/chromatic.yml\n\nconcurrency:\n  group: chromatic-${{ github.event.pull_request.number || github.ref }}\n  cancel-in-progress: true\n\njobs:\n  chromatic:\n    if: ${{ github.event.review.state == 'approved' && !contains(github.event.pull_request.labels.*.name, 'community') }}\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4.1.1\n        with:\n          fetch-depth: 0\n      - run: corepack enable\n      - uses: actions/setup-node@v4.0.2\n        with:\n          node-version: 20.x\n          cache: 'pnpm'\n      - run: pnpm install --frozen-lockfile\n\n      - name: Publish to Chromatic\n        uses: chromaui/action@v11\n        id: chromatic_tests\n        continue-on-error: true\n        with:\n          workingDir: packages/design-system\n          projectToken: ${{ secrets.CHROMATIC_PROJECT_TOKEN }}\n          exitZeroOnChanges: false\n\n      - name: Success comment\n        if: steps.chromatic_tests.outcome == 'success'\n        uses: peter-evans/create-or-update-comment@v4.0.0\n        with:\n          issue-number: ${{ github.event.pull_request.number }}\n          token: ${{ secrets.GITHUB_TOKEN }}\n          edit-mode: replace\n          body: |\n            :white_check_mark: No visual regressions found.\n\n      - name: Fail comment\n        if: steps.chromatic_tests.outcome != 'success'\n        uses: peter-evans/create-or-update-comment@v4.0.0\n        with:\n          issue-number: ${{ github.event.pull_request.number }}\n          token: ${{ secrets.GITHUB_TOKEN }}\n          edit-mode: replace\n          body: |\n            [:warning: Visual regressions found](${{steps.chromatic_tests.outputs.url}}): ${{steps.chromatic_tests.outputs.changeCount}}\n",
    "source": "wsdevv/n8n-clockify-workaround",
    "path": ".github/workflows/chromatic.yml",
    "url": "https://github.com/wsdevv/n8n-clockify-workaround/blob/a77b9bf3b3b616a908320a2002d7af886f760882/.github/workflows/chromatic.yml",
    "retrieved_at": "2025-08-31T01:45:31.720393Z"
  },
  {
    "question": "How does the workflow handle the potential impact of `ccache` on Windows builds, given the comments about its possible \"strange behavior?\"",
    "answer": "name: CMake Build Matrix\n\non: [push, pull_request]\n\nenv:\n  CMAKE_VERSION: 3.21.1\n  NINJA_VERSION: 1.10.2\n  BUILD_TYPE: Release\n\njobs:\n  build:\n    name: ${{ matrix.config.name }}\n    runs-on: ${{ matrix.config.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        config:\n          - {\n              name: \"Windows Latest MSVC\",\n              os: windows-latest,\n              artifact: \"Windows-MSVC.7z\",\n              build_type: \"Release\",\n              cc: \"cl\",\n              cxx: \"cl\",\n              environment_script: \"C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Auxiliary/Build/vcvars64.bat\",\n              archiver: \"7z a\",\n              generators: \"Visual Studio 17 2022\",\n            }\n          - {\n              name: \"Ubuntu_GCC_10\",\n              os: ubuntu-latest,\n              artifact: \"Linux.7z\",\n              build_type: \"Release\",\n              cc: \"gcc-10\",\n              cxx: \"g++-10\",\n              archiver: \"7z a\",\n              generators: \"Ninja\",\n            }\n          - {\n              name: \"Ubuntu_GCC_11\",\n              os: ubuntu-latest,\n              artifact: \"Linux-GCC-11.7z\",\n              build_type: \"Release\",\n              cc: \"gcc\",\n              cxx: \"g++\",\n              archiver: \"7z a\",\n              generators: \"Ninja\",\n            }\n          - {\n              name: \"macOS Latest Clang\",\n              os: macos-latest,\n              artifact: \"macOS.7z\",\n              build_type: \"Release\",\n              cc: \"clang\",\n              cxx: \"clang++\",\n              archiver: \"7za a\",\n              generators: \"Ninja\",\n            }\n\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          submodules: recursive\n\n      - name: Print env\n        run: |\n          echo github.event.action: ${{ github.event.action }}\n          echo github.event_name: ${{ github.event_name }}\n\n      - name: Download Ninja and CMake\n        shell: cmake -P {0}\n        run: |\n          set(cmake_version $ENV{CMAKE_VERSION})\n          set(ninja_version $ENV{NINJA_VERSION})\n\n          message(STATUS \"Using host CMake version: ${CMAKE_VERSION}\")\n\n          if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n            set(ninja_suffix \"win.zip\")\n            set(cmake_suffix \"windows-x86_64.zip\")\n            set(cmake_dir \"cmake-${cmake_version}-windows-x86_64/bin\")\n          elseif (\"${{ runner.os }}\" STREQUAL \"Linux\")\n            set(ninja_suffix \"linux.zip\")\n            set(cmake_suffix \"linux-x86_64.tar.gz\")\n            set(cmake_dir \"cmake-${cmake_version}-linux-x86_64/bin\")\n          elseif (\"${{ runner.os }}\" STREQUAL \"macOS\")\n            set(ninja_suffix \"mac.zip\")\n            set(cmake_suffix \"macos-universal.tar.gz\")\n            set(cmake_dir \"cmake-${cmake_version}-macos-universal/CMake.app/Contents/bin\")\n          endif()\n\n          set(ninja_url \"https://github.com/ninja-build/ninja/releases/download/v${ninja_version}/ninja-${ninja_suffix}\")\n          file(DOWNLOAD \"${ninja_url}\" ./ninja.zip SHOW_PROGRESS)\n          execute_process(COMMAND ${CMAKE_COMMAND} -E tar xvf ./ninja.zip)\n\n          set(cmake_url \"https://github.com/Kitware/CMake/releases/download/v${cmake_version}/cmake-${cmake_version}-${cmake_suffix}\")\n          file(DOWNLOAD \"${cmake_url}\" ./cmake.zip SHOW_PROGRESS)\n          execute_process(COMMAND ${CMAKE_COMMAND} -E tar xvf ./cmake.zip)\n\n          # Add to PATH environment variable\n          file(TO_CMAKE_PATH \"$ENV{GITHUB_WORKSPACE}/${cmake_dir}\" cmake_dir)\n          set(path_separator \":\")\n          if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n            set(path_separator \";\")\n          endif()\n          file(APPEND \"$ENV{GITHUB_PATH}\" \"$ENV{GITHUB_WORKSPACE}${path_separator}${cmake_dir}\")\n\n          if (NOT \"${{ runner.os }}\" STREQUAL \"Windows\")\n            execute_process(\n              COMMAND chmod +x ninja\n              COMMAND chmod +x ${cmake_dir}/cmake\n            )\n          endif()\n\n      - name: Install gcc-11\n        shell: bash\n        if: endsWith(matrix.config.name, 'GCC_11')\n        run: |\n          sudo apt-get update\n          sudo apt-get install gcc-11 g++-11\n          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 110 --slave /usr/bin/g++ g++ /usr/bin/g++-11 --slave /usr/bin/gcov gcov /usr/bin/gcov-11\n\n      - name: Install ccache\n        shell: cmake -P {0}\n        run: |\n          if(\"${{ runner.os }}\" STREQUAL \"Windows\")\n            # If ccache behaves badly on windows, skip this step\n            execute_process(COMMAND choco install ccache)\n          elseif(\"${{ runner.os }}\" STREQUAL \"macOS\")\n            execute_process(COMMAND brew install ccache)\n          elseif(\"${{ runner.os }}\" STREQUAL \"Linux\")\n            set(ccache_version \"4.6.3\")\n            set(ccache_dist \"ccache-${ccache_version}-linux-x86_64\")\n            set(ccache_url \"https://github.com/ccache/ccache/releases/download/v${ccache_version}/${ccache_dist}.tar.xz\")\n            file(DOWNLOAD \"${ccache_url}\" ./ccache.tar.xz SHOW_PROGRESS)\n            execute_process(COMMAND ${CMAKE_COMMAND} -E tar zxvf ./ccache.tar.xz)\n            # Add to PATH environment variable\n            file(TO_CMAKE_PATH \"$ENV{GITHUB_WORKSPACE}/${ccache_dist}\" ccache_dir)\n            set(path_separator \":\")\n            file(APPEND \"$ENV{GITHUB_PATH}\" \"$ENV{GITHUB_WORKSPACE}${path_separator}${ccache_dir}\")\n          else()\n            message(FATAL_ERROR, \"${{ runner.os }} is not supported\")\n          endif()\n\n      - name: Setup ccache\n        # If ccache behaves badly on windows, skip this step\n        # if: runner.os != 'Windows'\n        uses: Chocobo1/setup-ccache-action@v1\n        with:\n          install_ccache: false\n          update_packager_index: false\n          prepend_symlinks_to_path: false\n          windows_compile_environment: msvc # this field is required\n\n      - name: Configure\n        shell: cmake -P {0}\n        run: |\n          set(ENV{CC} ${{ matrix.config.cc }})\n          set(ENV{CXX} ${{ matrix.config.cxx }})\n\n          if (\"${{ runner.os }}\" STREQUAL \"Windows\" AND NOT \"x${{ matrix.config.environment_script }}\" STREQUAL \"x\")\n            execute_process(\n              COMMAND \"${{ matrix.config.environment_script }}\" && set\n              OUTPUT_FILE environment_script_output.txt\n            )\n            file(STRINGS environment_script_output.txt output_lines)\n            foreach(line IN LISTS output_lines)\n              if (line MATCHES \"^([a-zA-Z0-9_-]+)=(.*)$\")\n                set(ENV{${CMAKE_MATCH_1}} \"${CMAKE_MATCH_2}\")\n              endif()\n            endforeach()\n          endif()\n\n          set(path_separator \":\")\n          if (\"${{ runner.os }}\" STREQUAL \"Windows\")\n            set(path_separator \";\")\n          endif()\n          set(ENV{PATH} \"$ENV{GITHUB_WORKSPACE}${path_separator}$ENV{PATH}\")\n\n          # If ccache shows some strange behavior on windows, you can easily\n          # disable it here by setting the variable to \"OFF\"\n          if (NOT \"${{ runner.os }}\" STREQUAL \"Windows\")\n            set(enable_ccache \"ON\")\n          else()\n            set(enable_ccache \"ON\")\n          endif()\n\n          execute_process(\n            COMMAND cmake\n              -S .\n              -B build\n              -D CMAKE_BUILD_TYPE=$ENV{BUILD_TYPE}\n              -G Ninja\n              -D USE_CCACHE=${enable_ccache}\n              -D CMAKE_MAKE_PROGRAM=ninja\n              -D ASAP_BUILD_TESTS=ON\n              -D ASAP_BUILD_EXAMPLES=ON\n              -D CMAKE_INSTALL_PREFIX=install\n              -D CMAKE_VERBOSE_MAKEFILE=ON\n            RESULT_VARIABLE result\n          )\n          if (NOT result EQUAL 0)\n            message(FATAL_ERROR \"Bad exit status\")\n          endif()\n\n      - name: Build\n        shell: cmake -P {0}\n        run: |\n          set(ENV{NINJA_STATUS} \"[%f/%t %o/sec] \")\n\n          if (\"${{ runner.os }}\" STREQUAL \"Windows\" AND NOT \"x${{ matrix.config.environment_script }}\" STREQUAL \"x\")\n            file(STRINGS environment_script_output.txt output_lines)\n            foreach(line IN LISTS output_lines)\n              if (line MATCHES \"^([a-zA-Z0-9_-]+)=(.*)$\")\n                set(ENV{${CMAKE_MATCH_1}} \"${CMAKE_MATCH_2}\")\n              endif()\n            endforeach()\n          endif()\n\n          execute_process(\n            COMMAND cmake --build build --target all\n            RESULT_VARIABLE result\n            OUTPUT_VARIABLE output\n            ERROR_VARIABLE output\n            ECHO_OUTPUT_VARIABLE ECHO_ERROR_VARIABLE\n          )\n          if (NOT result EQUAL 0)\n            string(REGEX MATCH \"FAILED:.*$\" error_message \"${output}\")\n            string(REPLACE \"\\n\" \"%0A\" error_message \"${error_message}\")\n            message(\"::error::${error_message}\")\n            message(FATAL_ERROR \"Build failed\")\n          endif()\n\n      - name: Run tests\n        shell: cmake -P {0}\n        run: |\n          include(ProcessorCount)\n          ProcessorCount(N)\n\n          set(ENV{CTEST_OUTPUT_ON_FAILURE} \"ON\")\n\n          execute_process(\n            COMMAND ctest -j ${N}\n            WORKING_DIRECTORY build\n            RESULT_VARIABLE result\n            OUTPUT_VARIABLE output\n            ERROR_VARIABLE output\n            ECHO_OUTPUT_VARIABLE ECHO_ERROR_VARIABLE\n          )\n          if (NOT result EQUAL 0)\n            string(REGEX MATCH \"[0-9]+% tests.*[0-9.]+ sec.*$\" test_results \"${output}\")\n            string(REPLACE \"\\n\" \"%0A\" test_results \"${test_results}\")\n            message(\"::error::${test_results}\")\n            message(FATAL_ERROR \"Running tests failed!\")\n          endif()\n\n      - name: Install Strip\n        run: cmake --install build --strip\n\n      - name: Pack\n        working-directory: install\n        run: cmake -E tar cfv ../${{ matrix.config.artifact }} --format=7zip .\n\n      - name: Upload\n        uses: actions/upload-artifact@v1\n        with:\n          path: ./${{ matrix.config.artifact }}\n          name: ${{ matrix.config.artifact }}\n\n  release:\n    if: contains(github.ref, 'tags/v')\n    runs-on: ubuntu-latest\n    needs: build\n\n    steps:\n      - name: Create Release\n        id: create_release\n        uses: actions/create-release@v1.0.0\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          tag_name: ${{ github.ref }}\n          release_name: Release ${{ github.ref }}\n          draft: false\n          prerelease: false\n\n      - name: Store Release url\n        run: |\n          echo \"${{ steps.create_release.outputs.upload_url }}\" > ./upload_url\n\n      - uses: actions/upload-artifact@v1\n        with:\n          path: ./upload_url\n          name: upload_url\n\n  publish:\n    if: contains(github.ref, 'tags/v')\n    name: ${{ matrix.config.name }}\n    runs-on: ${{ matrix.config.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        config:\n          - {\n              name: \"Windows Latest MSVC\",\n              artifact: \"Windows-MSVC.7z\",\n              os: windows-latest,\n            }\n          - {\n              name: \"Ubuntu Latest GCC\",\n              artifact: \"Linux.7z\",\n              os: ubuntu-latest,\n            }\n          - {\n              name: \"macOS Latest Clang\",\n              artifact: \"macOS.7z\",\n              os: macos-latest,\n            }\n    needs: release\n\n    steps:\n      - name: Download artifact\n        uses: actions/download-artifact@v1\n        with:\n          name: ${{ matrix.config.artifact }}\n          path: ./\n\n      - name: Download URL\n        uses: actions/download-artifact@v1\n        with:\n          name: upload_url\n          path: ./\n\n      - id: set_upload_url\n        run: |\n          upload_url=`cat ./upload_url`\n          echo ::set-output name=upload_url::$upload_url\n        shell: bash\n\n      - name: Upload to Release\n        id: upload_to_release\n        uses: actions/upload-release-asset@v1.0.1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ steps.set_upload_url.outputs.upload_url }}\n          asset_path: ./${{ matrix.config.artifact }}\n          asset_name: ${{ matrix.config.artifact }}\n          asset_content_type: application/x-gtar\n",
    "source": "yunghegel/opengl_starter",
    "path": ".github/workflows/cmake-build.yml",
    "url": "https://github.com/yunghegel/opengl_starter/blob/78e4a5f89a2f6f8dbe9dc4606caa356445c6a041/.github/workflows/cmake-build.yml",
    "retrieved_at": "2025-08-31T01:45:32.569523Z"
  },
  {
    "question": "Under what conditions does job-3 execute, considering the commented-out `if` statement?",
    "answer": "# Display name of workflow\nname: Chaining Jobs\n\n\n# Controls when the action will run. Workflow runs when manually triggered using the UI or API.\non:\n  workflow_dispatch:\n    # Inputs the workflow accepts.\n    inputs:\n      run-job-3:\n        description: \"Run job 3\"\n        required: true\n        type: boolean\n\njobs:\n\n  job-1:\n    name: Job 1\n    runs-on: ubuntu-latest\n    steps:\n    - name: Output for Job 1\n      run: echo \"Hello from Job 1. Run Job 3 equals ${{ github.event.inputs.run-job-3 }}\" \n\n  job-2:\n    name: Job 2\n    runs-on: ubuntu-latest\n    #needs:\n    #  - job-1\n    steps:\n    - name: Output for Job 2\n      run: echo \"Hello from Job 2\"\n\n  job-3:\n    name: Job 3\n    #if: github.event.inputs.run-job-3 == 'true'\n    runs-on: ubuntu-latest\n    #needs:\n    #  - job-1\n    steps:\n    - name: Output for Job 3\n      run: echo \"Hello from Job 3\"\n\n  job-4:\n    name: Job 4\n    runs-on: ubuntu-latest\n    # if: always()\n    needs:\n      - job-2\n      - job-3\n    steps:\n    - name: Output for Job 4\n      run: echo \"Hello from Job 4\"\n",
    "source": "nampereira/desofs-tp04",
    "path": ".github/workflows/3-chaining.yaml",
    "url": "https://github.com/nampereira/desofs-tp04/blob/a901b8f0160c924cc14bc6013b432e8bed448453/.github/workflows/3-chaining.yaml",
    "retrieved_at": "2025-08-31T01:45:33.504144Z"
  },
  {
    "question": "What triggers this workflow to run, and how does it prevent the GitHub Classroom bot from initiating a run?",
    "answer": "name: Autograding Tests\n'on':\n  - push\n  - workflow_dispatch\n  - repository_dispatch\npermissions:\n  checks: write\n  actions: read\n  contents: read\njobs:\n  run-autograding-tests:\n    runs-on: ubuntu-latest\n    if: github.actor != 'github-classroom[bot]'\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Download mempool\n        uses: GuillaumeFalourd/clone-github-repo-action@v2.3\n        with:\n          owner: 'SummerOfBitcoin'\n          repository: 'code-challenge-2024-mempool'\n      - name: Validate block\n        id: validate-block\n        uses: SummerOfBitcoin/code-challenge-2024-grader@v2.8\n        with:\n          test-name: 'Validate block '\n          command: chmod +x ./run.sh && ./run.sh\n          timeout: 10\n          max-fee: 20616923\n          max-score: 100\n          passing-score: 60\n      - name: Autograding Reporter\n        uses: SummerOfBitcoin/autograding-grading-reporter@v2.2\n        env:\n          VALIDATE-BLOCK_RESULTS: \"${{steps.validate-block.outputs.result}}\"\n        with:\n          runners: validate-block\n",
    "source": "Hugongra/code-challenge-2024-Hugongra",
    "path": ".github/workflows/classroom.yml",
    "url": "https://github.com/Hugongra/code-challenge-2024-Hugongra/blob/493c09fba612c14feb89bfc7e245b81c8be2b541/.github/workflows/classroom.yml",
    "retrieved_at": "2025-09-01T01:54:54.262340Z"
  },
  {
    "question": "Under what conditions will the `release` job, which creates and uploads a release, be triggered?",
    "answer": "name: Release\n\non:\n  workflow_dispatch: # allows manual triggering\n    inputs:\n      create_release:\n        description: 'Create new release'\n        required: true\n        type: boolean\n  push:\n    branches:\n      - master\n    paths: ['.github/workflows/release.yml', '**/CMakeLists.txt', '**/.cmake', '**/*.h', '**/*.hpp', '**/*.c', '**/*.cpp', '**/*.cu', '**/*.cuh', '**/*.swift', '**/*.m', '**/*.metal', '**/*.comp']\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}\n  cancel-in-progress: true\n\nenv:\n  BRANCH_NAME: ${{ github.head_ref || github.ref_name }}\n  CMAKE_ARGS: \"-DLLAMA_BUILD_EXAMPLES=OFF -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_TOOLS=ON -DLLAMA_BUILD_SERVER=ON -DGGML_RPC=ON\"\n\njobs:\n  macOS-arm64:\n    runs-on: macos-14\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: macOS-latest-cmake-arm64\n          evict-old-files: 1d\n\n      - name: Dependencies\n        id: depends\n        continue-on-error: true\n        run: |\n          brew update\n          brew install curl\n\n      - name: Build\n        id: cmake_build\n        run: |\n          sysctl -a\n          cmake -B build \\\n            -DCMAKE_BUILD_RPATH=\"@loader_path\" \\\n            -DLLAMA_FATAL_WARNINGS=ON \\\n            -DGGML_METAL_USE_BF16=ON \\\n            -DGGML_METAL_EMBED_LIBRARY=ON \\\n            -DGGML_RPC=ON \\\n            ${{ env.CMAKE_ARGS }}\n          cmake --build build --config Release -j $(sysctl -n hw.logicalcpu)\n\n      - name: Determine tag name\n        id: tag\n        uses: ./.github/actions/get-tag-name\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        run: |\n          cp LICENSE ./build/bin/\n          zip -r llama-${{ steps.tag.outputs.name }}-bin-macos-arm64.zip ./build/bin/*\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-${{ steps.tag.outputs.name }}-bin-macos-arm64.zip\n          name: llama-bin-macos-arm64.zip\n\n  macOS-x64:\n    runs-on: macos-13\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: macOS-latest-cmake-x64\n          evict-old-files: 1d\n\n      - name: Dependencies\n        id: depends\n        continue-on-error: true\n        run: |\n          brew update\n          brew install curl\n\n      - name: Build\n        id: cmake_build\n        run: |\n          sysctl -a\n          # Metal is disabled due to intermittent failures with Github runners not having a GPU:\n          # https://github.com/ggml-org/llama.cpp/actions/runs/8635935781/job/23674807267#step:5:2313\n          cmake -B build \\\n            -DCMAKE_BUILD_RPATH=\"@loader_path\" \\\n            -DLLAMA_FATAL_WARNINGS=ON \\\n            -DGGML_METAL=OFF \\\n            -DGGML_RPC=ON\n          cmake --build build --config Release -j $(sysctl -n hw.logicalcpu)\n\n      - name: Determine tag name\n        id: tag\n        uses: ./.github/actions/get-tag-name\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        run: |\n          cp LICENSE ./build/bin/\n          zip -r llama-${{ steps.tag.outputs.name }}-bin-macos-x64.zip ./build/bin/*\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-${{ steps.tag.outputs.name }}-bin-macos-x64.zip\n          name: llama-bin-macos-x64.zip\n\n  ubuntu-22-cpu:\n    strategy:\n      matrix:\n        include:\n          - build: 'x64'\n            os: ubuntu-22.04\n          # GGML_BACKEND_DL and GGML_CPU_ALL_VARIANTS are not currently supported on arm\n          # - build: 'arm64'\n          #   os: ubuntu-22.04-arm\n\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: ubuntu-cpu-cmake\n          evict-old-files: 1d\n\n      - name: Dependencies\n        id: depends\n        run: |\n          sudo apt-get update\n          sudo apt-get install build-essential libcurl4-openssl-dev\n\n      - name: Build\n        id: cmake_build\n        run: |\n          cmake -B build \\\n            -DGGML_BACKEND_DL=ON \\\n            -DGGML_NATIVE=OFF \\\n            -DGGML_CPU_ALL_VARIANTS=ON \\\n            -DLLAMA_FATAL_WARNINGS=ON \\\n            ${{ env.CMAKE_ARGS }}\n          cmake --build build --config Release -j $(nproc)\n\n      - name: Determine tag name\n        id: tag\n        uses: ./.github/actions/get-tag-name\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        run: |\n          cp LICENSE ./build/bin/\n          zip -r llama-${{ steps.tag.outputs.name }}-bin-ubuntu-${{ matrix.build }}.zip ./build/bin/*\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-${{ steps.tag.outputs.name }}-bin-ubuntu-${{ matrix.build }}.zip\n          name: llama-bin-ubuntu-${{ matrix.build }}.zip\n\n  ubuntu-22-vulkan:\n    runs-on: ubuntu-22.04\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: ubuntu-22-cmake-vulkan\n          evict-old-files: 1d\n\n      - name: Dependencies\n        id: depends\n        run: |\n          wget -qO - https://packages.lunarg.com/lunarg-signing-key-pub.asc | sudo apt-key add -\n          sudo wget -qO /etc/apt/sources.list.d/lunarg-vulkan-jammy.list https://packages.lunarg.com/vulkan/lunarg-vulkan-jammy.list\n          sudo apt-get update -y\n          sudo apt-get install -y build-essential mesa-vulkan-drivers vulkan-sdk libcurl4-openssl-dev\n\n      - name: Build\n        id: cmake_build\n        run: |\n          cmake -B build \\\n            -DGGML_BACKEND_DL=ON \\\n            -DGGML_NATIVE=OFF \\\n            -DGGML_CPU_ALL_VARIANTS=ON \\\n            -DGGML_VULKAN=ON \\\n            ${{ env.CMAKE_ARGS }}\n          cmake --build build --config Release -j $(nproc)\n\n      - name: Determine tag name\n        id: tag\n        uses: ./.github/actions/get-tag-name\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        run: |\n          cp LICENSE ./build/bin/\n          zip -r llama-${{ steps.tag.outputs.name }}-bin-ubuntu-vulkan-x64.zip ./build/bin/*\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-${{ steps.tag.outputs.name }}-bin-ubuntu-vulkan-x64.zip\n          name: llama-bin-ubuntu-vulkan-x64.zip\n\n  windows-cpu:\n    runs-on: windows-latest\n\n    strategy:\n      matrix:\n        include:\n          - arch: 'x64'\n          - arch: 'arm64'\n\n    steps:\n      - name: Clone\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: windows-latest-cmake-cpu-${{ matrix.arch }}\n          variant: ccache\n          evict-old-files: 1d\n\n      - name: Install Ninja\n        run: |\n          choco install ninja\n\n      - name: libCURL\n        id: get_libcurl\n        uses: ./.github/actions/windows-setup-curl\n        with:\n          architecture: ${{ matrix.arch == 'x64' && 'win64' || 'win64a' }}\n\n      - name: Build\n        shell: cmd\n        env:\n          CURL_PATH: ${{ steps.get_libcurl.outputs.curl_path }}\n        run: |\n          call \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat\" ${{ matrix.arch }}\n          cmake -S . -B build -G \"Ninja Multi-Config\" ^\n            -D CMAKE_TOOLCHAIN_FILE=cmake/${{ matrix.arch }}-windows-llvm.cmake ^\n            -DGGML_NATIVE=OFF ^\n            -DGGML_BACKEND_DL=ON ^\n            -DGGML_CPU_ALL_VARIANTS=${{ matrix.arch == 'x64' && 'ON' || 'OFF' }} ^\n            -DGGML_OPENMP=ON ^\n            -DCURL_LIBRARY=\"%CURL_PATH%/lib/libcurl.dll.a\" -DCURL_INCLUDE_DIR=\"%CURL_PATH%/include\" ^\n            ${{ env.CMAKE_ARGS }}\n          cmake --build build --config Release\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        env:\n          CURL_PATH: ${{ steps.get_libcurl.outputs.curl_path }}\n        run: |\n          Copy-Item $env:CURL_PATH\\bin\\libcurl-${{ matrix.arch }}.dll .\\build\\bin\\Release\\\n          Copy-Item \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Redist\\MSVC\\14.42.34433\\debug_nonredist\\${{ matrix.arch }}\\Microsoft.VC143.OpenMP.LLVM\\libomp140.${{ matrix.arch == 'x64' && 'x86_64' || 'aarch64' }}.dll\" .\\build\\bin\\Release\\\n          7z a llama-bin-win-cpu-${{ matrix.arch }}.zip .\\build\\bin\\Release\\*\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-bin-win-cpu-${{ matrix.arch }}.zip\n          name: llama-bin-win-cpu-${{ matrix.arch }}.zip\n\n  windows:\n    runs-on: windows-latest\n\n    env:\n      OPENBLAS_VERSION: 0.3.23\n      VULKAN_VERSION: 1.4.309.0\n\n    strategy:\n      matrix:\n        include:\n          - backend: 'vulkan'\n            arch: 'x64'\n            defines: '-DGGML_VULKAN=ON'\n            target: 'ggml-vulkan'\n          - backend: 'opencl-adreno'\n            arch: 'arm64'\n            defines: '-G \"Ninja Multi-Config\" -D CMAKE_TOOLCHAIN_FILE=cmake/arm64-windows-llvm.cmake -DCMAKE_PREFIX_PATH=\"$env:RUNNER_TEMP/opencl-arm64-release\" -DGGML_OPENCL=ON -DGGML_OPENCL_USE_ADRENO_KERNELS=ON'\n            target: 'ggml-opencl'\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n\n      - name: ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: windows-latest-cmake-${{ matrix.backend }}-${{ matrix.arch }}\n          variant: ccache\n          evict-old-files: 1d\n\n      - name: Install Vulkan SDK\n        id: get_vulkan\n        if: ${{ matrix.backend == 'vulkan' }}\n        run: |\n          curl.exe -o $env:RUNNER_TEMP/VulkanSDK-Installer.exe -L \"https://sdk.lunarg.com/sdk/download/${env:VULKAN_VERSION}/windows/VulkanSDK-${env:VULKAN_VERSION}-Installer.exe\"\n          & \"$env:RUNNER_TEMP\\VulkanSDK-Installer.exe\" --accept-licenses --default-answer --confirm-command install\n          Add-Content $env:GITHUB_ENV \"VULKAN_SDK=C:\\VulkanSDK\\${env:VULKAN_VERSION}\"\n          Add-Content $env:GITHUB_PATH \"C:\\VulkanSDK\\${env:VULKAN_VERSION}\\bin\"\n\n      - name: Install Ninja\n        id: install_ninja\n        run: |\n          choco install ninja\n\n      - name: Install OpenCL Headers and Libs\n        id: install_opencl\n        if: ${{ matrix.backend == 'opencl-adreno' && matrix.arch == 'arm64' }}\n        run: |\n          git clone https://github.com/KhronosGroup/OpenCL-Headers\n          cd OpenCL-Headers\n          cmake -B build `\n            -DBUILD_TESTING=OFF `\n            -DOPENCL_HEADERS_BUILD_TESTING=OFF `\n            -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF `\n            -DCMAKE_INSTALL_PREFIX=\"$env:RUNNER_TEMP/opencl-arm64-release\"\n          cmake --build build --target install\n          git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader\n          cd OpenCL-ICD-Loader\n          cmake -B build-arm64-release `\n            -A arm64 `\n            -DCMAKE_PREFIX_PATH=\"$env:RUNNER_TEMP/opencl-arm64-release\" `\n            -DCMAKE_INSTALL_PREFIX=\"$env:RUNNER_TEMP/opencl-arm64-release\"\n          cmake --build build-arm64-release --target install --config release\n\n      - name: Build\n        id: cmake_build\n        run: |\n          cmake -S . -B build ${{ matrix.defines }} -DGGML_NATIVE=OFF -DGGML_CPU=OFF -DGGML_BACKEND_DL=ON -DLLAMA_CURL=OFF\n          cmake --build build --config Release --target ${{ matrix.target }}\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        run: |\n          7z a llama-bin-win-${{ matrix.backend }}-${{ matrix.arch }}.zip .\\build\\bin\\Release\\${{ matrix.target }}.dll\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-bin-win-${{ matrix.backend }}-${{ matrix.arch }}.zip\n          name: llama-bin-win-${{ matrix.backend }}-${{ matrix.arch }}.zip\n\n  windows-cuda:\n    runs-on: windows-2022\n\n    strategy:\n      matrix:\n        cuda: ['12.4']\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n\n      - name: Install ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: windows-cuda-${{ matrix.cuda }}\n          variant: ccache\n          evict-old-files: 1d\n\n      - name: Install Cuda Toolkit\n        uses: ./.github/actions/windows-setup-cuda\n        with:\n          cuda_version: ${{ matrix.cuda }}\n\n      - name: Install Ninja\n        id: install_ninja\n        run: |\n          choco install ninja\n\n      - name: Build\n        id: cmake_build\n        shell: cmd\n        run: |\n          call \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\n          cmake -S . -B build -G \"Ninja Multi-Config\" ^\n            -DGGML_BACKEND_DL=ON ^\n            -DGGML_NATIVE=OFF ^\n            -DGGML_CPU=OFF ^\n            -DGGML_CUDA=ON ^\n            -DLLAMA_CURL=OFF\n          set /A NINJA_JOBS=%NUMBER_OF_PROCESSORS%-1\n          cmake --build build --config Release -j %NINJA_JOBS% --target ggml-cuda\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        run: |\n          7z a llama-bin-win-cuda-${{ matrix.cuda }}-x64.zip .\\build\\bin\\Release\\ggml-cuda.dll\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-bin-win-cuda-${{ matrix.cuda }}-x64.zip\n          name: llama-bin-win-cuda-${{ matrix.cuda }}-x64.zip\n\n      - name: Copy and pack Cuda runtime\n        run: |\n          echo \"Cuda install location: ${{ env.CUDA_PATH }}\"\n          $dst='.\\build\\bin\\cudart\\'\n          robocopy \"${{env.CUDA_PATH}}\\bin\" $dst cudart64_*.dll cublas64_*.dll cublasLt64_*.dll\n          robocopy \"${{env.CUDA_PATH}}\\lib\" $dst cudart64_*.dll cublas64_*.dll cublasLt64_*.dll\n          7z a cudart-llama-bin-win-cuda-${{ matrix.cuda }}-x64.zip $dst\\*\n\n      - name: Upload Cuda runtime\n        uses: actions/upload-artifact@v4\n        with:\n          path: cudart-llama-bin-win-cuda-${{ matrix.cuda }}-x64.zip\n          name: cudart-llama-bin-win-cuda-${{ matrix.cuda }}-x64.zip\n\n  windows-sycl:\n    runs-on: windows-latest\n\n    defaults:\n      run:\n        shell: bash\n\n    env:\n      WINDOWS_BASEKIT_URL: https://registrationcenter-download.intel.com/akdlm/IRC_NAS/7cd9bba0-7aab-4e30-b3ae-2221006a4a05/intel-oneapi-base-toolkit-2025.1.1.34_offline.exe\n      WINDOWS_DPCPP_MKL: intel.oneapi.win.cpp-dpcpp-common:intel.oneapi.win.mkl.devel:intel.oneapi.win.dnnl:intel.oneapi.win.tbb.devel\n      ONEAPI_ROOT: \"C:/Program Files (x86)/Intel/oneAPI\"\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n\n      - name: ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: windows-latest-cmake-sycl\n          variant: ccache\n          evict-old-files: 1d\n\n      - name: Install\n        run:  |\n          scripts/install-oneapi.bat $WINDOWS_BASEKIT_URL $WINDOWS_DPCPP_MKL\n\n      - name: Build\n        id: cmake_build\n        shell: cmd\n        run: |\n          call \"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\" intel64 --force\n          cmake -G \"Ninja\" -B build ^\n            -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx ^\n            -DCMAKE_BUILD_TYPE=Release ^\n            -DGGML_BACKEND_DL=ON -DBUILD_SHARED_LIBS=ON ^\n            -DGGML_CPU=OFF -DGGML_SYCL=ON ^\n            -DLLAMA_CURL=OFF\n          cmake --build build --target ggml-sycl -j\n\n      - name: Build the release package\n        id: pack_artifacts\n        run: |\n          echo \"cp oneAPI running time dll files in ${{ env.ONEAPI_ROOT }} to ./build/bin\"\n\n          cp \"${{ env.ONEAPI_ROOT }}/mkl/latest/bin/mkl_sycl_blas.5.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/mkl/latest/bin/mkl_core.2.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/mkl/latest/bin/mkl_tbb_thread.2.dll\" ./build/bin\n\n          cp \"${{ env.ONEAPI_ROOT }}/compiler/latest/bin/ur_adapter_level_zero.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/compiler/latest/bin/ur_adapter_opencl.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/compiler/latest/bin/ur_loader.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/compiler/latest/bin/ur_win_proxy_loader.dll\" ./build/bin\n\n          cp \"${{ env.ONEAPI_ROOT }}/compiler/latest/bin/sycl8.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/compiler/latest/bin/svml_dispmd.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/compiler/latest/bin/libmmd.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/compiler/latest/bin/libiomp5md.dll\" ./build/bin\n\n          cp \"${{ env.ONEAPI_ROOT }}/dnnl/latest/bin/dnnl.dll\" ./build/bin\n          cp \"${{ env.ONEAPI_ROOT }}/tbb/latest/bin/tbb12.dll\" ./build/bin\n\n          echo \"cp oneAPI running time dll files to ./build/bin done\"\n          7z a llama-bin-win-sycl-x64.zip ./build/bin/*\n\n      - name: Upload the release package\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-bin-win-sycl-x64.zip\n          name: llama-bin-win-sycl-x64.zip\n\n  windows-hip:\n    runs-on: windows-latest\n\n    strategy:\n      matrix:\n        include:\n          - name: \"radeon\"\n            gpu_targets: \"gfx1100;gfx1101;gfx1102;gfx1030;gfx1031;gfx1032\"\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n\n      - name: Clone rocWMMA repository\n        id: clone_rocwmma\n        run: |\n          git clone https://github.com/rocm/rocwmma --branch rocm-6.2.4 --depth 1\n\n      - name: ccache\n        uses: hendrikmuhs/ccache-action@v1.2.16\n        with:\n          key: windows-latest-cmake-hip-${{ matrix.name }}-x64\n          evict-old-files: 1d\n\n      - name: Install\n        id: depends\n        run: |\n          $ErrorActionPreference = \"Stop\"\n          write-host \"Downloading AMD HIP SDK Installer\"\n          Invoke-WebRequest -Uri \"https://download.amd.com/developer/eula/rocm-hub/AMD-Software-PRO-Edition-24.Q3-WinSvr2022-For-HIP.exe\" -OutFile \"${env:RUNNER_TEMP}\\rocm-install.exe\"\n          write-host \"Installing AMD HIP SDK\"\n          Start-Process \"${env:RUNNER_TEMP}\\rocm-install.exe\" -ArgumentList '-install' -NoNewWindow -Wait\n          write-host \"Completed AMD HIP SDK installation\"\n\n      - name: Verify ROCm\n        id: verify\n        run: |\n          & 'C:\\Program Files\\AMD\\ROCm\\*\\bin\\clang.exe' --version\n\n      - name: Build\n        id: cmake_build\n        run: |\n          $env:HIP_PATH=$(Resolve-Path 'C:\\Program Files\\AMD\\ROCm\\*\\bin\\clang.exe' | split-path | split-path)\n          $env:CMAKE_PREFIX_PATH=\"${env:HIP_PATH}\"\n          cmake -G \"Unix Makefiles\" -B build -S . `\n            -DCMAKE_C_COMPILER=\"${env:HIP_PATH}\\bin\\clang.exe\" `\n            -DCMAKE_CXX_COMPILER=\"${env:HIP_PATH}\\bin\\clang++.exe\" `\n            -DCMAKE_CXX_FLAGS=\"-I$($PWD.Path.Replace('\\', '/'))/rocwmma/library/include/ -Wno-ignored-attributes -Wno-nested-anon-types\" `\n            -DCMAKE_BUILD_TYPE=Release `\n            -DGGML_BACKEND_DL=ON `\n            -DGGML_NATIVE=OFF `\n            -DGGML_CPU=OFF `\n            -DAMDGPU_TARGETS=\"${{ matrix.gpu_targets }}\" `\n            -DGGML_HIP_ROCWMMA_FATTN=ON `\n            -DGGML_HIP=ON `\n            -DLLAMA_CURL=OFF\n          cmake --build build --target ggml-hip -j ${env:NUMBER_OF_PROCESSORS}\n          md \"build\\bin\\rocblas\\library\\\"\n          cp \"${env:HIP_PATH}\\bin\\hipblas.dll\" \"build\\bin\\\"\n          cp \"${env:HIP_PATH}\\bin\\rocblas.dll\" \"build\\bin\\\"\n          cp \"${env:HIP_PATH}\\bin\\rocblas\\library\\*\" \"build\\bin\\rocblas\\library\\\"\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        run: |\n          7z a llama-bin-win-hip-${{ matrix.name }}-x64.zip .\\build\\bin\\*\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-bin-win-hip-${{ matrix.name }}-x64.zip\n          name: llama-bin-win-hip-${{ matrix.name }}-x64.zip\n\n  ios-xcode-build:\n    runs-on: macos-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Build\n        id: cmake_build\n        run: |\n          sysctl -a\n          cmake -B build -G Xcode \\\n            -DGGML_METAL_USE_BF16=ON \\\n            -DGGML_METAL_EMBED_LIBRARY=ON \\\n            -DLLAMA_CURL=OFF \\\n            -DLLAMA_BUILD_EXAMPLES=OFF \\\n            -DLLAMA_BUILD_TOOLS=OFF \\\n            -DLLAMA_BUILD_TESTS=OFF \\\n            -DLLAMA_BUILD_SERVER=OFF \\\n            -DCMAKE_SYSTEM_NAME=iOS \\\n            -DCMAKE_OSX_DEPLOYMENT_TARGET=14.0 \\\n            -DCMAKE_XCODE_ATTRIBUTE_DEVELOPMENT_TEAM=ggml\n          cmake --build build --config Release -j $(sysctl -n hw.logicalcpu) -- CODE_SIGNING_ALLOWED=NO\n\n      - name: xcodebuild for swift package\n        id: xcodebuild\n        run: |\n          ./build-xcframework.sh\n\n      - name: Build Xcode project\n        run: xcodebuild -project examples/llama.swiftui/llama.swiftui.xcodeproj -scheme llama.swiftui -sdk iphoneos CODE_SIGNING_REQUIRED=NO CODE_SIGN_IDENTITY= -destination 'generic/platform=iOS' FRAMEWORK_FOLDER_PATH=./build-ios build\n\n      - name: Determine tag name\n        id: tag\n        uses: ./.github/actions/get-tag-name\n\n      - name: Pack artifacts\n        id: pack_artifacts\n        run: |\n          zip --symlinks -r llama-${{ steps.tag.outputs.name }}-xcframework.zip build-apple/llama.xcframework\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          path: llama-${{ steps.tag.outputs.name }}-xcframework.zip\n          name: llama-${{ steps.tag.outputs.name }}-xcframework\n\n  release:\n    if: ${{ ( github.event_name == 'push' && github.ref == 'refs/heads/master' ) || github.event.inputs.create_release == 'true' }}\n\n    # Fine-grant permission\n    # https://docs.github.com/en/actions/security-for-github-actions/security-guides/automatic-token-authentication#modifying-the-permissions-for-the-github_token\n    permissions:\n        contents: write # for creating release\n\n    runs-on: ubuntu-latest\n\n    needs:\n      - windows\n      - windows-cpu\n      - windows-cuda\n      - windows-sycl\n      - windows-hip\n      - ubuntu-22-cpu\n      - ubuntu-22-vulkan\n      - macOS-arm64\n      - macOS-x64\n      - ios-xcode-build\n\n    steps:\n      - name: Clone\n        id: checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Determine tag name\n        id: tag\n        uses: ./.github/actions/get-tag-name\n\n      - name: Download artifacts\n        id: download-artifact\n        uses: actions/download-artifact@v4\n        with:\n          path: ./artifact\n          merge-multiple: true\n\n      - name: Move artifacts\n        id: move_artifacts\n        run: |\n          mkdir -p release\n\n          echo \"Adding CPU backend files to existing zips...\"\n          for arch in x64 arm64; do\n            cpu_zip=\"artifact/llama-bin-win-cpu-${arch}.zip\"\n            temp_dir=$(mktemp -d)\n            echo \"Extracting CPU backend for $arch...\"\n            unzip \"$cpu_zip\" -d \"$temp_dir\"\n\n            echo \"Adding CPU files to $arch zips...\"\n            for target_zip in artifact/llama-bin-win-*-${arch}.zip; do\n              if [[ \"$target_zip\" == \"$cpu_zip\" ]]; then\n                continue\n              fi\n              echo \"Adding CPU backend to $(basename \"$target_zip\")\"\n              realpath_target_zip=$(realpath \"$target_zip\")\n              (cd \"$temp_dir\" && zip -r \"$realpath_target_zip\" .)\n            done\n\n            rm -rf \"$temp_dir\"\n          done\n\n          echo \"Renaming and moving zips to release...\"\n          for zip_file in artifact/llama-bin-win-*.zip; do\n            base_name=$(basename \"$zip_file\" .zip)\n            zip_name=\"llama-${{ steps.tag.outputs.name }}-${base_name#llama-}.zip\"\n            echo \"Moving $zip_file to release/$zip_name\"\n            mv \"$zip_file\" \"release/$zip_name\"\n          done\n\n          echo \"Moving other artifacts...\"\n          mv -v artifact/*.zip release\n\n      - name: Create release\n        id: create_release\n        uses: ggml-org/action-create-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          tag_name: ${{ steps.tag.outputs.name }}\n\n      - name: Upload release\n        id: upload_release\n        uses: actions/github-script@v3\n        with:\n          github-token: ${{secrets.GITHUB_TOKEN}}\n          script: |\n            const path = require('path');\n            const fs = require('fs');\n            const release_id = '${{ steps.create_release.outputs.id }}';\n            for (let file of await fs.readdirSync('./release')) {\n              if (path.extname(file) === '.zip') {\n                console.log('uploadReleaseAsset', file);\n                await github.repos.uploadReleaseAsset({\n                  owner: context.repo.owner,\n                  repo: context.repo.repo,\n                  release_id: release_id,\n                  name: file,\n                  data: await fs.readFileSync(`./release/${file}`)\n                });\n              }\n            }\n",
    "source": "SKKU-ESLAB/specinfer.cpp",
    "path": ".github/workflows/release.yml",
    "url": "https://github.com/SKKU-ESLAB/specinfer.cpp/blob/c288e68531c4e64d768c29f77259f571aad1ddb4/.github/workflows/release.yml",
    "retrieved_at": "2025-09-01T01:54:55.531436Z"
  },
  {
    "question": "What operating systems are used to build the project in this workflow?",
    "answer": "name: \"Main / Pull requests build\"\non:\n    pull_request:\n        paths-ignore:\n            - '.txt'\n            - 'LICENSE'\n            - 'docs/**'\n        branches: [ main ]\n    push:\n        branches:\n            - main\n\njobs:\n    build:\n        runs-on: ${{ matrix.os }}\n        strategy:\n            fail-fast: true\n            matrix:\n                os: [ windows-latest, ubuntu-latest, macos-13 ]\n            max-parallel: 1\n        steps:\n            -   uses: actions/checkout@v4.1.6\n            -   name: Set up JDK 21\n                uses: actions/setup-java@v4.2.1\n                with:\n                    distribution: 'temurin'\n                    java-version: 21\n                    architecture: x64\n            -   name: Cache Maven packages\n                uses: actions/cache@v4.0.2\n                with:\n                    path: ~/.m2\n                    key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}\n                    restore-keys: ${{ runner.os }}-m2-\n            -   name: Build with Maven\n                run: mvn --no-transfer-progress verify\n",
    "source": "juhablkdk/MyWebGoat",
    "path": ".github/workflows/build.yml",
    "url": "https://github.com/juhablkdk/MyWebGoat/blob/a738c5c74d9405b947c6882ffa6513a77f12e4bd/.github/workflows/build.yml",
    "retrieved_at": "2025-09-01T01:54:56.226612Z"
  },
  {
    "question": "What triggers this workflow and what environment does it deploy to?",
    "answer": "name: deploy\n\non: push\n\n# It is important to specify \"concurrency\" for the workflow,\n# to prevent concurrency between different deploys.\nconcurrency: production_environment\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Setup PHP\n        uses: shivammathur/setup-php@v2\n        with:\n          php-version: '8.2'\n\n      - name: Install dependencies\n        run: composer install\n\n      - name: Deploy\n        uses: deployphp/action@v1\n        with:\n          dep: deploy\n          private-key: ${{ secrets.PRIVATE_KEY }}\n",
    "source": "RalfHei/Hajusrakendused",
    "path": ".github/workflows/deploy.yml",
    "url": "https://github.com/RalfHei/Hajusrakendused/blob/f59f7414d2a1ff64cb7b0d5b3f973b6b55922400/.github/workflows/deploy.yml",
    "retrieved_at": "2025-09-01T01:54:56.936355Z"
  },
  {
    "question": "Under what conditions are the builds uploaded to the `latest` S3 bucket?",
    "answer": "# The 32 and 64 bit version of these actions should be kept in sync\nname: Android 64-bit Release\n\non:\n  push:\n    branches:\n      - 'master'\n      - 'Stable*'\n    tags:\n      - 'v*'\n  pull_request:\n    branches:\n    - '*'\n\ndefaults:\n  run:\n    shell: bash\n\nenv:\n  SOURCE_DIR:   ${{ github.workspace }}\n  QT_VERSION:   5.15.2\n  ARTIFACT:     QGroundControl64.apk\n  BUILD_TYPE:   ${{ fromJSON('[\"DailyBuild\", \"StableBuild\"]')[ github.ref_type == 'tag' || contains(github.ref, 'Stable_' ) ] }}\n\njobs:\n  build:\n    runs-on:  ubuntu-20.04\n\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v2\n        with:\n          submodules: recursive\n\n      - name: Get all tags for correct version determination\n        working-directory:  ${{ github.workspace }}\n        run: |\n          git fetch --all --tags -f\n\n      - name: Install Qt\n        uses: jurplel/install-qt-action@v3\n        with:\n          version:      ${{ env.QT_VERSION }}\n          host:         linux\n          target:       android\n          dir:          ${{ runner.temp }}\n          modules:      qtcharts\n          setup-python: true\n\n      - name: Install Android NDK\n        uses: nttld/setup-ndk@v1\n        id: setup-ndk\n        with:\n          ndk-version: r21e\n          add-to-path: false\n\n      - name: Remove Android SDK android-33-ext\n        run: |\n            ${ANDROID_SDK_ROOT}/cmdline-tools/latest/bin/sdkmanager --uninstall \"platforms;android-33-ext5\"\n            ${ANDROID_SDK_ROOT}/cmdline-tools/latest/bin/sdkmanager --uninstall \"platforms;android-33-ext4\"\n\n      - name: Install ccache\n        run:  sudo apt-get install ccache\n\n      - name: Prepare ccache timestamp\n        id: ccache_cache_timestamp\n        shell: cmake -P {0}\n        run: |\n          string(TIMESTAMP current_date \"%Y-%m-%d-%H;%M;%S\" UTC)\n          message(\"::set-output name=timestamp::${current_date}\")\n\n      - name: ccache cache files\n        uses: actions/cache@v2\n        with:\n          path:         ~/.ccache\n          key:          ${{ runner.os }}-ccache-${{steps.ccache_cache_timestamp.outputs.timestamp}}\n          restore-keys: ${{ runner.os }}-ccache-\n\n      - name: Setup ccache\n        run: |\n            mkdir -p ~/.ccache\n            echo \"base_dir = ${GITHUB_WORKSPACE}\" > ~/.ccache/ccache.conf\n            echo \"compression = true\" >> ~/.ccache/ccache.conf\n            echo \"compression_level = 5\" >> ~/.ccache/ccache.conf\n            ccache -s\n            ccache -z\n\n      - name: Create build directory\n        run:  mkdir ${{ runner.temp }}/shadow_build_dir\n\n      - name:               Install gstreamer\n        working-directory:  ${{ github.workspace }}\n        run: |\n            wget --quiet https://gstreamer.freedesktop.org/data/pkg/android/1.18.5/gstreamer-1.0-android-universal-1.18.5.tar.xz\n            mkdir gstreamer-1.0-android-universal-1.18.5\n            tar xf gstreamer-1.0-android-universal-1.18.5.tar.xz -C gstreamer-1.0-android-universal-1.18.5\n\n      # This will set GIT_BRANCH_NAME environment variable\n      - name: Git branch name\n        id:   git-branch-name\n        uses: EthanSK/git-branch-name-action@v1\n\n      - name: Update android manifest\n        run: |\n          if [ $GIT_BRANCH_NAME != \"Stable*\" ]; then\n            ${SOURCE_DIR}/tools/update_android_manifest_package.sh ${GIT_BRANCH_NAME}\n          fi\n\n      - name: Build\n        working-directory: ${{ runner.temp }}/shadow_build_dir\n        env:\n          ANDROID_KEYSTORE_PASSWORD: ${{ secrets.ANDROID_KEYSTORE_PASSWORD }}\n          ANDROID_NDK_ROOT: ${{ steps.setup-ndk.outputs.ndk-path }}\n          ANDROID_NDK_HOME: ${{ steps.setup-ndk.outputs.ndk-path }}\n          ANDROID_NDK_LATEST_HOME: ${{ steps.setup-ndk.outputs.ndk-path }}\n          ANDROID_NDK: ${{ steps.setup-ndk.outputs.ndk-path }}\n        run:  |\n            qmake -r ${SOURCE_DIR}/qgroundcontrol.pro -spec android-clang CONFIG+=${BUILD_TYPE} CONFIG+=installer ANDROID_ABIS=\"arm64-v8a\"\n            make -j2\n\n      - name: ccache post-run\n        run:  ccache -s\n\n      - name: Save artifact\n        uses: actions/upload-artifact@master\n        with:\n          name: ${{ env.ARTIFACT }}\n          path: ${{ runner.temp }}/shadow_build_dir/package/${{ env.ARTIFACT }}\n\n      - name: Upload build to S3 Bucket\n        if:                 github.event_name == 'push'\n        working-directory:  ${{ runner.temp }}/shadow_build_dir/package\n        run: |\n              aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}\n              aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n              aws s3 cp ${ARTIFACT} s3://qgroundcontrol/builds/${GIT_BRANCH_NAME}/${ARTIFACT} --region us-west-2 --acl public-read\n\n      - name: Upload tagged stable build to S3 latest Bucket\n        if:                 github.event_name == 'push' && github.ref_type == 'tag'\n        working-directory:  ${{ runner.temp }}/shadow_build_dir/package\n        run: |\n              aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}\n              aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n              aws s3 cp ${ARTIFACT} s3://qgroundcontrol/latest/${ARTIFACT} --region us-west-2 --acl public-read\n",
    "source": "HETONGAPP/qgroundcontrol",
    "path": ".github/workflows/android_64_release.yml",
    "url": "https://github.com/HETONGAPP/qgroundcontrol/blob/1ac709c82d5eb592e7db1d5af7d275055bfa2d2a/.github/workflows/android_64_release.yml",
    "retrieved_at": "2025-09-01T01:54:57.742222Z"
  },
  {
    "question": "What tests are executed by the `python3 setup.py test` command within the workflow?",
    "answer": "name: ci\n\non: [push, pull_request]\n\njobs:\n  build:\n    runs-on: ubuntu-22.04\n    steps:\n      # Checkout Repository\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Setup CCache\n        uses: hendrikmuhs/ccache-action@v1.2\n\n      # Install Tools\n      - name: Install Tools\n        run: |\n          sudo apt-get install wget build-essential ninja-build\n          sudo apt-get install libevent-dev libjson-c-dev flex bison\n          sudo apt-get install libfl-dev libfl2 zlib1g-dev\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.9\"\n          cache: \"pip\"\n          cache-dependency-path: \"setup.py\"\n\n      - name: Install Python dependencies\n        run: |\n          python3 -m pip install setuptools requests pexpect meson\n\n      # Install (n)Migen / LiteX / Cores\n      - name: Install LiteX\n        run: |\n          python3 litex_setup.py --config=full --init --install --user --dev\n\n      # Install GCC Toolchains\n      - name: Install GCC Toolchains\n        run: |\n          sudo python3 litex_setup.py --gcc=riscv\n          sudo python3 litex_setup.py --gcc=openrisc\n          sudo python3 litex_setup.py --gcc=powerpc\n\n      # Build / Install GHDL\n      - name: Build GHDL\n        run: |\n          sudo apt-get install gnat llvm\n          git clone https://github.com/ghdl/ghdl.git\n          cd ghdl\n          ./configure --with-llvm-config\n          make\n          sudo make install\n\n      # Build / Install Verilator\n      - name: Build Verilator\n        run: |\n          sudo apt-get install help2man\n          export PATH=\"/usr/lib/ccache:/usr/local/opt/ccache/libexec:$PATH\"\n          git clone https://github.com/verilator/verilator\n          cd verilator\n          git checkout 7d2d32420a630befa4097170ecbf227e04e32522\n          autoconf\n          ./configure\n          make -j$(nproc)\n          sudo make install\n\n      # Install Project\n      - name: Install Project\n        run: python3 setup.py develop --user\n\n      # Test\n      - name: Run Tests\n        run: |\n          python3 setup.py test\n",
    "source": "kuznia-rdzeni/litex_",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/kuznia-rdzeni/litex_/blob/6ec1e14269f1b984c0355989a9dbeca6802d21cc/.github/workflows/ci.yml",
    "retrieved_at": "2025-09-01T01:54:58.433192Z"
  },
  {
    "question": "What actions are performed on the checked-out repository code within the workflow?",
    "answer": "name: GitHub Actions Demo\non: [push]\njobs:\n  Explore-GitHub-Actions:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"🎉 The job was automatically triggered by a ${{ github.event_name }} event.\"\n      - run: echo \"🐧 This job is now running on a ${{ runner.os }} server hosted by GitHub!\"\n      - run: echo \"🔎 The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}.\"\n      - name: Check out repository code\n        uses: actions/checkout@v2\n      - run: echo \"💡 The ${{ github.repository }} repository has been cloned to the runner.\"\n      - run: echo \"🖥️ The workflow is now ready to test your code on the runner.\"\n      - name: List files in the repository\n        run: |\n          ls ${{ github.workspace }}\n      - run: echo \"🍏 This job's status is ${{ job.status }}.\"\n\n",
    "source": "WildCodeSchool/2104-wns-lyon-groupe2-back",
    "path": ".github/workflows/main.yml",
    "url": "https://github.com/WildCodeSchool/2104-wns-lyon-groupe2-back/blob/69ab6a140cfd1448222a1142b6a1df8a90a457bf/.github/workflows/main.yml",
    "retrieved_at": "2025-09-01T01:54:59.085990Z"
  },
  {
    "question": "For jobs using the retry-action, what types of failures trigger a retry, and how many retry attempts are made?",
    "answer": "# To enable retrying a job on failure or a specific timeout, instead of the run step, use uses: nick-fields/retry@v2.9.0(see the linux-gcc-make-tsan jsob)\n# To retry only on timeout set retry_on: timeout\n# To retry only on error set retry_on: error\n# For more information on the retry action see https://github.com/nick-fields/retry\n\nname: Compile and Testrun\n\non:\n  pull_request:\n    types: [opened]\n  push:\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}\n  cancel-in-progress: true\n\njobs:\n  android-arm64-v8a-ndk-latest-cmake:\n   runs-on: ubuntu-22.04\n   steps:\n       - uses: actions/checkout@v3\n       - uses: nttld/setup-ndk@v1\n         with:\n            ndk-version: r25c\n            add-to-path: true\n       - run: cmake -S$GITHUB_WORKSPACE -B$HOME/android-build -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-21 -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_LATEST_HOME/build/cmake/android.toolchain.cmake && cmake --build $HOME/android-build --target all\n\n  android-arm64-v8a-ndk-cmake:\n   runs-on: ubuntu-22.04\n   steps:\n       - uses: actions/checkout@v3\n       - uses: nttld/setup-ndk@v1\n         with:\n            ndk-version: r25c\n            add-to-path: true\n       - run: cmake -S$GITHUB_WORKSPACE -B$HOME/android-build -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-21 -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_HOME/build/cmake/android.toolchain.cmake && cmake --build $HOME/android-build --target all\n\n  android-armeabi-v7a-ndk-cmake:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - uses: nttld/setup-ndk@v1\n        with:\n          ndk-version: r25c\n          add-to-path: true\n      - run: cmake -S$GITHUB_WORKSPACE -B$HOME/android-build -DANDROID_ABI=armeabi-v7a -DANDROID_PLATFORM=android-21 -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_HOME/build/cmake/android.toolchain.cmake && cmake --build $HOME/android-build --target all\n\n  linux-gcc-make:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev redis-server libmysqlclient-dev\n      - run: ./configure --everything --omit=PDF && make all -s -j4 && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"Data/ODBC Data/MySQL Data/PostgreSQL MongoDB\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-cxx20:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev redis-server libmysqlclient-dev\n      - run: ./configure --config=Linux-c++20 --everything --omit=PDF && make all -s -j4 && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"Data/ODBC Data/MySQL Data/PostgreSQL MongoDB\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-asan:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev redis-server\n      - run: ./configure --everything --no-samples --omit=PDF && make all -s -j4 SANITIZEFLAGS=-fsanitize=address && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"Data/ODBC Data/PostgreSQL Data/MySQL MongoDB\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-asan-no-soo:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev redis-server\n      - run: ./configure --everything --no-samples --omit=PDF --no-soo && make all -s -j4 SANITIZEFLAGS=-fsanitize=address && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"Data/MySQL Data/ODBC Data/PostgreSQL MongoDB\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-ubsan:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev redis-server\n      - run: ./configure --everything --no-samples --omit=PDF && make all -s -j4 SANITIZEFLAGS=-fsanitize=undefined && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"Data/MySQL Data/ODBC Data/PostgreSQL MongoDB\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-tsan:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev redis-server\n      - run: ./configure --everything --no-samples --omit=CppParser,Encodings,Data/MySQL,Data/ODBC,Data/PostgreSQL,MongoDB,PageCompiler,PDF,PocoDoc,ProGen,Redis,SevenZip && make all -s -j4 SANITIZEFLAGS=-fsanitize=thread && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            ./ci/runtests.sh TSAN\n\n  linux-gcc-cmake:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install cmake ninja-build libssl-dev unixodbc-dev libmysqlclient-dev redis-server\n      - run: cmake -S. -Bcmake-build -GNinja -DENABLE_PDF=OFF -DENABLE_TESTS=ON && cmake --build cmake-build --target all\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            cd cmake-build &&\n            sudo -s\n            PWD=`pwd`\n            ctest --output-on-failure -E \"(DataMySQL)|(DataODBC)|(PostgreSQL)|(MongoDB)\"\n\n  linux-emscripten-cmake:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install cmake ninja-build emscripten\n      - run: emcmake cmake -H. -B cmake-build -DENABLE_ACTIVERECORD_COMPILER=OFF -DENABLE_PAGECOMPILER=OFF -DENABLE_PAGECOMPILER_FILE2PAGE=off && emmake cmake --build cmake-build --target all -j4\n# TODO: How to run unit tests in emscripten?\n#      - uses: ./.github/actions/retry-action\n#        with:\n#          timeout_minutes: 90\n#          max_attempts: 3\n#          retry_on: any\n#          command: >-\n#            cd cmake-build &&\n#            sudo -s\n#            PWD=`pwd`\n#            ctest --output-on-failure -E \"(DataMySQL)|(DataODBC)|(PostgreSQL)|(MongoDB)\"\n\n  linux-gcc-make-cross-armhf:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: >-\n          sudo apt-get -y update &&\n          sudo apt-get -y install crossbuild-essential-armhf\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            ./configure --config=ARM-Linux --everything --omit=PDF,Crypto,NetSSL_OpenSSL,JWT,Data/MySQL,Data/ODBC,Data/PostgreSQL,PageCompiler,PageCompiler/File2Page &&\n            make all -s -j4 ARCHFLAGS=\"-mcpu=cortex-a8 -mfloat-abi=hard -mfpu=neon\" TOOL=arm-linux-gnueabihf\n\n  macos-clang-make:\n    runs-on: macos-12\n    steps:\n      - uses: actions/checkout@v3\n      - run: brew install openssl@1.1 mysql-client unixodbc libpq\n      - run: >-\n          ./configure --everything --no-prefix --omit=PDF\n          --odbc-include=/usr/local/opt/unixodbc/include --odbc-lib=/usr/local/opt/unixodbc/lib\n          --mysql-include=/usr/local/opt/mysql-client/include --mysql-lib=/usr/local/opt/mysql-client/lib\n          --include-path=\"/usr/local/opt/openssl@1.1/include\" --library-path=\"/usr/local/opt/openssl@1.1/lib\" &&\n          make all -s -j4\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            CPPUNIT_IGNORE=\"\n            CppUnit::TestCaller<ThreadTest>.testTrySleep,\n            CppUnit::TestCaller<TimestampTest>.testTimestamp,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<UniqueExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<SyslogTest>.testOldBSD,\n            CppUnit::TestCaller<PollSetTest>.testPollClosedServer\"\n            EXCLUDE_TESTS=\"Redis Data/MySQL Data/ODBC Data/PostgreSQL MongoDB PDF\"\n            ./ci/runtests.sh\n\n  macos-clang-make-visibility-hidden:\n    runs-on: macos-12\n    steps:\n      - uses: actions/checkout@v3\n      - run: brew install openssl@1.1 mysql-client unixodbc libpq\n      - run: >-\n          ./configure --everything --no-prefix --cflags=\"-fvisibility=hidden\" --omit=PDF\n          --odbc-include=/usr/local/opt/unixodbc/include --odbc-lib=/usr/local/opt/unixodbc/lib\n          --mysql-include=/usr/local/opt/mysql-client/include --mysql-lib=/usr/local/opt/mysql-client/lib\n          --include-path=\"/usr/local/opt/openssl@1.1/include\" --library-path=\"/usr/local/opt/openssl@1.1/lib\" &&\n          make all -s -j4\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            CPPUNIT_IGNORE=\"\n            CppUnit::TestCaller<ThreadTest>.testTrySleep,\n            CppUnit::TestCaller<TimestampTest>.testTimestamp,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<UniqueExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<SyslogTest>.testOldBSD,\n            CppUnit::TestCaller<PollSetTest>.testPollClosedServer\"\n            EXCLUDE_TESTS=\"Redis Data/MySQL Data/ODBC Data/PostgreSQL MongoDB PDF\"\n            ./ci/runtests.sh\n\n  macos-clang-cmake-openssl:\n    runs-on: macos-12\n    steps:\n      - uses: actions/checkout@v3\n      - run: brew install openssl@1.1 mysql-client unixodbc libpq\n      - run: cmake -S. -Bcmake-build -DENABLE_PDF=OFF -DENABLE_TESTS=ON -DOPENSSL_ROOT_DIR=/usr/local/opt/openssl@1.1 -DMYSQL_ROOT_DIR=/usr/local/opt/mysql-client && cmake --build cmake-build --target all\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            cd cmake-build &&\n            sudo -s\n            CPPUNIT_IGNORE=\"\n            CppUnit::TestCaller<ThreadTest>.testTrySleep,\n            CppUnit::TestCaller<TimestampTest>.testTimestamp,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<UniqueExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<PollSetTest>.testPollClosedServer\"\n            PWD=`pwd`\n            ctest --output-on-failure -E \"(DataMySQL)|(DataODBC)|(PostgreSQL)|(MongoDB)|(Redis)\"\n\n  macos-clang-cmake-openssl3:\n    runs-on: macos-12\n    steps:\n      - uses: actions/checkout@v3\n      - run: brew install openssl@3 mysql-client unixodbc libpq\n      - run: cmake -S. -Bcmake-build -DENABLE_PDF=OFF -DENABLE_TESTS=ON -DOPENSSL_ROOT_DIR=/usr/local/opt/openssl@3 -DMYSQL_ROOT_DIR=/usr/local/opt/mysql-client && cmake --build cmake-build --target all\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            cd cmake-build &&\n            sudo -s\n            CPPUNIT_IGNORE=\"\n            CppUnit::TestCaller<ThreadTest>.testTrySleep,\n            CppUnit::TestCaller<TimestampTest>.testTimestamp,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<UniqueExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<PollSetTest>.testPollClosedServer\"\n            PWD=`pwd`\n            ctest --output-on-failure -E \"(DataMySQL)|(DataODBC)|(PostgreSQL)|(MongoDB)|(Redis)\"\n\n  macos-clang-cmake-openssl3-visibility-hidden:\n    runs-on: macos-12\n    steps:\n      - uses: actions/checkout@v3\n      - run: brew install openssl@3 mysql-client unixodbc libpq\n      - run: cmake -S. -Bcmake-build -DCMAKE_CXX_VISIBILITY_PRESET=hidden -DENABLE_ENCODINGS_COMPILER=ON -DENABLE_PDF=ON -DENABLE_SEVENZIP=ON -DENABLE_CPPPARSER=ON -DENABLE_TESTS=ON -DOPENSSL_ROOT_DIR=/usr/local/opt/openssl@3 -DMYSQL_ROOT_DIR=/usr/local/opt/mysql-client && cmake --build cmake-build --target all\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            cd cmake-build &&\n            sudo -s\n            CPPUNIT_IGNORE=\"\n            CppUnit::TestCaller<ThreadTest>.testTrySleep,\n            CppUnit::TestCaller<TimestampTest>.testTimestamp,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<UniqueExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<PollSetTest>.testPollClosedServer\"\n            PWD=`pwd`\n            ctest --output-on-failure -E \"(DataMySQL)|(DataODBC)|(PostgreSQL)|(MongoDB)|(Redis)\"\n\n  macos-clang-make-openssl3-tsan:\n    runs-on: macos-12\n    steps:\n      - uses: actions/checkout@v3\n      - run: brew install openssl@3\n      - run: >-\n          ./configure --everything --no-prefix --no-samples --omit=CppParser,Encodings,Data/MySQL,Data/ODBC,Data/PostgreSQL,MongoDB,PageCompiler,PDF,PocoDoc,ProGen,Redis,SevenZip\n          --odbc-include=/usr/local/opt/unixodbc/include --odbc-lib=/usr/local/opt/unixodbc/lib\n          --mysql-include=/usr/local/opt/mysql-client/include --mysql-lib=/usr/local/opt/mysql-client/lib\n          --include-path=\"/usr/local/opt/openssl@3/include\" --library-path=\"/usr/local/opt/openssl@3/lib\" &&\n          make all -s -j4 SANITIZEFLAGS=-fsanitize=thread\n\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            CPPUNIT_IGNORE=\"\n            CppUnit::TestCaller<ThreadTest>.testTrySleep,\n            CppUnit::TestCaller<TimestampTest>.testTimestamp,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<UniqueExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<PollSetTest>.testPollClosedServer\"\n            EXCLUDE_TESTS=\"Redis Data/MySQL Data/ODBC Data/PostgreSQL MongoDB PDF\"\n            ./ci/runtests.sh TSAN\n\n  macos-clang-make-openssl3-ubsan:\n    runs-on: macos-12\n    steps:\n      - uses: actions/checkout@v3\n      - run: brew install openssl@3 mysql-client unixodbc libpq\n      - run: >-\n          ./configure --everything --no-prefix --no-samples --omit=PDF\n          --odbc-include=/usr/local/opt/unixodbc/include --odbc-lib=/usr/local/opt/unixodbc/lib\n          --mysql-include=/usr/local/opt/mysql-client/include --mysql-lib=/usr/local/opt/mysql-client/lib\n          --include-path=\"/usr/local/opt/openssl@3/include\" --library-path=\"/usr/local/opt/openssl@3/lib\" &&\n          make all -s -j4 SANITIZEFLAGS=-fsanitize=undefined\n\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            CPPUNIT_IGNORE=\"\n            CppUnit::TestCaller<ThreadTest>.testTrySleep,\n            CppUnit::TestCaller<TimestampTest>.testTimestamp,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<UniqueExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<PollSetTest>.testPollClosedServer\"\n            EXCLUDE_TESTS=\"Redis Data/MySQL Data/ODBC Data/PostgreSQL MongoDB PDF\"\n            ./ci/runtests.sh\n\n  macos-clang-make-openssl3-asan:\n    runs-on: macos-12\n    steps:\n      - uses: actions/checkout@v3\n      - run: brew install openssl@3 mysql-client unixodbc libpq\n      - run: >-\n          ./configure --everything --no-prefix --no-samples --omit=PDF\n          --odbc-include=/usr/local/opt/unixodbc/include --odbc-lib=/usr/local/opt/unixodbc/lib\n          --mysql-include=/usr/local/opt/mysql-client/include --mysql-lib=/usr/local/opt/mysql-client/lib\n          --include-path=\"/usr/local/opt/openssl@3/include\" --library-path=\"/usr/local/opt/openssl@3/lib\" &&\n          make all -s -j4 SANITIZEFLAGS=-fsanitize=address\n\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            CPPUNIT_IGNORE=\"\n            CppUnit::TestCaller<ThreadTest>.testTrySleep,\n            CppUnit::TestCaller<TimestampTest>.testTimestamp,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<UniqueExpireLRUCacheTest>.testExpireN,\n            CppUnit::TestCaller<ExpireLRUCacheTest>.testAccessExpireN,\n            CppUnit::TestCaller<PollSetTest>.testPollClosedServer\"\n            EXCLUDE_TESTS=\"Redis Data/MySQL Data/ODBC Data/PostgreSQL MongoDB PDF\"\n            ./ci/runtests.sh\n\n#   windows-2019-msvc-cmake:\n#     runs-on: windows-2019\n#     env:\n#       CPPUNIT_IGNORE: >-\n#         class CppUnit::TestCaller<class PathTest>.testFind,\n#         class CppUnit::TestCaller<class ICMPSocketTest>.testSendToReceiveFrom,\n#         class CppUnit::TestCaller<class ICMPClientTest>.testPing,\n#         class CppUnit::TestCaller<class ICMPClientTest>.testBigPing,\n#         class CppUnit::TestCaller<class ICMPSocketTest>.testMTU,\n#         class CppUnit::TestCaller<class HTTPSClientSessionTest>.testProxy,\n#         class CppUnit::TestCaller<class HTTPSStreamFactoryTest>.testProxy,\n#         class CppUnit::TestCaller<class PollSetTest>.testPollClosedServer\n#     steps:\n#       - uses: actions/checkout@v3\n#       - run: cmake -S. -Bcmake-build -DENABLE_NETSSL_WIN=ON -DENABLE_NETSSL=OFF -DENABLE_CRYPTO=OFF -DENABLE_JWT=OFF -DENABLE_DATA=ON -DENABLE_DATA_ODBC=ON -DENABLE_DATA_MYSQL=OFF -DENABLE_DATA_POSTGRESQL=OFF -DENABLE_TESTS=ON\n#       - run: cmake --build cmake-build --config Release\n#       - uses: ./.github/actions/retry-action\n#          with:\n#             timeout_minutes: 90\n#             max_attempts: 3\n#             retry_on: any\n#             command: >-\n#             cd cmake-build;\n#             ctest --output-on-failure -E \"(DataMySQL)|(DataODBC)|(Redis)|(MongoDB)\" -C Release\n\n#   windows-2019-msvc-buildwin-x64:\n#     runs-on: windows-2019\n#     env:\n#       CPPUNIT_IGNORE: >-\n#         class CppUnit::TestCaller<class PathTest>.testFind,\n#         class CppUnit::TestCaller<class ICMPSocketTest>.testSendToReceiveFrom,\n#         class CppUnit::TestCaller<class ICMPClientTest>.testPing,\n#         class CppUnit::TestCaller<class ICMPClientTest>.testBigPing,\n#         class CppUnit::TestCaller<class ICMPSocketTest>.testMTU,\n#         class CppUnit::TestCaller<class HTTPSClientSessionTest>.testProxy,\n#         class CppUnit::TestCaller<class HTTPSStreamFactoryTest>.testProxy\n#     steps:\n#       - uses: actions/checkout@v3\n#       - uses: ./.github/actions/retry-action\n#         with:\n#           timeout_minutes: 90\n#           max_attempts: 3\n#           retry_on: any\n#           command: .\\buildwin.ps1 -poco_base . -vs 160 -action build -linkmode all -config release -platform x64 -samples -tests -omit \"Crypto,NetSSL_OpenSSL,Data/MySQL,Data/PostgreSQL,JWT\"\n\n#  windows-2019-msvc-buildwin-win32:\n#    runs-on: windows-2019\n#    env:\n#      CPPUNIT_IGNORE: class CppUnit::TestCaller<class PathTest>.testFind,class CppUnit::TestCaller<class ICMPSocketTest>.testSendToReceiveFrom,class CppUnit::TestCaller<class ICMPClientTest>.testPing,class CppUnit::TestCaller<class ICMPClientTest>.testBigPing,class CppUnit::TestCaller<class ICMPSocketTest>.testMTU,class CppUnit::TestCaller<class HTTPSClientSessionTest>.testProxy,class CppUnit::TestCaller<class HTTPSStreamFactoryTest>.testProxy\n#    steps:\n#      - uses: actions/checkout@v3\n#      - uses: ./.github/actions/retry-action\n#        with:\n#          timeout_minutes: 90\n#          max_attempts: 3\n#          retry_on: any\n#          command: .\\buildwin.ps1 -poco_base . -vs 160 -action build -linkmode all -config release -platform Win32 -samples -tests -omit \"Crypto,NetSSL_OpenSSL,Data/MySQL,Data/PostgreSQL,JWT\"\n\n  windows-2022-msvc-buildwin-x64:\n    runs-on: windows-2022\n    env:\n      CPPUNIT_IGNORE: >-\n        class CppUnit::TestCaller<class PathTest>.testFind,\n        class CppUnit::TestCaller<class ICMPSocketTest>.testSendToReceiveFrom,\n        class CppUnit::TestCaller<class ICMPClientTest>.testPing,\n        class CppUnit::TestCaller<class ICMPClientTest>.testBigPing,\n        class CppUnit::TestCaller<class ICMPSocketTest>.testMTU,\n        class CppUnit::TestCaller<class HTTPSClientSessionTest>.testProxy,\n        class CppUnit::TestCaller<class HTTPSStreamFactoryTest>.testProxy\n    steps:\n      - uses: actions/checkout@v3\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: .\\buildwin.ps1 -poco_base . -vs 170 -action build -linkmode all -config release -platform x64 -samples -tests -omit \"Crypto,NetSSL_OpenSSL,Data/MySQL,Data/PostgreSQL,JWT\"\n\n#  windows-2022-msvc-buildwin-win32:\n#    runs-on: windows-2022\n#    env:\n#      CPPUNIT_IGNORE: >-\n#        class CppUnit::TestCaller<class PathTest>.testFind,\n#        class CppUnit::TestCaller<class ICMPSocketTest>.testSendToReceiveFrom,\n#        class CppUnit::TestCaller<class ICMPClientTest>.testPing,\n#        class CppUnit::TestCaller<class ICMPClientTest>.testBigPing,\n#        class CppUnit::TestCaller<class ICMPSocketTest>.testMTU,\n#        class CppUnit::TestCaller<class HTTPSClientSessionTest>.testProxy,\n#        class CppUnit::TestCaller<class HTTPSStreamFactoryTest>.testProxy\n#    steps:\n#      - uses: actions/checkout@v3\n#      - uses: ./.github/actions/retry-action\n#      with:\n#        timeout_minutes: 90\n#        max_attempts: 3\n#        retry_on: any\n#        command: .\\buildwin.ps1 -poco_base . -vs 170 -action build -linkmode all -config release -platform Win32 -samples -tests -omit \"Crypto,NetSSL_OpenSSL,Data/MySQL,Data/PostgreSQL,JWT\"\n\n  windows-2022-msvc-cmake:\n    runs-on: windows-2022\n    env:\n      CPPUNIT_IGNORE: >-\n        class CppUnit::TestCaller<class PathTest>.testFind,\n        class CppUnit::TestCaller<class ICMPSocketTest>.testSendToReceiveFrom,\n        class CppUnit::TestCaller<class ICMPClientTest>.testPing,\n        class CppUnit::TestCaller<class ICMPClientTest>.testBigPing,\n        class CppUnit::TestCaller<class ICMPSocketTest>.testMTU,\n        class CppUnit::TestCaller<class HTTPSClientSessionTest>.testProxy,\n        class CppUnit::TestCaller<class HTTPSStreamFactoryTest>.testProxy\n    steps:\n      - uses: actions/checkout@v3\n      - run: cmake -S. -Bcmake-build -DENABLE_NETSSL_WIN=ON -DENABLE_NETSSL=OFF -DENABLE_CRYPTO=OFF -DENABLE_JWT=OFF -DENABLE_DATA=ON -DENABLE_DATA_ODBC=ON -DENABLE_DATA_MYSQL=OFF -DENABLE_DATA_POSTGRESQL=OFF -DENABLE_TESTS=ON\n      - run: cmake --build cmake-build --config Release\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            cd cmake-build;\n            ctest --output-on-failure -E \"(DataMySQL)|(DataODBC)|(Redis)|(MongoDB)\" -C Release\n\n# missing asan dll path\n#  windows-2022-msvc-cmake-asan:\n#    runs-on: windows-2022\n#    env:\n#      CPPUNIT_IGNORE: >-\n#        class CppUnit::TestCaller<class PathTest>.testFind,\n#        class CppUnit::TestCaller<class ICMPSocketTest>.testSendToReceiveFrom,\n#        class CppUnit::TestCaller<class ICMPClientTest>.testPing,\n#        class CppUnit::TestCaller<class ICMPClientTest>.testBigPing,\n#        class CppUnit::TestCaller<class ICMPSocketTest>.testMTU,\n#        class CppUnit::TestCaller<class HTTPSClientSessionTest>.testProxy,\n#        class CppUnit::TestCaller<class HTTPSStreamFactoryTest>.testProxy\n#    steps:\n#      - uses: actions/checkout@v3\n#      - run: cmake -S. -Bcmake-build -DPOCO_SANITIZE_ASAN=ON -DENABLE_NETSSL_WIN=ON -DENABLE_NETSSL=OFF -DENABLE_CRYPTO=OFF -DENABLE_JWT=OFF -DENABLE_DATA=ON -DENABLE_DATA_ODBC=ON -DENABLE_DATA_MYSQL=OFF -DENABLE_DATA_POSTGRESQL=OFF -DENABLE_TESTS=ON\n#      - run: cmake --build cmake-build --config Debug\n#      - uses: ./.github/actions/retry-action\n#        with:\n#          timeout_minutes: 90\n#          max_attempts: 3\n#          retry_on: any\n#          command: >-\n#          cd cmake-build;\n#          ctest --output-on-failure -E \"(DataMySQL)|(DataODBC)|(Redis)|(MongoDB)\" -C Debug\n\n  linux-gcc-make-mysql:\n    runs-on: ubuntu-22.04\n    services:\n      mysql:\n        image: mysql:8.1.0\n        env:\n          MYSQL_ALLOW_EMPTY_PASSWORD: yes\n          MYSQL_USER: pocotest\n          MYSQL_PASSWORD: pocotest\n          MYSQL_DATABASE: pocotest\n        ports:\n          - 3306:3306\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev  mysql-client\n      - run: ./configure --everything --no-samples --omit=ActiveRecord,ApacheConnector,CppParser,Crypto,Data/PostgreSQL,Data/SQLite,Data/ODBC,Encodings,JSON,JWT,MongoDB,Net,NetSSL_OpenSSL,NetSSL_Win,PDF,PageCompiler,PocoDoc,ProGen,Prometheus,Redis,SevenZip,Util,XML,Zip && make all -s -j4 && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"ActiveRecord ApacheConnector CppParser CppUnit Crypto Data Data/PostgreSQL Data/ODBC Data/SQLite Encodings Foundation JSON JWT MongoDB Net NetSSL_OpenSSL NetSSL_Win PDF PageCompiler PocoDoc ProGen Prometheus Redis SevenZip Util XML Zip\"\n            ./ci/runtests.sh\n\n# TODO tests sometimes failing on testTransaction and testReconnect\n  linux-gcc-make-postgres:\n    runs-on: ubuntu-22.04\n    services:\n      postgres:\n        image: postgres:16.0\n        env:\n          POSTGRES_PASSWORD: postgres\n        ports:\n          - 5432:5432\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev odbc-postgresql\n      - run: ./configure --everything --no-samples --omit=ActiveRecord,ApacheConnector,CppParser,Crypto,Data/MySQL,Data/ODBC,Data/SQLite,Encodings,JSON,JWT,MongoDB,Net,NetSSL_OpenSSL,NetSSL_Win,PDF,PageCompiler,PocoDoc,ProGen,Prometheus,Redis,SevenZip,Util,XML,Zip && make all -s -j4 && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"ActiveRecord ApacheConnector CppParser CppUnit Crypto Data Data/ODBC Data/MySQL Data/SQLite Encodings Foundation JSON JWT MongoDB Net NetSSL_OpenSSL NetSSL_Win PDF PageCompiler PocoDoc ProGen Prometheus Redis SevenZip Util XML Zip\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-redis:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev\n      - run: |\n          curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n          echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n          sudo apt-get -y update\n          sudo apt-get -y install redis\n      - run: ./configure --everything --no-samples --omit=ActiveRecord,ApacheConnector,CppParser,Crypto,Data/ODBC,Data/MySQL,Data/SQLite,Data/PostgreSQL,Encodings,JSON,JWT,MongoDB,Net,NetSSL_OpenSSL,NetSSL_Win,PDF,PageCompiler,PocoDoc,ProGen,Prometheus,SevenZip,Util,XML,Zip && make all -s -j4 && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"ActiveRecord ApacheConnector CppParser CppUnit Crypto Data Data/ODBC Data/MySQL Data/SQLite Data/PostgreSQL Encodings Foundation JSON JWT MongoDB Net NetSSL_OpenSSL NetSSL_Win PDF PageCompiler PocoDoc ProGen Prometheus SevenZip Util XML Zip\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-mongodb:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - uses: supercharge/mongodb-github-action@1.10.0\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev\n      - run: ./configure --everything --no-samples --omit=ActiveRecord,ApacheConnector,CppParser,Crypto,Data/ODBC,Data/MySQL,Data/SQLite,Data/PostgreSQL,Encodings,JSON,JWT,Net,NetSSL_OpenSSL,NetSSL_Win,PDF,PageCompiler,PocoDoc,ProGen,Prometheus,Redis,SevenZip,Util,XML,Zip && make all -s -j4 && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"ActiveRecord ApacheConnector CppParser CppUnit Crypto Data Data/ODBC Data/MySQL Data/SQLite Data/PostgreSQL Encodings Foundation JSON JWT Net NetSSL_OpenSSL NetSSL_Win PDF PageCompiler PocoDoc ProGen Prometheus Redis SevenZip Util XML Zip\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-odbc:\n    runs-on: ubuntu-22.04\n    services:\n      mysql:\n        image: mysql:8.1.0\n        env:\n          MYSQL_ALLOW_EMPTY_PASSWORD: yes\n          MYSQL_USER: pocotest\n          MYSQL_PASSWORD: pocotest\n          MYSQL_DATABASE: pocotest\n        ports:\n          - 3306:3306\n      postgres:\n        image: postgres:16.0\n        env:\n          POSTGRES_PASSWORD: postgres\n        ports:\n          - 5432:5432\n      oracle:\n        image: container-registry.oracle.com/database/express:21.3.0-xe\n        env:\n          ORACLE_PWD: poco\n        ports:\n          - 1521:1521\n      sqlserver:\n        image: mcr.microsoft.com/mssql/server:2022-latest\n        env:\n          MSSQL_PID: Express\n          ACCEPT_EULA: Y\n          MSSQL_SA_PASSWORD: Pocopoco1\n        ports:\n          - 1433:1433\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update && sudo apt -y install libssl-dev unixodbc-dev libmysqlclient-dev mysql-client alien libaio1 gnupg2 curl #odbc-postgresql\n      - run: ./configure --everything --no-samples --omit=ActiveRecord,ApacheConnector,CppParser,Crypto,Data/MySQL,Data/PostgreSQL,Data/SQLite,Encodings,JSON,JWT,MongoDB,Net,NetSSL_OpenSSL,NetSSL_Win,PDF,PageCompiler,PocoDoc,ProGen,Prometheus,Redis,SevenZip,Util,XML,Zip && make all -s -j4 && sudo make install\n      # - name: Setup MySQL ODBC connector\n      #   run: |\n      #     wget https://dev.mysql.com/get/Downloads/Connector-ODBC/8.2/mysql-connector-odbc_8.2.0-1ubuntu22.04_amd64.deb\n      #     wget https://dev.mysql.com/get/Downloads/MySQL-8.2/mysql-community-client-plugins_8.2.0-1ubuntu22.04_amd64.deb\n      #     sudo dpkg -i mysql-community-client-plugins_8.2.0-1ubuntu22.04_amd64.deb mysql-connector-odbc_8.2.0-1ubuntu22.04_amd64.deb\n      # - name: Setup Oracle ODBC connector\n      #   run: |\n      #     wget https://download.oracle.com/otn_software/linux/instantclient/2112000/oracle-instantclient-basic-21.12.0.0.0-1.x86_64.rpm\n      #     wget https://download.oracle.com/otn_software/linux/instantclient/2112000/oracle-instantclient-sqlplus-21.12.0.0.0-1.x86_64.rpm\n      #     wget https://download.oracle.com/otn_software/linux/instantclient/2112000/oracle-instantclient-odbc-21.12.0.0.0-1.x86_64.rpm\n      #     sudo alien --scripts ./oracle-instantclient-basic-21.12.0.0.0-1.x86_64.rpm\n      #     sudo alien --scripts ./oracle-instantclient-sqlplus-21.12.0.0.0-1.x86_64.rpm\n      #     sudo alien --scripts ./oracle-instantclient-odbc-21.12.0.0.0-1.x86_64.rpm\n      #     sudo apt install ./oracle-instantclient-basic_21.12.0.0.0-2_amd64.deb\n      #     sudo apt install ./oracle-instantclient-sqlplus_21.12.0.0.0-2_amd64.deb\n      #     sudo apt install ./oracle-instantclient-odbc_21.12.0.0.0-2_amd64.deb\n      #     sudo /usr/lib/oracle/21/client64/bin/odbc_update_ini.sh / \"/usr/lib/oracle/21/client64/lib\" \"\" \"\"  \"/etc/odbc.ini\"\n      - name: Setup SQL Server ODBC connector\n        run: |\n           curl https://packages.microsoft.com/keys/microsoft.asc | sudo tee /etc/apt/trusted.gpg.d/microsoft.asc\n           curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\n           sudo apt-get update\n           sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"ActiveRecord ApacheConnector CppParser CppUnit Crypto Data Data/MySQL Data/PostgreSQL Data/SQLite Encodings Foundation JSON JWT MongoDB Net NetSSL_OpenSSL NetSSL_Win PDF PageCompiler PocoDoc ProGen Prometheus Redis SevenZip Util XML Zip\"\n            ./ci/runtests.sh\n\n  linux-gcc-make-sqlite-no-sqlparser:\n    runs-on: ubuntu-22.04\n    services:\n      mysql:\n        image: mysql:8.1.0\n        env:\n          MYSQL_ALLOW_EMPTY_PASSWORD: yes\n          MYSQL_USER: pocotest\n          MYSQL_PASSWORD: pocotest\n          MYSQL_DATABASE: pocotest\n        ports:\n          - 3306:3306\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt -y update\n      - run: ./configure --everything --no-samples --no-sqlparser --omit=ActiveRecord,ApacheConnector,CppParser,Crypto,Data/PostgreSQL,Data/MySQL,Data/ODBC,Encodings,JSON,JWT,MongoDB,Net,NetSSL_OpenSSL,NetSSL_Win,PDF,PageCompiler,PocoDoc,ProGen,Prometheus,Redis,SevenZip,Util,XML,Zip && make all -s -j4 && sudo make install\n      - uses: ./.github/actions/retry-action\n        with:\n          timeout_minutes: 90\n          max_attempts: 3\n          retry_on: any\n          command: >-\n            sudo -s\n            EXCLUDE_TESTS=\"ActiveRecord ApacheConnector CppParser CppUnit Crypto Data Data/PostgreSQL Data/ODBC Data/MySQL Encodings Foundation JSON JWT MongoDB Net NetSSL_OpenSSL NetSSL_Win PDF PageCompiler PocoDoc ProGen Prometheus Redis SevenZip Util XML Zip\"\n            ./ci/runtests.sh\n",
    "source": "ISISComputingGroup/poco",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/ISISComputingGroup/poco/blob/5cc749aa5baa4405ec2f74ea72975f37f81361c0/.github/workflows/ci.yml",
    "retrieved_at": "2025-09-01T01:55:00.620497Z"
  },
  {
    "question": "What is the purpose of the `FIXME__LZ4_CI_IGNORE` environment variable and how is it used within the workflow?",
    "answer": "# For details, see README.md in this directory.\n\n###############################################################\n# C compilers\n#\n# - gcc\n# - clang\n#\n# Known Issue\n# - All test cases which described as 'fail' must be fixed and replaced with 'true'.\n#   - gcc-11 (x32, x86) : \"../lib/lz4hc.c:148: LZ4HC_countBack: Assertion `(size_t)(match - mMin) < (1U<<31)' failed.\"\n#   - all clangs (x32, x86) : \"../lib/lz4hc.c:282: int LZ4HC_InsertAndGetWiderMatch(...): Assertion `matchPtr >= lowPrefixPtr' failed.\"\n#\nname: lz4 CI\non: [push, pull_request]\njobs:\n  lz4-c-compilers:\n    name: CC=${{ matrix.cc }}, ${{ matrix.os }}\n    strategy:\n      fail-fast: false  # 'false' means Don't stop matrix workflows even if some matrix failed.\n      matrix:\n        include: [\n          # You can access the following values via ${{ matrix.??? }}\n          #\n          #   pkgs    : apt-get package names.  It can include multiple package names which are delimited by space.\n          #   cc      : C compiler executable.\n          #   cxx     : C++ compiler executable for `make ctocpptest`.\n          #   x32     : Set 'true' if compiler supports x32.  Otherwise, set 'false'.\n          #             Set 'fail' if it supports x32 but fails for now.  'fail' cases must be removed.\n          #   x86     : Set 'true' if compiler supports x86 (-m32).  Otherwise, set 'false'.\n          #             Set 'fail' if it supports x86 but fails for now.  'fail' cases must be removed.\n          #   cxxtest : Set 'true' if it can be compiled as C++ code.  Otherwise, set 'false'.\n          #   os      : GitHub Actions YAML workflow label.  See https://github.com/actions/virtual-environments#available-environments\n\n          # cc\n          { pkgs: '',                                                   cc: cc,        cxx: c++,         x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-latest, },\n\n          # gcc\n          { pkgs: '',                                                   cc: gcc,       cxx: g++,         x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-latest, },\n          { pkgs: 'gcc-11 g++-11 lib32gcc-11-dev libx32gcc-11-dev',     cc: gcc-11,    cxx: g++-11,      x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'gcc-10 lib32gcc-10-dev libx32gcc-10-dev',            cc: gcc-10,    cxx: g++-10,      x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'gcc-9  lib32gcc-9-dev  libx32gcc-9-dev',             cc: gcc-9,     cxx: g++-9,       x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'gcc-8 g++-8 lib32gcc-8-dev libx32gcc-8-dev',         cc: gcc-8,     cxx: g++-8,       x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'gcc-7 g++-7 lib32gcc-7-dev libx32gcc-7-dev',         cc: gcc-7,     cxx: g++-7,       x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'gcc-6 g++-6 lib32gcc-6-dev libx32gcc-6-dev',         cc: gcc-6,     cxx: g++-6,       x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-18.04,  },\n          { pkgs: 'gcc-5 g++-5 lib32gcc-5-dev libx32gcc-5-dev',         cc: gcc-5,     cxx: g++-5,       x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-18.04,  },\n          { pkgs: 'gcc-4.8 g++-4.8 lib32gcc-4.8-dev libx32gcc-4.8-dev', cc: gcc-4.8,   cxx: g++-4.8,     x32: 'true', x86: 'true', cxxtest: 'true',  os: ubuntu-18.04,  },\n\n          # clang\n          { pkgs: 'lib32gcc-11-dev libx32gcc-11-dev',                   cc: clang,     cxx: clang++,     x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-latest, },\n          { pkgs: 'clang-12  lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-12,  cxx: clang++-12,  x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'clang-11  lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-11,  cxx: clang++-11,  x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'clang-10  lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-10,  cxx: clang++-10,  x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'clang-9   lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-9,   cxx: clang++-9,   x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'clang-8   lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-8,   cxx: clang++-8,   x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'clang-7   lib32gcc-7-dev  libx32gcc-7-dev',          cc: clang-7,   cxx: clang++-7,   x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'clang-6.0 lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-6.0, cxx: clang++-6.0, x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-20.04,  },\n          { pkgs: 'clang-5.0 lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-5.0, cxx: clang++-5.0, x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-18.04,  },\n          { pkgs: 'clang-4.0 lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-4.0, cxx: clang++-4.0, x32: 'fail', x86: 'fail', cxxtest: 'true',  os: ubuntu-18.04,  },\n          { pkgs: 'clang-3.9 lib32gcc-11-dev libx32gcc-11-dev',         cc: clang-3.9, cxx: clang++-3.9, x32: 'fail', x86: 'fail', cxxtest: 'false', os: ubuntu-18.04,  },\n        ]\n\n    runs-on: ${{ matrix.os }}\n    env:                        # Set environment variables\n      # We globally set CC and CXX to improve compatibility with .travis.yml\n      CC: ${{ matrix.cc }}\n      CXX: ${{ matrix.cxx }}\n      FIXME__LZ4_CI_IGNORE : ' echo Error.  But we ignore it for now.'\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install gcc-multilib\n        sudo apt-get install ${{ matrix.pkgs }}\n\n    - name: Environment info\n      run: |\n        echo && type $CC && which $CC && $CC --version\n        echo && type $CXX && which $CXX && $CXX --version\n\n    - name: make\n      if: always()\n      run: make V=1\n\n    - name: make all\n      if: always()\n      run: make V=1 clean all\n\n    - name: make c_standards (C90)\n      if: always()\n      run: make V=1 clean c_standards_c90\n\n    - name: make c_standards (C11)\n      if: always()\n      run: make V=1 clean c_standards_c11\n\n    - name: make c-to-c++\n      if: always()\n      run: make V=1 clean ctocpptest\n\n    - name: make cxxtest\n      if: ${{ matrix.cxxtest == 'true' }}\n      run: make V=1 clean cxxtest\n\n    - name: make -C programs default\n      if: always()\n      run: make V=1 -C programs clean default\n\n    - name: make -C programs default -D_FORTIFY_SOURCE=2\n      if: always()\n      run: CFLAGS='-fPIC' LDFLAGS='-pie -fPIE -D_FORTIFY_SOURCE=2' make V=1 -C programs clean default\n\n    - name: make -C tests test-lz4\n      if: always()\n      run: MOREFLAGS='-Werror' make V=1 -C tests clean test-lz4\n\n    - name: make clangtest (clang only)\n      if: ${{ startsWith( matrix.cc , 'clang' ) }}\n      run: make V=1 clean clangtest\n\n    - name: make -C tests test MOREFLAGS='-mx32'\n      if: ${{ matrix.x32 == 'true' }}\n      run: LDFLAGS='-Wl,--verbose' MOREFLAGS='-mx32' make V=1 -C tests clean test\n\n    - name: make -C tests test-lz4c32\n      if: ${{ matrix.x86 == 'true' }}\n      run: LDFLAGS='-Wl,--verbose' MOREFLAGS='-Werror' make V=1 -C tests clean test-lz4c32\n\n\n    ###############################################################\n    #                                                             #\n    #      Remove this block when we stabilize the tests.         #\n    #                                                             #\n\n    - name: make -C tests test MOREFLAGS='-mx32' || echo Ignore failure for now.\n      if: ${{ matrix.x32 == 'fail' }}\n      run: LDFLAGS='-Wl,--verbose' MOREFLAGS='-mx32' make V=1 -C tests clean test || $FIXME__LZ4_CI_IGNORE\n\n    - name: make -C tests test-lz4c32 || echo Ignore failure for now.\n      if: ${{ matrix.x86 == 'fail' }}\n      run: LDFLAGS='-Wl,--verbose' MOREFLAGS='-Werror' make V=1 -C tests clean test-lz4c32 || $FIXME__LZ4_CI_IGNORE\n\n    #                                                             #\n    ###############################################################\n\n\n\n###############################################################\n# LZ4 self tests\n#\n# - Benchmark\n# - Fuzzer\n# - LZ4 Frame\n# - LZ4 versions\n# - Custom LZ4_DISTANCE_MAX\n#\n  lz4-benchmark:\n    name: Benchmark\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install gcc-multilib\n\n    - name: benchmark (-C tests test-lz4)\n      run: make V=1 -C tests test-lz4\n\n    - name: benchmark (-C tests test-lz4c)\n      run: make V=1 -C tests test-lz4c\n\n    - name: benchmark (-C tests test-lz4c32)\n      run: make V=1 -C tests test-lz4c32\n\n    - name: benchmark (-C tests test-fullbench)\n      run: make V=1 -C tests test-fullbench\n\n    - name: benchmark (-C tests test-fullbench32)\n      run: make V=1 -C tests test-fullbench32\n\n\n  lz4-fuzzer:\n    name: Fuzzer test\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install gcc-multilib\n\n    - name: setup\n      run: sudo sysctl -w vm.mmap_min_addr=4096\n\n    - name: fuzzer\n      run: make V=1 -C tests test-fuzzer\n\n    - name: fuzzer32\n      run: make V=1 -C tests test-fuzzer32\n\n\n  lz4-versions:\n    name: LZ4 versions test\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install gcc-multilib\n\n    - name: make -C tests versionsTest\n      run: make V=1 -C tests versionsTest\n\n\n  lz4-frame:\n    name: LZ4 frame test\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install gcc-multilib\n\n    - name: LZ4 frame test\n      run: make V=1 -C tests test-frametest\n\n    - name: LZ4 frame test (32-bit)\n      run: make V=1 -C tests test-frametest32\n\n\n  # Custom LZ4_DISTANCE_MAX ; lz4-wlib (CLI linked to dynamic library); LZ4_USER_MEMORY_FUNCTIONS\n  lz4-custom-distance:\n    name: Custom LZ4_DISTANCE_MAX\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: custom LZ4_DISTANCE_MAX\n      run: |\n        MOREFLAGS='-DLZ4_DISTANCE_MAX=8000' make V=1 check\n        make V=1 clean\n        make V=1 -C programs lz4-wlib\n        make V=1 clean\n        make V=1 -C tests fullbench-wmalloc  # test LZ4_USER_MEMORY_FUNCTIONS\n        make V=1 clean\n        CC=\"c++ -Wno-deprecated\" make V=1 -C tests fullbench-wmalloc  # stricter function signature check\n\n\n\n###############################################################\n# Check tools\n#\n# - cppcheck\n# - scan-build\n# - valgrind\n# - ubsan\n# - asan\n# - unicode-lint\n# - build examples\n#\n  lz4-cppcheck:\n    name: make cppcheck\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install cppcheck\n\n    - name: Environment info\n      run: echo && type cppcheck && which cppcheck && cppcheck --version\n\n    - name: cppcheck\n      # This test script ignores the exit code of cppcheck.\n      # See known issues in README.md.\n      run: make V=1 clean cppcheck || echo There are some cppcheck reports but we ignore it.\n\n\n  lz4-scan-build:\n    name: make staticAnalyze\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install clang-tools\n\n    - name: Environment info\n      run: |\n        echo && type gcc && which gcc && gcc --version\n        echo && type clang && which clang && clang --version\n        echo && type scan-build && which scan-build               # scan-build doesn't have any --version equivalent option\n        echo && type make && which make && make -v\n        echo && cat /proc/cpuinfo || echo /proc/cpuinfo is not present\n\n    - name: make staticAnalyze\n      run: make V=1 clean staticAnalyze\n\n\n  lz4-valgrind:\n    name: valgrind\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install valgrind\n\n    - name: Environment info\n      run: |\n        echo && type cc && which cc && cc --version\n        echo && type valgrind && which valgrind && valgrind --version\n\n    - name: valgrind\n      run: make V=1 -C tests test-mem\n\n\n  lz4-ubsan-x64:\n    name: Linux x64 ubsan\n    runs-on: ubuntu-latest\n    env:                        # Set environment variables\n      FIXME__LZ4_CI_IGNORE : ' echo Error.  But we ignore it for now.'\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: ubsan\n      #########################################################\n      # For now, we ignore the exit code of `make usan`.\n      # See \"Known issues / lz4-ubsan-x64\" in README.md\n      # When we'll resolve this issue, remove \"|| $FIXME__LZ4_CI_IGNORE\"\n      #########################################################\n      run: make V=1 clean usan MOREFLAGS='-Wcomma -Werror' || $FIXME__LZ4_CI_IGNORE\n\n\n  lz4-ubsan-x86:\n    name: Linux x86 ubsan\n    runs-on: ubuntu-latest\n    env:                        # Set environment variables\n      FIXME__LZ4_CI_IGNORE : ' echo Error.  But we ignore it for now.'\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install gcc-multilib\n        sudo apt-get install lib32gcc-11-dev\n\n    - name: ubsan32\n      #########################################################\n      # For now, we ignore the exit code of `make usan32`.\n      # See \"Known issues / lz4-ubsaan-x86\" in README.md.\n      # When we'll resolve this issue, remove \"|| $FIXME__LZ4_CI_IGNORE\"\n      #########################################################\n      run: CC=clang make V=1 clean usan32 MOREFLAGS='-Wcomma -Werror' || $FIXME__LZ4_CI_IGNORE\n\n\n  lz4-asan-x64:\n    name: Linux x64 ASAN\n    runs-on: ubuntu-latest\n    env:                        # Set environment variables\n      FIXME__LZ4_CI_IGNORE : ' echo Error.  But we ignore it for now.'\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: setup\n      run: sudo sysctl -w vm.mmap_min_addr=4096\n\n    - name: frametest\n      run: CC=clang MOREFLAGS=-fsanitize=address make V=1 -C tests clean test-frametest\n\n    - name: fuzzer\n      run: CC=clang MOREFLAGS=-fsanitize=address make V=1 -C tests clean test-fuzzer\n\n  unicode-lint:\n    name: lint unicode in ./lib/, ./tests/ and ./programs/\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n    - name: unicode lint\n      run: bash ./tests/unicode_lint.sh\n\n\n  lz4-examples:\n    name: make examples\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n\n    - name: Environment info\n      run: |\n        echo && type cc && which cc && cc --version\n        echo && type c++ && which c++ && c++ --version\n\n    - name: examples\n      run: make V=1 clean examples\n\n    - name: examples (compile as C++ code)\n      run: make V=1 -C examples clean cxxtest\n\n\n###############################################################\n# Platforms\n#\n# - QEMU (ARM, ARM64, PPC, PPC64LE, S390X)\n# - macOS\n#\n\n  # QEMU\n  # All tests use QEMU (static) and gcc cross compiler.\n  #\n  # note:\n  #   We don't employ completely matrix method which provides `MOREFLAGS`\n  #   etc in the matrix.  Because some platform may need its special\n  #   compiler options and test.\n  #   For example, xxHash already has tests for scalar and SIMD version of\n  #   it.  But compiler options are quite different between platforms.\n  #\n  #   So, please keep them simple and independent.\n  #\n  lz4-qemu-platforms:\n    name: QEMU ${{ matrix.type }}\n    strategy:\n      fail-fast: false  # 'false' means Don't stop matrix workflows even if some matrix instance failed.\n      matrix:\n        include: [\n          # You can access the following values via ${{ matrix.??? }}\n          #   type : Architecture type for `if:` statement.\n          #   pkgs : apt-get package names.  You can include multiple packages which are delimited by space.\n          #   xcc  : gcc cross C compiler executable.\n          #   xemu : QEMU static emulator executable.\n          #   os   : GitHub Actions YAML workflow label.  See https://github.com/actions/virtual-environments#available-environments\n\n          { type: ARM,      pkgs: 'qemu-system-arm   gcc-arm-linux-gnueabi',     xcc: arm-linux-gnueabi-gcc,     xemu: qemu-arm-static,     os: ubuntu-latest, },\n          { type: ARM64,    pkgs: 'qemu-system-arm   gcc-aarch64-linux-gnu',     xcc: aarch64-linux-gnu-gcc,     xemu: qemu-aarch64-static, os: ubuntu-latest, },\n          { type: PPC,      pkgs: 'qemu-system-ppc   gcc-powerpc-linux-gnu',     xcc: powerpc-linux-gnu-gcc,     xemu: qemu-ppc-static,     os: ubuntu-latest, },\n          { type: PPC64LE,  pkgs: 'qemu-system-ppc   gcc-powerpc64le-linux-gnu', xcc: powerpc64le-linux-gnu-gcc, xemu: qemu-ppc64le-static, os: ubuntu-latest, },\n          { type: S390X,    pkgs: 'qemu-system-s390x gcc-s390x-linux-gnu',       xcc: s390x-linux-gnu-gcc,       xemu: qemu-s390x-static,   os: ubuntu-latest, },\n        ]\n\n    runs-on: ${{ matrix.os }}\n    env:                        # Set environment variables\n      XCC: ${{ matrix.xcc }}\n      XEMU: ${{ matrix.xemu }}\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: apt-get install\n      run: |\n        sudo apt-get update\n        sudo apt-get install gcc-multilib\n        sudo apt-get install qemu-utils qemu-user-static\n        sudo apt-get install ${{ matrix.pkgs }}\n\n    - name: Environment info\n      run: |\n        echo && type $XCC && which $XCC && $XCC --version\n        echo && $XCC -v                       # Show built-in specs\n        echo && type $XEMU && which $XEMU && $XEMU --version\n\n    - name: ARM64\n      if: ${{ matrix.type == 'ARM64' }}\n      run: make V=1 platformTest CC=$XCC QEMU_SYS=$XEMU\n\n    - name: ARM\n      if: ${{ matrix.type == 'ARM' }}\n      run: make V=1 platformTest CC=$XCC QEMU_SYS=$XEMU\n\n    - name: PPC\n      if: ${{ matrix.type == 'PPC' }}\n      run: make V=1 platformTest CC=$XCC QEMU_SYS=$XEMU\n\n    - name: PPC64LE\n      if: ${{ matrix.type == 'PPC64LE' }}\n      run: make V=1 platformTest CC=$XCC QEMU_SYS=$XEMU MOREFLAGS=-m64\n\n    - name: S390X\n      if: ${{ matrix.type == 'S390X' }}\n      run: make V=1 platformTest CC=$XCC QEMU_SYS=$XEMU\n\n\n  # macOS\n  lz4-platform-macos-latest:\n    name: macOS\n    runs-on: macos-latest\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Environment info\n      run: |\n        echo && type cc && which cc && cc --version\n        echo && type make && which make && make -v\n        echo && sysctl -a | grep machdep.cpu   # cpuinfo\n\n    - name: make default\n      run: CFLAGS=\"-Werror\" make V=1 clean default\n\n    - name: make test\n      run: make V=1 clean test MOREFLAGS='-Werror -Wconversion -Wno-sign-conversion'\n\n    - name: make test | tee\n      # test scenario where `stdout` is not the console\n      run: make V=1 clean test MOREFLAGS='-Werror -Wconversion -Wno-sign-conversion' | tee\n\n\n\n###############################################################\n# Build systems\n#\n# - make\n# - cmake\n# - meson\n#\n\n  # make\n  lz4-build-make:\n    name: make\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: Environment info\n      run: |\n        echo && type cc && which cc && cc --version\n        echo && type make && which make && make -v\n\n    - name: make\n      run: make V=1\n\n\n  lz4-build-make-travis-install:\n    name: make travis-install\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: travis-install\n      run: make V=1 clean travis-install\n\n    - name: travis-install result\n      run: |\n        echo && echo Installed files\n        ( cd ~/install_test_dir; find .; )\n\n\n  # cmake\n  lz4-build-cmake:\n    name: cmake\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n\n    - name: Environment info\n      run: |\n        echo && type cmake && which cmake && cmake --version\n        echo && type make && which make && make -v\n\n    - name: cmake\n      run: |\n        cd build/cmake\n        mkdir build\n        cd build\n        cmake ..\n        CFLAGS=-Werror make VERBOSE=1\n\n\n  # Invoke cmake via Makefile\n  lz4-build-make-cmake:\n    name: make cmake\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n    - name: make cmake\n      # V=1 for lz4 Makefile, VERBOSE=1 for cmake Makefile.\n      run: make V=1 VERBOSE=1 clean cmake\n\n\n  # Meson\n  lz4-build-meson:\n    name: Meson + Ninja\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # https://github.com/actions/checkout\n    - uses: actions/setup-python@v2 # https://github.com/actions/setup-python\n      with:\n        python-version: '3.x'\n\n    - name: Install\n      run: |\n        sudo apt-get update\n        sudo apt-get install tree ninja-build\n        python -m pip install --upgrade pip\n        pip3 install --user meson\n\n    - name: Environment info\n      run: |\n        echo && type clang && which clang && clang --version\n        echo && type python && which python && python --version\n        echo && type meson && which meson && meson --version\n\n    - name: meson\n      # 'run: >' replaces all newlines in the following block with spaces\n      run: >\n        meson setup\n        --buildtype=debug\n        -Db_lundef=false\n        -Dauto_features=enabled\n        -Ddefault_library=both\n        -Dbin_programs=true\n        -Dbin_contrib=true\n        -Dbin_tests=true\n        -Dbin_examples=true\n        contrib/meson build\n\n    - name: staging\n      run: |\n        cd build\n        DESTDIR=./staging ninja install\n        tree ./staging\n\n\n\n############################################################\n# Check git tag for LZ4 releases\n#\n  lz4-check-tag:\n    name: git version tag checking for release\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: make -C tests checkTag\n      if: startsWith(github.ref, 'refs/tags/v')   # If git tag name starts with 'v'\n      run: |\n        echo \"tag=${GITHUB_REF#refs/*/}\"\n        make -C tests checkTag\n        tests/checkTag ${GITHUB_REF#refs/*/}\n\n\n\n############################################################\n# Gather CI environment information.\n#\n  lz4-env-info:\n    name: GH-Actions Virtual Env Info (${{ matrix.os }})\n    strategy:\n      matrix:\n        include: [\n          { os: ubuntu-latest,  }, # https://github.com/actions/virtual-environments/\n          { os: ubuntu-20.04,   }, # https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu2004-README.md\n          { os: ubuntu-18.04,   }, # https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu1804-README.md\n        ]\n\n    runs-on: ${{ matrix.os }}\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: init\n      run: |\n        sudo apt-get update\n\n    - name: cc --version\n      run: echo && type cc && which cc && cc --version\n\n    - name: gcc --version\n      run: echo && type gcc && which gcc && gcc --version\n\n    - name: clang --version\n      run: echo && type clang && which clang && clang --version\n\n    - name: make -v\n      run: echo && type make && which make && make -v\n\n    - name: g++ --version\n      run: echo && type g++ && which g++ && g++ --version\n\n    - name: git --version\n      run: echo && type git && which git && git --version\n\n    - name: gcc packages (apt-cache)\n      run: apt-cache search gcc | grep \"^gcc-[0-9\\.]* \" | sort\n\n    - name: lib32gcc packages for i386 (apt-cache)\n      run: apt-cache search lib32gcc | grep \"^lib32gcc-\" | sort\n\n    - name: libx32gcc packages for x32 (apt-cache)\n      run: apt-cache search libx32gcc | grep \"^libx32gcc-\" | sort\n\n    - name: gcc multilib packages (apt-cache)\n      run: apt-cache search multilib | grep \"gcc-\" | sort\n\n    - name: clang packages (apt-cache)\n      run: apt-cache search clang | grep \"^clang-[0-9\\.]* \" | sort\n\n    - name: QEMU packages (apt-cache)\n      run: apt-cache search qemu | grep \"^qemu-system-.*QEMU full system\" | sort\n",
    "source": "crdroidandroid/android_external_lz4",
    "path": ".github/workflows/ci.yml",
    "url": "https://github.com/crdroidandroid/android_external_lz4/blob/96c6f5cda6e9b247370709fc4914a3bb31b48ce8/.github/workflows/ci.yml",
    "retrieved_at": "2025-09-01T01:55:01.972635Z"
  },
  {
    "question": "Does this workflow automatically commit minified files to the same branch that triggered the workflow?",
    "answer": "# This is a basic workflow to help you get started with Actions\n\nname: Auto Minify CSS/JS file\n\n# Controls when the action will run. Triggers the workflow on push or pull request\n# events but only for the dev branch\non:\n  push:\n    branches: [ dev ]\n    paths:\n      - 'public/assets/css/**'\n      - 'public/assets/js/**'\n      - 'public/theme/material/css/**'\n      - 'public/theme/material/js/**'\n  pull_request:\n    branches: [ dev ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Auto Minify Theme CSS file\n        uses: nizarmah/auto-minify@master\n        with:\n          directory: 'public/theme/material/css'\n      - name: Auto Minify Theme JS file\n        uses: nizarmah/auto-minify@master\n        with:\n          directory: 'public/theme/material/js'\n      - name: Auto Minify CSS file\n        uses: nizarmah/auto-minify@master\n        with:\n          directory: 'public/assets/css'\n      - name: Auto Minify JS file\n        uses: nizarmah/auto-minify@master\n        with:\n          directory: 'public/assets/js'\n      - name: Auto committing minified files\n        uses: stefanzweifel/git-auto-commit-action@v3.0.0\n        with:\n          repository: 'public'\n          commit_message: \"Github Action: Auto Minified Theme CSS/JS Files\"\n          branch: ${{ github.ref }}\n",
    "source": "iamsaltedfish/sspanel-v3-tabler",
    "path": ".github/workflows/minify.yml",
    "url": "https://github.com/iamsaltedfish/sspanel-v3-tabler/blob/fa82f6ba92e3a2da6439d62d3a712be96db1e575/.github/workflows/minify.yml",
    "retrieved_at": "2025-09-01T01:55:02.693925Z"
  }
]